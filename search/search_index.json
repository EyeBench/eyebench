{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"EyeBench: Predictive Modeling from Eye Movements in Reading","text":"<p> Figure 1: Overview of EyeBench v1.0. The benchmark curates multiple datasets for predicting reader properties (\ud83d\udc69), and reader\u2013text interactions (\ud83d\udc69+\ud83d\udcdd) from eye movements. * marks prediction tasks newly introduced in EyeBench. The data are preprocessed and standardized into aligned text and gaze sequences, which are then used as input to models trained to predict task-specific targets. The models are systematically evaluated under three generalization regimes \u2014 unseen readers, unseen texts, or both. The benchmark supports the evaluation and addition of new models, datasets, and tasks.</p>"},{"location":"#introduction","title":"\ud83e\udde0 Introduction","text":"<p>EyeBench is the first benchmark designed to evaluate machine learning models that decode cognitive and linguistic information from eye movements during reading. It provides a standardized, extensible framework for predictive modeling from eye tracking data, aiming to bridge cognitive science and multimodal AI.</p> <p>EyeBench curates multiple publicly available datasets and tasks, covering both reader properties and reader\u2013text interactions, and includes baselines, state-of-the-art models, and evaluation protocols that ensure reproducibility and comparability across studies.</p> <p>Progress on EyeBench is expected to advance both scientific understanding of human language processing and practical applications such as adaptive educational systems and cognitive-aware user interfaces.</p> <p>Official repository: https://github.com/EyeBench/eyebench</p>"},{"location":"#tasks-and-datasets","title":"\ud83d\udcda Tasks and Datasets","text":"<p>EyeBench v1.0 includes seven prediction tasks spanning six harmonized datasets. Each task is formulated as a single-trial prediction problem from a reader\u2019s eye movements while reading a passage (and optionally an auxiliary text, such as a question or claim).</p>"},{"location":"#reader-properties","title":"Reader Properties (\ud83d\udc64)","text":"Task Dataset Type Target Reading Comprehension Skill CopCo Regression Continuous comprehension score (1\u201310) Vocabulary Knowledge MECO L2 Regression LexTALE vocabulary test score (0\u2013100) Dyslexia Detection CopCo Classification Clinically diagnosed dyslexia (yes/no)"},{"location":"#readertext-interactions","title":"Reader\u2013Text Interactions (\ud83d\udc64 + \ud83d\udcd6)","text":"Task Dataset(s) Type Target Reading Comprehension OneStop, SB-SAT, PoTeC Classification Correct answer to a comprehension question Subjective Text Difficulty SB-SAT Regression Perceived difficulty rating (Likert) Domain Expertise PoTeC Classification High vs low domain expertise Claim Verification IITB-HGC Classification Correct claim verification judgment"},{"location":"#datasets-overview","title":"Datasets Overview","text":"Dataset Language Group #Participants #Words #Fixations Tasks OneStop (Ordinary Reading) English L1 180 19 427 1.1 M Reading Comprehension SB-SAT English L1/L2 95 2 622 263 k Reading Comprehension, Subjective Text Difficulty PoTeC German L1 75 1 895 404 k Reading Comprehension, Domain Expertise MECO L2 English L2 1 098 1 646 2.4 M Vocabulary Knowledge CopCo Danish L1/L2/L1-Dyslexia 57 32 140 398 k Reading Comprehension Skill, Dyslexia Detection IITB-HGC English L1/L2 5 53 528 164 k Claim Verification"},{"location":"#implemented-models-and-baselines","title":"\ud83e\udde9 Implemented Models and Baselines","text":"<p>EyeBench provides 12 implemented models and 6 baselines, unified under a shared training and evaluation framework.</p>"},{"location":"#neural-models","title":"Neural Models","text":"<ul> <li>AhnCNN \u2013 CNN over fixation sequences (coordinates, durations, pupil size)  </li> <li>AhnRNN \u2013 RNN variant of AhnCNN  </li> <li>BEyeLSTM \u2013 LSTM combining sequential fixations and global gaze statistics  </li> <li>PLM-AS \u2013 LSTM processing fixation-ordered word embeddings  </li> <li>PLM-AS-RM \u2013 RNN integrating fixation-ordered embeddings with reading measures  </li> <li>RoBERTEye-W \u2013 Transformer integrating word embeddings and word-level gaze features  </li> <li>RoBERTEye-F \u2013 Fixation-level variant of RoBERTEye-W  </li> <li>MAG-Eye \u2013 Multimodal Adaptation Gate injecting gaze into transformer layers  </li> <li>PostFusion-Eye \u2013 Cross-attention fusion of RoBERTa embeddings and CNN fixation features  </li> </ul>"},{"location":"#traditional-ml-models","title":"Traditional ML Models","text":"<ul> <li>Logistic / Linear Regression </li> <li>Support Vector Machine (SVM / SVR) </li> <li>Random Forest (Classifier / Regressor) </li> </ul>"},{"location":"#baselines","title":"Baselines","text":"<ul> <li>Random and Majority Class (classification)  </li> <li>Mean and Median (regression)  </li> <li>Reading Speed </li> <li>Text-Only RoBERTa (no gaze input)</li> </ul>"},{"location":"#evaluation-protocol","title":"\ud83e\uddee Evaluation Protocol","text":"<p>EyeBench evaluates models under three complementary generalization regimes:</p> Regime Description Typical Use Case Unseen Reader Texts seen, readers unseen New readers, known materials Unseen Text Readers seen, texts unseen Personalized reading of new content Unseen Reader &amp; Text Both unseen Fully general setting"},{"location":"#metrics","title":"Metrics","text":"<ul> <li>Classification: AUROC, Balanced Accuracy  </li> <li>Regression: RMSE, MAE, R\u00b2  </li> <li>Aggregate: Average Normalized Score and Mean Rank across all task\u2013dataset pairs.</li> </ul>"},{"location":"#getting-started","title":"\u2699\ufe0f Getting Started","text":""},{"location":"#1-clone-and-install","title":"1. Clone and Install","text":"<pre><code>git clone https://github.com/EyeBench/eyebench.git\ncd eyebench\nmamba env create -f environment.yml\nconda activate eyebench\n</code></pre>"},{"location":"#2-download-and-preprocess-data","title":"2. Download and Preprocess Data","text":"<pre><code>bash src/data/preprocessing/get_data.sh\n</code></pre> <p>This script downloads, harmonizes, and creates standardized folds for all datasets under <code>data/processed/</code>.</p>"},{"location":"#3-log-into-weights-biases-wandb","title":"3. Log into Weights &amp; Biases (WandB)","text":"<pre><code>wandb login\n</code></pre>"},{"location":"#usage","title":"\ud83d\ude80 Usage","text":""},{"location":"#train-a-model","title":"Train a Model","text":"<pre><code>python src/run/single_run/train.py +trainer=TrainerDL +model=RoBERTEyeW +data=OneStop_TRC\n</code></pre>"},{"location":"#run-a-hyperparameter-sweep","title":"Run a Hyperparameter Sweep","text":"<pre><code>bash src/run/multi_run/sweep_wrapper.sh --data_tasks CopCo_TYP --folds 0,1,2,3 --cuda 0,1\n</code></pre>"},{"location":"#test-a-model","title":"Test a Model","text":"<pre><code>python src/run/single_run/test_dl.py +model=RoBERTEyeW +data=OneStop_TRC\n</code></pre> <p>Results are stored under: <code>results/raw/{data_model_trainer_task}/fold_index={i}/trial_level_test_results.csv results/eyebench_benchmark_results/{metric}.csv</code></p>"},{"location":"#adding-a-new-model","title":"\ud83e\udde0 Adding a New Model","text":"<ol> <li>Create a file under <code>src/models/YourModel.py</code> inheriting from <code>BaseModel</code>.    Implement <code>forward()</code> and <code>shared_step()</code> methods.</li> <li> <p>Register it in:</p> <ul> <li><code>src/configs/enums.py</code> \u2192 <code>ModelNames</code></li> <li><code>src/configs/model_args.py</code> \u2192 model config class</li> <li><code>src/configs/config.py</code> \u2192 <code>ModelMapping</code></li> </ul> </li> <li> <p>Define its default parameters and search space in <code>src/run/multi_run/search_spaces.py</code>.</p> </li> <li>Verify integration:</li> </ol> <pre><code>bash src/run/multi_run/model_checker.sh\n</code></pre>"},{"location":"#adding-a-new-dataset","title":"\ud83d\udcca Adding a New Dataset","text":"<ol> <li>Store raw or preprocessed data in <code>data/YOUR_DATASET/</code>.</li> <li>Define its loading logic in <code>src/data/datasets/YOUR_DATASET.py</code> (inherits from <code>ETDataset</code>).</li> <li>Add preprocessing logic under <code>src/data/preprocessing/dataset_preprocessing/YOUR_DATASET.py</code>.</li> <li>Register the dataset in <code>src/configs/data.py</code> and <code>src/configs/constants.py</code>.</li> <li>Add a corresponding task configuration class if it supports multiple tasks.</li> </ol> <p>Datasets must comply with EyeBench\u2019s selection criteria:</p> <ul> <li>Passage-level texts</li> <li>\u2265 500 Hz sampling rate</li> <li>Publicly available raw or fixation-level data</li> <li>Released texts and gaze\u2013text alignment</li> </ul>"},{"location":"#documentation","title":"\ud83d\udcd8 Documentation","text":"<p>To build the local documentation site:</p> <pre><code>pip install mkdocs mkdocs-material 'mkdocstrings[python]' mkdocs-gen-files mkdocs-literate-nav\nmkdocs serve\n</code></pre>"},{"location":"#citation","title":"\ud83d\udcc4 Citation","text":"<p>If you use EyeBench in your research, please cite:</p> <p>Omer Shubi, David R. Reich, Keren Gruteke Klein, Yuval Angel, Paul Prasse, Lena J\u00e4ger, Yevgeni Berzak. EyeBench: Predictive Modeling from Eye Movements in Reading. NeurIPS 2025.</p> <pre><code>@inproceedings{shubi2025eyebench,\n  title={EyeBench: Predictive Modeling from Eye Movements in Reading},\n  author={Omer Shubi and David R. Reich and Keren Gruteke Klein and Yuval Angel and Paul Prasse and Lena J\u00e4ger and Yevgeni Berzak},\n  booktitle={Advances in Neural Information Processing Systems},\n  year={2025}\n}\n</code></pre>"},{"location":"#acknowledgments","title":"\ud83e\udd1d Acknowledgments","text":"<p>EyeBench development is supported by:</p> <ul> <li>COST Action MultiplEYE (CA21131)</li> <li>Swiss National Science Foundation (EyeNLG, IZCOZ0 _220330)</li> <li>Israel Science Foundation (grant 1499/22)</li> </ul>"},{"location":"#license","title":"\ud83e\udde9 License","text":"<p>All datasets included in EyeBench follow their respective original licenses. Code released under the MIT License.</p>"},{"location":"NOTES/","title":"Datasets","text":""},{"location":"NOTES/#copco","title":"CopCo","text":""},{"location":"NOTES/#features-and-adjustements","title":"Features and Adjustements","text":"<ul> <li>Accounted for non-breaking white spaces in interest areas <code>\\xa0</code>.</li> </ul>"},{"location":"NOTES/#filtered","title":"Filtered","text":"<ul> <li>Excluded practice trials indicated via <code>paragraph_id=-1</code> and <code>speech_id=1327</code>.</li> <li>Excluded participants without <code>RCS score</code> for <code>Reading Comprehension Skill</code> task.</li> </ul>"},{"location":"NOTES/#missing-values","title":"Missing values","text":"<ul> <li><code>NEXT_SAC_ANGLE</code> set to 0.</li> <li><code>NEXT_FIX_ANGLE</code> set to 0.</li> <li><code>NEXT_FIX_DISTANCE</code> set to 0.</li> <li><code>PREVIOUS_FIX_ANGLE</code> set to 0.</li> <li><code>CURRENT_FIX_PUPIL</code> set to 0.</li> <li><code>NEXT_SAC_AVG_VELOCITY</code> set to 0.</li> </ul>"},{"location":"NOTES/#iitb-hgc","title":"IITB-HGC","text":""},{"location":"NOTES/#features-and-adjustements_1","title":"Features and Adjustements","text":"<ul> <li>Since the dataset only published <code>word indices</code>, approximated <code>x</code> coordinated via <code>word index</code>, set and set <code>y</code> to 0. </li> <li>To match the <code>paragraph</code> and provided interest areas in the fixation report we adjusted:</li> <li>3: <code>with Andy</code> non-breaking whitespace in interest area</li> <li>5: <code>watch Jose</code> non-breaking whitespace in interest area</li> <li>9: <code>$1,750</code> vs <code>$__NBWS__1,750.00</code></li> <li>25: <code>\u00a33</code> vs <code>\u00a3__NBWS__3.00</code></li> <li>26: <code>for Virgil</code> non-breaking whitespace in interest area</li> <li>33: <code>at FC</code> non-breaking whitespace in interest area</li> <li>53: <code>$5,000</code> vs <code>$__NBWS__5,000.00</code></li> <li>74: <code>$20,000</code> vs <code>$__NBWS__20,000.00</code></li> <li>82: <code>\u00a3750,000</code> vs <code>\u00a3__NBWS__7,50,000.00</code></li> <li>99: <code>$5.3</code> vs <code>$__NBWS__5.30</code></li> <li>102: <code>$10</code> vs <code>$__NBWS__10.00</code>, and <code>$9</code> vs <code>$__NBWS__9.00</code></li> <li>130: <code>$50,000</code> vs <code>$__NBWS__50,000.00</code></li> <li>257: <code>$10</code> vs <code>$__NBWS__10.00</code></li> <li>280: <code>Jedinak twisted</code> vs <code>Jedinak__NBWS__twisted</code></li> <li>288: <code>$2</code> vs <code>$__NBWS__2.00</code></li> <li>298: <code>$200</code> vs <code>$__NBWS__200.00</code></li> <li>325: <code>$9</code> vs <code>$__NBWS__9.00</code></li> <li>357: <code>$1.8</code> vs <code>$__NBWS__1.80</code>, and <code>$2.6</code> vs <code>$__NBWS__2.60</code></li> <li>365: <code>$25,000</code> vs <code>$__NBWS__25,000.00</code></li> <li>373: <code>$260</code> vs <code>$__NBWS__260.00</code>, <code>$1.7</code> vs <code>$__NBWS__1.70</code>, and <code>$1.37</code> vs <code>$__NBWS__1.37</code></li> <li>403: <code>hour-long</code> vs <code>hour-long hour-long</code></li> <li>404: <code>$2.9</code> vs <code>$__NBWS__2.90</code></li> <li>425: <code>- many</code> vs <code>-__NBWS__many</code>, <code>against Leicester City</code> vs <code>against__NBWS__Leicester__NBWS__City</code>, and <code>van Gaal's</code> vs <code>van__NBWS__Gaal's</code></li> <li>441: <code>$10,000</code> vs <code>$__NBWS__10,000.00</code></li> <li>460: <code>$1.6</code> vs <code>$__NBWS__1.60</code></li> <li>468: <code>$105</code> vs <code>$__NBWS__105.00</code></li> <li>483: <code>(\u00a34,943)</code> vs <code>(\u00a3__NBWS__4,943.00)</code></li> <li>485: <code>long-running</code> vs <code>long-running long-running</code></li> </ul>"},{"location":"NOTES/#missing-values_1","title":"Missing values","text":"<ul> <li><code>NEXT_SAC_START_X</code> set to 0</li> <li><code>NEXT_SAC_END_X</code> set to 0</li> <li><code>NEXT_SAC_END_Y</code> set to 0</li> <li><code>NEXT_SAC_START_Y</code> set to 0</li> <li><code>PREVIOUS_FIX_DISTANCE</code> set to 0</li> <li><code>NEXT_SAC_ANGLE</code> set to 0</li> <li><code>NEXT_FIX_ANGLE</code> set to 0</li> <li><code>NEXT_FIX_DISTANCE</code> set to 0</li> <li><code>PREVIOUS_FIX_ANGLE</code> set to 0</li> </ul>"},{"location":"NOTES/#mecol2","title":"MECOL2","text":""},{"location":"NOTES/#features-and-adjustments","title":"Features and Adjustments","text":"<ul> <li>Fixed misalignments for <code>participant_id</code> in paragraph id's (original: adjusted)</li> <li><code>sp_36</code> <ul> <li>11: 12</li> <li>10: 11</li> <li>9: 10</li> <li>8: 9</li> <li>7: 8</li> <li>6: 7</li> <li>5: 6</li> </ul> </li> <li><code>gr_45</code><ul> <li>11: 12</li> <li>10: 11</li> <li>9: 10</li> <li>8: 9</li> <li>7: 8</li> </ul> </li> <li><code>it_25</code><ul> <li>10: 11</li> <li>9: 10</li> <li>7: 8</li> <li>6: 7</li> </ul> </li> <li><code>se_38</code><ul> <li>11: 12</li> <li>10: 11</li> <li>9: 10</li> <li>8: 9</li> <li>7: 8</li> <li>6: 7</li> <li>5: 6</li> <li>4: 5</li> </ul> </li> </ul>"},{"location":"NOTES/#filtered_1","title":"Filtered","text":"<ul> <li>Excluded participants without Lextale score (label for VK).</li> </ul>"},{"location":"NOTES/#potec","title":"PoTeC","text":""},{"location":"NOTES/#filtered_2","title":"Filtered","text":"<ul> <li>Excluded participants without background questions.</li> </ul>"},{"location":"NOTES/#missing-values_2","title":"Missing values","text":"<ul> <li><code>NEXT_SAC_START_X</code> set to 0</li> <li><code>NEXT_SAC_START_Y</code> set to 0</li> <li><code>NEXT_SAC_END_X</code> set to 0</li> <li><code>NEXT_SAC_END_Y</code> set to 0</li> <li><code>NEXT_SAC_AVG_VELOCITY</code> set to 0</li> <li><code>NEXT_SAC_AMPLITUDE</code> set to 0</li> </ul>"},{"location":"NOTES/#sbsat","title":"SBSAT","text":""},{"location":"NOTES/#features-and-adjustments_1","title":"Features and Adjustments","text":"<ul> <li>Adjusted encoding inside of areas of interest to match statistical and machine learning models:</li> <li><code>\\x92':</code>'`</li> <li><code>\\x93</code>: <code>\"</code></li> <li><code>\\x94</code>: <code>\"</code></li> <li><code>\\x97</code>: <code>\u2014</code></li> <li>Matched paragraphs to areas of interest:</li> <li><code>reading-dickens-3</code>: Sempere &amp; with non-breaking whitespace</li> <li><code>reading-dickens-5</code>: <code>Mr. Dickens</code> occupied one area of interest, split into <code>Mr.</code> and <code>Dickens</code></li> <li><code>reading-flytrap-3</code>: <code>Burdon-Sanderson's</code> split across two lines lines but one area of interest defined by authors. Split into two: <code>Burdon-</code> and <code>Sanderson's</code></li> <li><code>reading-genome-2</code>: <code>species\u2014in</code> split across two lines but only one area of interest defined by authors. Split into two: <code>species\u2014</code> and <code>in</code>.</li> <li><code>reading-genome-3</code>: <code>gee-whiz,</code> split across two lines but only one area of interest defined by authors. Split into two: <code>gee-</code> and <code>whiz,</code>.</li> <li>Used OCR to get the text from the original stimuli pictures, afterwards manually corrected.  (@aarbeikop)</li> </ul>"},{"location":"NOTES/#missing-values_3","title":"Missing values","text":"<ul> <li><code>NEXT_SAC_DURATION</code> set to 0</li> <li><code>start_of_line</code> set to 0</li> <li><code>end_of_line</code> set to 0</li> </ul>"},{"location":"NOTES/#onestop","title":"OneStop","text":""},{"location":"NOTES/#data-splits","title":"Data Splits","text":"<ul> <li>Used participant and item splits from previous work - Fine-Grained Prediction of Reading Comprehension from Eye Movements </li> </ul>"},{"location":"reference/SUMMARY/","title":"SUMMARY","text":"<ul> <li>configs<ul> <li>constants</li> <li>data</li> <li>main_config</li> <li>models<ul> <li>base_model</li> <li>dl<ul> <li>Ahn</li> <li>BEyeLSTM</li> <li>MAG</li> <li>PLMAS</li> <li>PLMASF</li> <li>PostFusion</li> <li>RoBERTeye</li> </ul> </li> <li>ml<ul> <li>DummyClassifier</li> <li>LogisticRegression</li> <li>RandomForest</li> <li>SVM</li> <li>XGBoost</li> </ul> </li> </ul> </li> <li>trainers</li> <li>utils</li> </ul> </li> <li>data<ul> <li>datamodules<ul> <li>base_datamodule</li> <li>copco</li> <li>iitbhgc</li> <li>mecol2</li> <li>onestop</li> <li>potec</li> <li>sbsat</li> </ul> </li> <li>datasets<ul> <li>TextDataSet</li> <li>base_dataset</li> <li>copco</li> <li>iitbhgc</li> <li>mecol2</li> <li>onestop</li> <li>potec</li> <li>sbsat</li> </ul> </li> <li>preprocessing<ul> <li>create_folds</li> <li>dataset_preprocessing<ul> <li>base</li> <li>copco</li> <li>iitbhgc</li> <li>meco</li> <li>onestop</li> <li>potec</li> <li>sbsat</li> <li>sbsat_add_is_question_column</li> <li>template</li> </ul> </li> <li>download_data</li> <li>preprocess_data</li> <li>stats</li> <li>union_raw_files</li> </ul> </li> <li>utils</li> </ul> </li> <li>models<ul> <li>ahn_model</li> <li>base_model</li> <li>base_roberta</li> <li>beyelstm_model</li> <li>mag_model</li> <li>models_ml</li> <li>plm_as_f_model</li> <li>plm_as_model</li> <li>post_fusion_model</li> <li>roberteye_model</li> <li>utils</li> </ul> </li> <li>run<ul> <li>multi_run<ul> <li>cleanup_models</li> <li>csv_to_latex</li> <li>raw_to_processed_results</li> <li>search_spaces</li> <li>sweep_creator</li> <li>utils</li> </ul> </li> <li>single_run<ul> <li>test_dl</li> <li>test_ml</li> <li>train</li> <li>utils</li> </ul> </li> </ul> </li> </ul>"},{"location":"reference/configs/__init__/","title":"init","text":"<p>Configuration package for the project.</p>"},{"location":"reference/configs/constants/","title":"constants","text":"<p>Constants used throughout the project.</p>"},{"location":"reference/configs/constants/#configs.constants.Accelerators","title":"<code>Accelerators</code>","text":"<p>               Bases: <code>StrEnum</code></p> <p>Enum for accelerator types.</p> <p>Attributes:</p> Name Type Description <code>AUTO</code> <code>str</code> <p>Represents the automatic selection of accelerator based on availability.</p> <code>CPU</code> <code>str</code> <p>Represents the Central Processing Unit as the accelerator.</p> <code>GPU</code> <code>str</code> <p>Represents the Graphics Processing Unit as the accelerator.</p> Source code in <code>src/configs/constants.py</code> <pre><code>class Accelerators(StrEnum):\n    \"\"\"\n    Enum for accelerator types.\n\n    Attributes:\n        AUTO (str): Represents the automatic selection of accelerator based on availability.\n        CPU (str): Represents the Central Processing Unit as the accelerator.\n        GPU (str): Represents the Graphics Processing Unit as the accelerator.\n    \"\"\"\n\n    AUTO = 'auto'\n    CPU = 'cpu'\n    GPU = 'gpu'\n</code></pre>"},{"location":"reference/configs/constants/#configs.constants.BackboneNames","title":"<code>BackboneNames</code>","text":"<p>               Bases: <code>StrEnum</code></p> <p>Enum for backbone names.</p> <p>Attributes:</p> Name Type Description <code>ROBERTA_BASE</code> <code>str</code> <p>Represents the base version of the RoBERTa model.</p> <code>ROBERTA_LARGE</code> <code>str</code> <p>Represents the large version of the RoBERTa model.</p> <code>ROBERTA_RACE</code> <code>str</code> <p>Represents the fine-tuned-on-RACE RoBERTa model.</p> Source code in <code>src/configs/constants.py</code> <pre><code>class BackboneNames(StrEnum):\n    \"\"\"\n    Enum for backbone names.\n\n    Attributes:\n        ROBERTA_BASE (str): Represents the base version of the RoBERTa model.\n        ROBERTA_LARGE (str): Represents the large version of the RoBERTa model.\n        ROBERTA_RACE (str): Represents the fine-tuned-on-RACE RoBERTa model.\n    \"\"\"\n\n    ROBERTA_BASE = 'roberta-base'\n    ROBERTA_LARGE = 'roberta-large'\n    ROBERTA_RACE = 'LIAMF-USP/roberta-large-finetuned-race'\n    XLM_ROBERTA_BASE = 'FacebookAI/xlm-roberta-base'\n    XLM_ROBERTA_LARGE = 'FacebookAI/xlm-roberta-large'\n</code></pre>"},{"location":"reference/configs/constants/#configs.constants.ConfigName","title":"<code>ConfigName</code>","text":"<p>               Bases: <code>StrEnum</code></p> <p>Enum for config names.</p> <p>Attributes:</p> Name Type Description <code>DATA</code> <code>str</code> <p>Represents the data config.</p> <code>TRAINER</code> <code>str</code> <p>Represents the trainer config.</p> <code>MODEL</code> <code>str</code> <p>Represents the model config.</p> Source code in <code>src/configs/constants.py</code> <pre><code>class ConfigName(StrEnum):\n    \"\"\"\n    Enum for config names.\n\n    Attributes:\n        DATA (str): Represents the data config.\n        TRAINER (str): Represents the trainer config.\n        MODEL (str): Represents the model config.\n    \"\"\"\n\n    DATA = 'data'\n    TRAINER = 'trainer'\n    MODEL = 'model'\n</code></pre>"},{"location":"reference/configs/constants/#configs.constants.DLModelNames","title":"<code>DLModelNames</code>","text":"<p>               Bases: <code>StrEnum</code></p> <p>Enum for model names.</p> <p>Attributes:</p> Name Type Description <code>MAG_MODEL</code> <code>str</code> <p>Represents the name of the MAG model.</p> <code>ROBERTEYE_MODEL</code> <code>str</code> <p>Represents the name of the Eye BERT model.</p> <code>AHN_CNN_MODEL</code> <code>str</code> <p>Represents the name of the AHN CNN model.</p> <code>AHN_RNN_MODEL</code> <code>str</code> <p>Represents the name of the AHN RNN model.</p> <code>BEYELSTM_MODEL</code> <code>str</code> <p>Represents the name of the BEYELSTM model.</p> <code>POSTFUSION_MODEL</code> <code>str</code> <p>Represents the name of the PostFusion model.</p> <code>PLMAS_MODEL</code> <code>str</code> <p>Represents the name of the PLMAS model.</p> <code>PLMASF_MODEL</code> <code>str</code> <p>Represents the name of the PLMASF model.</p> Source code in <code>src/configs/constants.py</code> <pre><code>class DLModelNames(StrEnum):\n    \"\"\"\n    Enum for model names.\n\n    Attributes:\n        MAG_MODEL (str): Represents the name of the MAG model.\n        ROBERTEYE_MODEL (str): Represents the name of the Eye BERT model.\n        AHN_CNN_MODEL (str): Represents the name of the AHN CNN model.\n        AHN_RNN_MODEL (str): Represents the name of the AHN RNN model.\n        BEYELSTM_MODEL (str): Represents the name of the BEYELSTM model.\n        POSTFUSION_MODEL (str): Represents the name of the PostFusion model.\n        PLMAS_MODEL (str): Represents the name of the PLMAS model.\n        PLMASF_MODEL (str): Represents the name of the PLMASF model.\n    \"\"\"\n\n    ROBERTEYE_MODEL = 'Roberteye'\n    POSTFUSION_MODEL = 'PostFusionModel'\n    PLMAS_MODEL = 'PLMASModel'\n    PLMASF_MODEL = 'PLMASFModel'\n    MAG_MODEL = 'MAGModel'\n    AHN_CNN_MODEL = 'AhnCNNModel'\n    AHN_RNN_MODEL = 'AhnRNNModel'\n    BEYELSTM_MODEL = 'BEyeLSTMModel'\n</code></pre>"},{"location":"reference/configs/constants/#configs.constants.DataSets","title":"<code>DataSets</code>","text":"<p>               Bases: <code>StrEnum</code></p> <p>DataSets is an enumeration that represents different datasets used in the application.</p> <p>Attributes:</p> Name Type Description <code>ONESTOP</code> <code>str</code> <p>Represents the OneStop dataset.</p> <code>COPCO</code> <code>str</code> <p>Represents the CopCo dataset.</p> <code>POTEC</code> <code>str</code> <p>Represents the PoTeC dataset.</p> <code>SBSAT</code> <code>str</code> <p>Represents the SBSAT dataset.</p> <code>HALLUCINATION</code> <code>str</code> <p>Represents the IITBHGC dataset.</p> <code>MECO_L2</code> <code>str</code> <p>Represents the MECO L2 dataset.</p> <code>MECO_L2W1</code> <code>str</code> <p>Represents the MECO L2W1 dataset.</p> <code>MECO_L2W2</code> <code>str</code> <p>Represents the MECO L2W2 dataset.</p> Source code in <code>src/configs/constants.py</code> <pre><code>class DataSets(StrEnum):\n    \"\"\"\n    DataSets is an enumeration that represents different datasets used in the application.\n\n    Attributes:\n        ONESTOP (str): Represents the OneStop dataset.\n        COPCO (str): Represents the CopCo dataset.\n        POTEC (str): Represents the PoTeC dataset.\n        SBSAT (str): Represents the SBSAT dataset.\n        HALLUCINATION (str): Represents the IITBHGC dataset.\n        MECO_L2 (str): Represents the MECO L2 dataset.\n        MECO_L2W1 (str): Represents the MECO L2W1 dataset.\n        MECO_L2W2 (str): Represents the MECO L2W2 dataset.\n    \"\"\"\n\n    ONESTOP = 'OneStop'\n    COPCO = 'CopCo'\n    POTEC = 'PoTeC'\n    SBSAT = 'SBSAT'\n    HALLUCINATION = 'IITBHGC'\n    MECO_L2 = 'MECOL2'\n    MECO_L2W1 = 'MECOL2W1'\n    MECO_L2W2 = 'MECOL2W2'\n</code></pre>"},{"location":"reference/configs/constants/#configs.constants.DataType","title":"<code>DataType</code>","text":"<p>               Bases: <code>StrEnum</code></p> <p>DataType is an enumeration that represents different types of data used in the application.</p> <p>Attributes:</p> Name Type Description <code>IA</code> <code>str</code> <p>Represents interest area data.</p> <code>FIXATIONS</code> <code>str</code> <p>Represents fixation data.</p> <code>RAW</code> <code>str</code> <p>Represents raw eye-tracking data.</p> <code>TRIAL_LEVEL</code> <code>str</code> <p>Represents trial-level aggregated data.</p> <code>METADATA</code> <code>str</code> <p>Represents metadata.</p> Source code in <code>src/configs/constants.py</code> <pre><code>class DataType(StrEnum):\n    \"\"\"\n    DataType is an enumeration that represents different types of data used in the application.\n\n    Attributes:\n        IA (str): Represents interest area data.\n        FIXATIONS (str): Represents fixation data.\n        RAW (str): Represents raw eye-tracking data.\n        TRIAL_LEVEL (str): Represents trial-level aggregated data.\n        METADATA (str): Represents metadata.\n    \"\"\"\n\n    IA = 'ia'\n    FIXATIONS = 'fixations'\n    RAW = 'raw'\n    TRIAL_LEVEL = 'trial_level'\n    METADATA = 'metadata'\n</code></pre>"},{"location":"reference/configs/constants/#configs.constants.DatasetLanguage","title":"<code>DatasetLanguage</code>","text":"<p>               Bases: <code>StrEnum</code></p> <p>Enum for dataset languages.</p> <p>Attributes:</p> Name Type Description <code>ENGLISH</code> <code>str</code> <p>Represents English language datasets.</p> <code>GERMAN</code> <code>str</code> <p>Represents German language datasets.</p> <code>DANISH</code> <code>str</code> <p>Represents Danish language datasets.</p> Source code in <code>src/configs/constants.py</code> <pre><code>class DatasetLanguage(StrEnum):\n    \"\"\"\n    Enum for dataset languages.\n\n    Attributes:\n        ENGLISH (str): Represents English language datasets.\n        GERMAN (str): Represents German language datasets.\n        DANISH (str): Represents Danish language datasets.\n    \"\"\"\n\n    ENGLISH = 'English'\n    GERMAN = 'German'\n    DANISH = 'Danish'\n</code></pre>"},{"location":"reference/configs/constants/#configs.constants.FeatureMode","title":"<code>FeatureMode</code>","text":"<p>               Bases: <code>StrEnum</code></p> <p>Enum for feature modes.</p> <p>Attributes:</p> Name Type Description <code>EYES</code> <code>str</code> <p>Represents the eyes feature mode.</p> <code>WORDS</code> <code>str</code> <p>Represents the words feature mode.</p> <code>EYES_WORDS</code> <code>str</code> <p>Represents the combined eyes and words feature mode.</p> Source code in <code>src/configs/constants.py</code> <pre><code>class FeatureMode(StrEnum):\n    \"\"\"\n    Enum for feature modes.\n\n    Attributes:\n        EYES (str): Represents the eyes feature mode.\n        WORDS (str): Represents the words feature mode.\n        EYES_WORDS (str): Represents the combined eyes and words feature mode.\n    \"\"\"\n\n    EYES = 'eyes'\n    WORDS = 'words'\n    EYES_WORDS = 'eyes_words'\n</code></pre>"},{"location":"reference/configs/constants/#configs.constants.Fields","title":"<code>Fields</code>","text":"<p>               Bases: <code>StrEnum</code></p> <p>Enum for field names in the data.</p> <p>Attributes:</p> Name Type Description <code>BATCH</code> <code>str</code> <p>Represents the article_batch.</p> <code>PARAGRAPH_ID</code> <code>str</code> <p>Represents the paragraph_id.</p> <code>UNIQUE_PARAGRAPH_ID</code> <code>str</code> <p>Represents the unique_paragraph_id.</p> <code>ARTICLE_ID</code> <code>str</code> <p>Represents the article_id.</p> <code>ARTICLE_IND</code> <code>str</code> <p>Represents the article_index.</p> <code>QUESTION</code> <code>str</code> <p>Represents the question.</p> <code>LEVEL</code> <code>str</code> <p>Represents the difficulty_level.</p> <code>LIST</code> <code>str</code> <p>Represents the list_number.</p> <code>PARAGRAPH</code> <code>str</code> <p>Represents the paragraph.</p> <code>HAS_PREVIEW</code> <code>str</code> <p>Represents the question_preview.</p> <code>SUBJECT_ID</code> <code>str</code> <p>Represents the participant_id.</p> <code>FINAL_ANSWER</code> <code>str</code> <p>Represents the selected_answer_position.</p> <code>REREAD</code> <code>str</code> <p>Represents the repeated_reading_trial.</p> <code>IA_DATA_IA_ID_COL_NAME</code> <code>str</code> <p>Represents the IA_ID.</p> <code>FIXATION_REPORT_IA_ID_COL_NAME</code> <code>str</code> <p>Represents the CURRENT_FIX_INTEREST_AREA_INDEX.</p> <code>IS_CORRECT</code> <code>str</code> <p>Represents the is_correct.</p> <code>PRACTICE</code> <code>str</code> <p>Represents the practice_trial.</p> Source code in <code>src/configs/constants.py</code> <pre><code>class Fields(StrEnum):\n    \"\"\"\n    Enum for field names in the data.\n\n    Attributes:\n        BATCH (str): Represents the article_batch.\n        PARAGRAPH_ID (str): Represents the paragraph_id.\n        UNIQUE_PARAGRAPH_ID (str): Represents the unique_paragraph_id.\n        ARTICLE_ID (str): Represents the article_id.\n        ARTICLE_IND (str): Represents the article_index.\n        QUESTION (str): Represents the question.\n        LEVEL (str): Represents the difficulty_level.\n        LIST (str): Represents the list_number.\n        PARAGRAPH (str): Represents the paragraph.\n        HAS_PREVIEW (str): Represents the question_preview.\n        SUBJECT_ID (str): Represents the participant_id.\n        FINAL_ANSWER (str): Represents the selected_answer_position.\n        REREAD (str): Represents the repeated_reading_trial.\n        IA_DATA_IA_ID_COL_NAME (str): Represents the IA_ID.\n        FIXATION_REPORT_IA_ID_COL_NAME (str): Represents the CURRENT_FIX_INTEREST_AREA_INDEX.\n        IS_CORRECT (str): Represents the is_correct.\n        PRACTICE (str): Represents the practice_trial.\n    \"\"\"\n\n    UNIQUE_TRIAL_ID = 'unique_trial_id'\n    BATCH = 'article_batch'\n    PARAGRAPH_ID = 'paragraph_id'\n    UNIQUE_PARAGRAPH_ID = 'unique_paragraph_id'\n    ARTICLE_ID = 'article_id'\n    LEVEL = 'difficulty_level'\n    LIST = 'list_number'\n    PARAGRAPH = 'paragraph'\n    HAS_PREVIEW = 'question_preview'\n    SUBJECT_ID = 'participant_id'\n    REREAD = 'repeated_reading_trial'\n    IA_DATA_IA_ID_COL_NAME = 'IA_ID'\n    FIXATION_REPORT_IA_ID_COL_NAME = 'CURRENT_FIX_INTEREST_AREA_INDEX'\n    IS_CORRECT = 'is_correct'\n    PRACTICE = 'practice_trial'\n    QUESTION = 'question'\n</code></pre>"},{"location":"reference/configs/constants/#configs.constants.ItemLevelFeaturesModes","title":"<code>ItemLevelFeaturesModes</code>","text":"<p>               Bases: <code>StrEnum</code></p> <p>Enum for item-level feature modes.</p> <p>Attributes:</p> Source code in <code>src/configs/constants.py</code> <pre><code>class ItemLevelFeaturesModes(StrEnum):\n    \"\"\"\n    Enum for item-level feature modes.\n\n    Attributes:\n    \"\"\"\n\n    RF = 'RF'\n    BEYELSTM = 'BEYELSTM'\n    SVM = 'SVM'\n    LOGISTIC = 'LOGISTIC'\n    READING_SPEED = 'READING_SPEED'\n</code></pre>"},{"location":"reference/configs/constants/#configs.constants.MLModelNames","title":"<code>MLModelNames</code>","text":"<p>               Bases: <code>StrEnum</code></p> <p>Enum for ML model names.</p> <p>Attributes:</p> Name Type Description <code>LOGISTIC_REGRESSION</code> <code>str</code> <p>Represents the logistic regression model.</p> <code>SVM</code> <code>str</code> <p>Represents the support vector machine model.</p> <code>RANDOM_FOREST</code> <code>str</code> <p>Represents the random forest model.</p> <code>DUMMY_CLASSIFIER</code> <code>str</code> <p>Represents the dummy classifier model.</p> <code>XGBOOST</code> <code>str</code> <p>Represents the XGBoost model.</p> <code>LOGISTIC_REGRESSION_REG</code> <code>str</code> <p>Represents the logistic regression regressor.</p> <code>SVM_REG</code> <code>str</code> <p>Represents the support vector regressor.</p> <code>RANDOM_FOREST_REG</code> <code>str</code> <p>Represents the random forest regressor.</p> <code>DUMMY_REGRESSOR</code> <code>str</code> <p>Represents the dummy regressor.</p> <code>XGBOOST_REG</code> <code>str</code> <p>Represents the XGBoost regressor.</p> Source code in <code>src/configs/constants.py</code> <pre><code>class MLModelNames(StrEnum):\n    \"\"\"\n    Enum for ML model names.\n\n    Attributes:\n        LOGISTIC_REGRESSION (str): Represents the logistic regression model.\n        SVM (str): Represents the support vector machine model.\n        RANDOM_FOREST (str): Represents the random forest model.\n        DUMMY_CLASSIFIER (str): Represents the dummy classifier model.\n        XGBOOST (str): Represents the XGBoost model.\n        LOGISTIC_REGRESSION_REG (str): Represents the logistic regression regressor.\n        SVM_REG (str): Represents the support vector regressor.\n        RANDOM_FOREST_REG (str): Represents the random forest regressor.\n        DUMMY_REGRESSOR (str): Represents the dummy regressor.\n        XGBOOST_REG (str): Represents the XGBoost regressor.\n    \"\"\"\n\n    LOGISTIC_REGRESSION = 'LogisticRegressionMLModel'\n    SVM = 'SupportVectorMachineMLModel'\n    RANDOM_FOREST = 'RandomForestMLModel'\n    DUMMY_CLASSIFIER = 'DummyClassifierMLModel'\n    XGBOOST = 'XGBoostMLModel'\n    LINEAR_REG = 'LinearRegressionRegressorMLModel'\n    SVM_REG = 'SupportVectorRegressorMLModel'\n    RANDOM_FOREST_REG = 'RandomForestRegressorMLModel'\n    DUMMY_REGRESSOR = 'DummyRegressorMLModel'\n    XGBOOST_REG = 'XGBoostRegressorMLModel'\n</code></pre>"},{"location":"reference/configs/constants/#configs.constants.MatmulPrecisionLevel","title":"<code>MatmulPrecisionLevel</code>","text":"<p>               Bases: <code>StrEnum</code></p> <p>Enum for matrix multiplication precision levels.</p> <p>Attributes:</p> Name Type Description <code>HIGHEST</code> <code>str</code> <p>Corresponds to \"highest\".</p> <code>HIGH</code> <code>str</code> <p>Corresponds to \"high\".</p> <code>MEDIUM</code> <code>str</code> <p>Corresponds to \"medium\".</p> Source code in <code>src/configs/constants.py</code> <pre><code>class MatmulPrecisionLevel(StrEnum):\n    \"\"\"\n    Enum for matrix multiplication precision levels.\n\n    Attributes:\n        HIGHEST (str): Corresponds to \"highest\".\n        HIGH (str): Corresponds to \"high\".\n        MEDIUM (str): Corresponds to \"medium\".\n    \"\"\"\n\n    HIGHEST = 'highest'\n    HIGH = 'high'\n    MEDIUM = 'medium'\n</code></pre>"},{"location":"reference/configs/constants/#configs.constants.NormalizationModes","title":"<code>NormalizationModes</code>","text":"<p>               Bases: <code>StrEnum</code></p> <p>Enum for normalization modes.</p> <p>Attributes:</p> Name Type Description <code>ALL</code> <code>str</code> <p>Represents the mode where data is normalized based on all trials.</p> <code>TRIAL</code> <code>str</code> <p>Represents the mode where data is normalized based on a trial level.</p> <code>NONE</code> <code>str</code> <p>Represents the mode where no data is normalized.</p> Source code in <code>src/configs/constants.py</code> <pre><code>class NormalizationModes(StrEnum):\n    \"\"\"\n    Enum for normalization modes.\n\n    Attributes:\n        ALL (str): Represents the mode where data is normalized based on all trials.\n        TRIAL (str): Represents the mode where data is normalized based on a trial level.\n        NONE (str): Represents the mode where no data is normalized.\n    \"\"\"\n\n    ALL = 'all'\n    TRIAL = 'trial'\n    NONE = 'none'\n</code></pre>"},{"location":"reference/configs/constants/#configs.constants.Precision","title":"<code>Precision</code>","text":"<p>               Bases: <code>StrEnum</code></p> <p>Enum for precision types.</p> <p>Attributes:</p> Name Type Description <code>SIXTEEN_MIXED</code> <code>str</code> <p>Corresponds to \"16-mixed\".</p> <code>THIRTY_TWO_TRUE</code> <code>str</code> <p>Corresponds to \"32-true\".</p> Source code in <code>src/configs/constants.py</code> <pre><code>class Precision(StrEnum):\n    \"\"\"\n    Enum for precision types.\n\n    Attributes:\n        SIXTEEN_MIXED (str): Corresponds to \"16-mixed\".\n        THIRTY_TWO_TRUE (str): Corresponds to \"32-true\".\n    \"\"\"\n\n    SIXTEEN_MIXED = '16-mixed'\n    THIRTY_TWO_TRUE = '32-true'\n</code></pre>"},{"location":"reference/configs/constants/#configs.constants.PredMode","title":"<code>PredMode</code>","text":"<p>               Bases: <code>StrEnum</code></p> <p>Enum for prediction modes.</p> Source code in <code>src/configs/constants.py</code> <pre><code>class PredMode(StrEnum):\n    \"\"\"\n    Enum for prediction modes.\n    \"\"\"\n\n    RCS = 'RCS'  # Reading Comprehension Skill\n    RC = 'RC'  # Reading Comprehension\n    STD = 'STD'  # Subjective Text Difficulty\n    TYP = 'TYP'  # Typicality\n    CV = 'CV'  # Claim Verification\n    LEX = 'LEX'  # Vocabulary Knowledge\n    DE = 'DE'  # Domain Expertise\n</code></pre>"},{"location":"reference/configs/constants/#configs.constants.RunModes","title":"<code>RunModes</code>","text":"<p>               Bases: <code>StrEnum</code></p> <p>Enum for run modes.</p> <p>Attributes:</p> Name Type Description <code>DEBUG</code> <code>str</code> <p>Represents the debug mode used for debugging the code.</p> <code>FAST_DEV_RUN</code> <code>str</code> <p>Represents the fast development run mode used for quick testing.</p> <code>TRAIN</code> <code>str</code> <p>Represents the train mode used for training the model.</p> Source code in <code>src/configs/constants.py</code> <pre><code>class RunModes(StrEnum):\n    \"\"\"\n    Enum for run modes.\n\n    Attributes:\n        DEBUG (str): Represents the debug mode used for debugging the code.\n        FAST_DEV_RUN (str): Represents the fast development run mode used for quick testing.\n        TRAIN (str): Represents the train mode used for training the model.\n    \"\"\"\n\n    DEBUG = 'debug'\n    FAST_DEV_RUN = 'fast_dev_run'\n    TRAIN = 'train'\n</code></pre>"},{"location":"reference/configs/constants/#configs.constants.Scaler","title":"<code>Scaler</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Enum for scaler types. Each scaler type is associated with a     corresponding scaler class from sklearn.preprocessing.</p> <p>Attributes:</p> Name Type Description <code>MIN_MAX_SCALER</code> <code>type</code> <p>Corresponds to sklearn.preprocessing.MinMaxScaler.</p> <code>ROBUST_SCALER</code> <code>type</code> <p>Corresponds to sklearn.preprocessing.RobustScaler.</p> <code>STANDARD_SCALER</code> <code>type</code> <p>Corresponds to sklearn.preprocessing.StandardScaler.</p> Source code in <code>src/configs/constants.py</code> <pre><code>class Scaler(Enum):\n    \"\"\"\n    Enum for scaler types. Each scaler type is associated with a\n        corresponding scaler class from sklearn.preprocessing.\n\n    Attributes:\n        MIN_MAX_SCALER (type): Corresponds to sklearn.preprocessing.MinMaxScaler.\n        ROBUST_SCALER (type): Corresponds to sklearn.preprocessing.RobustScaler.\n        STANDARD_SCALER (type): Corresponds to sklearn.preprocessing.StandardScaler.\n    \"\"\"\n\n    MIN_MAX_SCALER = MinMaxScaler\n    ROBUST_SCALER = RobustScaler\n    STANDARD_SCALER = StandardScaler\n</code></pre>"},{"location":"reference/configs/constants/#configs.constants.SetNames","title":"<code>SetNames</code>","text":"<p>               Bases: <code>StrEnum</code></p> <p>Enum for set names.</p> <p>Attributes:</p> Name Type Description <code>TRAIN</code> <code>str</code> <p>Represents the training set.</p> <code>VAL</code> <code>str</code> <p>Represents the validation set.</p> <code>TEST</code> <code>str</code> <p>Represents the test set.</p> <code>SEEN_SUBJECT_UNSEEN_ITEM</code> <code>str</code> <p>Represents the seen subject unseen item set.</p> <code>UNSEEN_SUBJECT_SEEN_ITEM</code> <code>str</code> <p>Represents the unseen subject seen item set.</p> <code>UNSEEN_SUBJECT_UNSEEN_ITEM</code> <code>str</code> <p>Represents the unseen subject unseen item set.</p> Source code in <code>src/configs/constants.py</code> <pre><code>class SetNames(StrEnum):\n    \"\"\"\n    Enum for set names.\n\n    Attributes:\n        TRAIN (str): Represents the training set.\n        VAL (str): Represents the validation set.\n        TEST (str): Represents the test set.\n        SEEN_SUBJECT_UNSEEN_ITEM (str): Represents the seen subject unseen item set.\n        UNSEEN_SUBJECT_SEEN_ITEM (str): Represents the unseen subject seen item set.\n        UNSEEN_SUBJECT_UNSEEN_ITEM (str): Represents the unseen subject unseen item set.\n    \"\"\"\n\n    TRAIN = 'train'\n    VAL = 'val'\n    TEST = 'test'\n    SEEN_SUBJECT_UNSEEN_ITEM = 'seen_subject_unseen_item'\n    UNSEEN_SUBJECT_SEEN_ITEM = 'unseen_subject_seen_item'\n    UNSEEN_SUBJECT_UNSEEN_ITEM = 'unseen_subject_unseen_item'\n</code></pre>"},{"location":"reference/configs/constants/#configs.constants.TaskTypes","title":"<code>TaskTypes</code>","text":"<p>               Bases: <code>StrEnum</code></p> <p>Enum for task types.</p> <p>Attributes:</p> Name Type Description <code>BINARY_CLASSIFICATION</code> <code>str</code> <p>Represents binary classification tasks.</p> <code>REGRESSION</code> <code>str</code> <p>Represents regression tasks.</p> Source code in <code>src/configs/constants.py</code> <pre><code>class TaskTypes(StrEnum):\n    \"\"\"\n    Enum for task types.\n\n    Attributes:\n        BINARY_CLASSIFICATION (str): Represents binary classification tasks.\n        REGRESSION (str): Represents regression tasks.\n    \"\"\"\n\n    BINARY_CLASSIFICATION = 'binary_classification'\n    REGRESSION = 'regression'\n</code></pre>"},{"location":"reference/configs/data/","title":"data","text":"<p>Data arguments for the eye tracking data.</p>"},{"location":"reference/configs/data/#configs.data.CopCo","title":"<code>CopCo</code>  <code>dataclass</code>","text":"<p>               Bases: <code>DataArgs</code></p> <p>CopCo data.</p> Source code in <code>src/configs/data.py</code> <pre><code>@register_data\n@dataclass\nclass CopCo(DataArgs):\n    \"\"\"\n    CopCo data.\n    \"\"\"\n\n    split_item_columns: list[str] = field(\n        default_factory=lambda: [\n            'speech_id',\n        ]\n    )\n\n    stratify: str = 'dyslexia'\n    text_source: str = 'Danish Natural Reading Corpus'\n    text_language: str = DatasetLanguage.DANISH\n    text_domain: str = 'News'\n    text_type: str = 'paragraph'\n\n    additional_groupby_columns: list[str] = field(default_factory=lambda: [])\n    tasks: dict[str, str] = field(\n        default_factory=lambda: {\n            PredMode.RCS: 'RCS_score',\n            PredMode.TYP: 'dyslexia',\n        }\n    )\n\n    max_scanpath_length: int = 484\n\n    def __post_init__(self) -&gt; None:\n        super().__post_init__()\n        self.raw_ia_path: Path = (\n            self.base_path / 'precomputed_reading_measures/combined_ia.csv'\n        )\n        self.raw_fixations_path: Path = (\n            self.base_path / 'precomputed_events/combined_fixations.csv'\n        )\n        self.participant_stats_path: Path = (\n            self.base_path / 'labels/participant_stats.csv'\n        )\n        self.stimuli_and_comp_results_path: Path = (\n            self.base_path / 'labels/stimuli_and_comp_results.csv'\n        )\n</code></pre>"},{"location":"reference/configs/data/#configs.data.CopCo_RCS","title":"<code>CopCo_RCS</code>  <code>dataclass</code>","text":"<p>               Bases: <code>CopCo</code></p> <p>CopCo General Reading Comprehension</p> Source code in <code>src/configs/data.py</code> <pre><code>@register_data\n@dataclass\nclass CopCo_RCS(CopCo):\n    \"\"\"\n    CopCo General Reading Comprehension\n    \"\"\"\n\n    task: PredMode = PredMode.RCS\n    target_column: str = 'RCS_score'\n    class_names: list[str] = field(default_factory=lambda: ['score'])\n    # max_seq_len: int = 350\n    max_tokens_in_word: int = 15\n</code></pre>"},{"location":"reference/configs/data/#configs.data.CopCo_TYP","title":"<code>CopCo_TYP</code>  <code>dataclass</code>","text":"<p>               Bases: <code>CopCo</code></p> <p>CopCo Reading Type (Dyslexia vs. Typical)</p> Source code in <code>src/configs/data.py</code> <pre><code>@register_data\n@dataclass\nclass CopCo_TYP(CopCo):\n    \"\"\"\n    CopCo Reading Type (Dyslexia vs. Typical)\n    \"\"\"\n\n    task: PredMode = PredMode.TYP\n    target_column: str = 'dyslexia'\n    class_names: list[str] = field(default_factory=lambda: ['Typical', 'Dyslexia'])\n    # max_seq_len: int = 256\n    max_tokens_in_word: int = 15\n</code></pre>"},{"location":"reference/configs/data/#configs.data.DataArgs","title":"<code>DataArgs</code>  <code>dataclass</code>","text":"<p>A dataclass for storing configuration parameters for handling eye tracking data.</p> <p>Attributes:</p> Name Type Description <code>n_folds</code> <code>int</code> <p>Number of folds for cross-validation.</p> <code>fold_index</code> <code>int</code> <p>Defines the test fold. +1 is validation, rest (out of n_folds) are train.</p> <code>subject_column</code> <code>str</code> <p>Column that defines the subject.</p> <code>unique_item_column</code> <code>str</code> <p>Column that defines an item.</p> <code>ia_query</code> <code>str | None</code> <p>Interest area query for filtering rows.</p> <code>fixation_query</code> <code>str | None</code> <p>Fixation query for filtering rows.</p> <code>split_item_columns</code> <code>list[str]</code> <p>Defines item for train-test split grouping.</p> <code>additional_groupby_columns</code> <code>list[str]</code> <p>Additional columns for grouping data.</p> <code>groupby_columns</code> <code>list[str]</code> <p>Columns used for grouping data. Defined in post_init.</p> <code>stratify</code> <code>str</code> <p>Whether to stratify the data based on the target variable.</p> <code>processed_data_path</code> <code>Path</code> <p>Path to the processed data directory.</p> <code>ia_path</code> <code>Path</code> <p>Path to the interest area report.</p> <code>fixations_path</code> <code>Path</code> <p>Path to the fixation report.</p> <code>all_folds_folder</code> <code>Path</code> <p>Path to the folder containing all folds.</p> <code>folds_folder_name</code> <code>str</code> <p>Name of the folder containing the folds.</p> <code>metadata_path</code> <code>Path</code> <p>Path to the metadata file.</p> <code>higher_level_split</code> <code>str | None</code> <p>Higher level split for the data.</p> <code>base_path</code> <code>Path</code> <p>Base path for the data directory.</p> <code>max_scanpath_length</code> <code>int</code> <p>The maximum scanpath length for the eye input.</p> <code>n_questions_per_item</code> <code>int</code> <p>Number of questions associated with each item.</p> <p>Methods:</p> Name Description <code>__post_init__</code> <p>Initializes the groupby_columns attribute based on the values of other attributes.</p> Source code in <code>src/configs/data.py</code> <pre><code>@dataclass\nclass DataArgs:\n    \"\"\"\n    A dataclass for storing configuration parameters for handling eye tracking data.\n\n    Attributes:\n        n_folds (int): Number of folds for cross-validation.\n        fold_index (int): Defines the test fold. +1 is validation, rest (out of n_folds) are train.\n        subject_column (str): Column that defines the subject.\n        unique_item_column (str): Column that defines an item.\n        ia_query (str | None): Interest area query for filtering rows.\n        fixation_query (str | None): Fixation query for filtering rows.\n        split_item_columns (list[str]): Defines item for train-test split grouping.\n        additional_groupby_columns (list[str]): Additional columns for grouping data.\n        groupby_columns (list[str]): Columns used for grouping data. Defined in __post_init__.\n        stratify (str): Whether to stratify the data based on the target variable.\n        processed_data_path (Path): Path to the processed data directory.\n        ia_path (Path): Path to the interest area report.\n        fixations_path (Path): Path to the fixation report.\n        all_folds_folder (Path): Path to the folder containing all folds.\n        folds_folder_name (str): Name of the folder containing the folds.\n        metadata_path (Path): Path to the metadata file.\n        higher_level_split (str | None): Higher level split for the data.\n        base_path (Path): Base path for the data directory.\n        max_scanpath_length (int): The maximum scanpath length for the eye input.\n        n_questions_per_item (int): Number of questions associated with each item.\n\n    Methods:\n        __post_init__: Initializes the groupby_columns attribute based on the values of other attributes.\n    \"\"\"\n\n    task: PredMode = MISSING\n    n_folds: int = 4\n    n_questions_per_item: int = 0\n    fold_index: int = 0\n    subject_column: str = Fields.SUBJECT_ID\n    unique_item_column: str = Fields.UNIQUE_PARAGRAPH_ID\n    unique_trial_id_column: str = Fields.UNIQUE_TRIAL_ID\n    ia_query: str | None = None\n    fixation_query: str | None = None\n    split_item_columns: list[str | None] = field(\n        default_factory=lambda: [Fields.UNIQUE_PARAGRAPH_ID]\n    )\n\n    additional_groupby_columns: list[str] = field(default_factory=list)\n\n    # Defined in __post_init__ below\n    groupby_columns: list[str] = field(default_factory=list)\n\n    processed_data_path: Path = Path(\n        ''\n    )  # Path to the data directory. Can be used specify a common path for all data files.\n    ia_path: Path = Path('')  # Full path to the interest area report\n    fixations_path: Path = Path('')  # Full path to the fixation report\n    raw_ia_path: Path = Path('')  # Full path to the raw interest area report\n    raw_fixations_path: Path = Path('')\n    trial_level_path: Path = Path('')  # Full path to the trial_level report\n    all_folds_folder: Path = Path('data')\n    folds_folder_name: str = 'folds'\n    metadata_path: Path = Path('')\n    stratify: str | None = None\n    higher_level_split: str | None = None\n    datamodule_name: str = ''\n    base_path: Path = Path('')\n    target_column: str = ''\n    class_names: list[str] = field(default_factory=list)\n    text_source: str = ''\n    text_language: str = ''\n    text_domain: str = ''\n    text_type: str = ''\n    tasks: dict[str, str] = field(default_factory=dict)\n    full_dataset_name: str = ''\n    max_scanpath_length: int = -1\n    max_q_len: int = 0\n    max_seq_len: int = 512  # not including the question\n    max_tokens_in_word = 12\n\n    def __post_init__(self):\n        self.groupby_columns = (\n            [self.unique_item_column, self.subject_column, self.unique_trial_id_column]\n            + self.additional_groupby_columns\n            + list(self.tasks.values())\n        )\n        # Just so they don't get dropped in filtering in preprocess\n        if self.split_item_columns[0] not in self.groupby_columns:\n            self.groupby_columns += self.split_item_columns\n\n        self.datamodule_name = self.dataset_name + 'DataModule'\n        self.base_path = Path('data') / self.dataset_name\n        self.processed_data_path = self.base_path / 'processed'\n        self.ia_path = self.processed_data_path / 'ia.feather'\n        self.fixations_path = self.processed_data_path / 'fixations.feather'\n        self.trial_level_path = self.processed_data_path / 'trial_level.feather'\n\n    @property\n    def dataset_name(self) -&gt; str:\n        return self.__class__.__name__.split('_')[0]\n\n    @property\n    def is_regression(self) -&gt; bool:\n        \"\"\"\n        Determine if the task is regression based on class_names.\n        Regression tasks have a single class name (e.g., ['score'], ['lextale']).\n        Classification tasks have multiple class names (e.g., ['Incorrect', 'Correct']).\n        \"\"\"\n        return len(self.class_names) == 1\n\n    @property\n    def is_english(self) -&gt; bool:\n        \"\"\"\n        Return True if the dataset's text language is English.\n        Handles both DatasetLanguage enum values and string names.\n        \"\"\"\n        if isinstance(self.text_language, DatasetLanguage):\n            return self.text_language == DatasetLanguage.ENGLISH\n        return str(self.text_language).strip().lower() in ('english', 'en')\n</code></pre>"},{"location":"reference/configs/data/#configs.data.DataArgs.is_english","title":"<code>is_english</code>  <code>property</code>","text":"<p>Return True if the dataset's text language is English. Handles both DatasetLanguage enum values and string names.</p>"},{"location":"reference/configs/data/#configs.data.DataArgs.is_regression","title":"<code>is_regression</code>  <code>property</code>","text":"<p>Determine if the task is regression based on class_names. Regression tasks have a single class name (e.g., ['score'], ['lextale']). Classification tasks have multiple class names (e.g., ['Incorrect', 'Correct']).</p>"},{"location":"reference/configs/data/#configs.data.IITBHGC","title":"<code>IITBHGC</code>  <code>dataclass</code>","text":"<p>               Bases: <code>DataArgs</code></p> <p>IITBHGC data.</p> Source code in <code>src/configs/data.py</code> <pre><code>@register_data\n@dataclass\nclass IITBHGC(DataArgs):\n    \"\"\"\n    IITBHGC data.\n    \"\"\"\n\n    text_language: str = DatasetLanguage.ENGLISH\n    stratify: str = 'label'\n    split_item_columns: list[str] = field(\n        default_factory=lambda: [\n            Fields.UNIQUE_PARAGRAPH_ID,\n        ]\n    )\n    tasks: dict[str, str] = field(\n        default_factory=lambda: {\n            PredMode.CV: 'label',\n        }\n    )\n\n    max_scanpath_length: int = 557\n\n    def __post_init__(self) -&gt; None:\n        super().__post_init__()\n        self.raw_ia_path: Path = Path(\n            self.base_path / 'precomputed_events' / 'combined_fixations.csv'\n        )\n        self.raw_fixations_path: Path = (\n            self.base_path / 'precomputed_events' / 'combined_fixations.csv'\n        )\n</code></pre>"},{"location":"reference/configs/data/#configs.data.IITBHGC_CV","title":"<code>IITBHGC_CV</code>  <code>dataclass</code>","text":"<p>               Bases: <code>IITBHGC</code></p> <p>IITBHGC Hallucination Detection</p> Source code in <code>src/configs/data.py</code> <pre><code>@register_data\n@dataclass\nclass IITBHGC_CV(IITBHGC):\n    \"\"\"\n    IITBHGC Hallucination Detection\n    \"\"\"\n\n    task: PredMode = PredMode.CV\n    target_column: str = 'label'\n    class_names: list[str] = field(default_factory=lambda: ['unverified', 'verified'])\n    # max_seq_len: int = 256\n    max_tokens_in_word: int = 12\n</code></pre>"},{"location":"reference/configs/data/#configs.data.MECOL2","title":"<code>MECOL2</code>  <code>dataclass</code>","text":"<p>               Bases: <code>DataArgs</code></p> <p>MECOL2 data.</p> Source code in <code>src/configs/data.py</code> <pre><code>@register_data\n@dataclass\nclass MECOL2(DataArgs):\n    \"\"\"\n    MECOL2 data.\n    \"\"\"\n\n    text_language: str = DatasetLanguage.ENGLISH\n    # can't stratify due to regression label\n    split_item_columns: list[str] = field(\n        default_factory=lambda: [\n            Fields.UNIQUE_PARAGRAPH_ID,\n        ]\n    )\n    tasks: dict[str, str] = field(\n        default_factory=lambda: {\n            PredMode.LEX: 'lextale',\n        }\n    )\n    max_scanpath_length: int = 802\n\n    def __post_init__(self) -&gt; None:\n        super().__post_init__()\n        self.subject_column = 'participant_id'\n        self.raw_ia_path: Path = Path(\n            self.base_path / 'precomputed_reading_measures' / 'combined_ia.csv'\n        )\n        self.raw_fixations_path: Path = (\n            self.base_path / 'precomputed_events' / 'combined_fixations.csv'\n        )\n</code></pre>"},{"location":"reference/configs/data/#configs.data.MECOL2W1","title":"<code>MECOL2W1</code>  <code>dataclass</code>","text":"<p>               Bases: <code>DataArgs</code></p> <p>MECOL2W data.</p> Source code in <code>src/configs/data.py</code> <pre><code>@register_data\n@dataclass\nclass MECOL2W1(DataArgs):\n    \"\"\"\n    MECOL2W data.\n    \"\"\"\n\n    text_language: str = DatasetLanguage.ENGLISH\n    # can't stratify due to regression label\n    split_item_columns: list[str] = field(\n        default_factory=lambda: [\n            Fields.UNIQUE_PARAGRAPH_ID,\n        ]\n    )\n    tasks: dict[str, str] = field(\n        default_factory=lambda: {\n            PredMode.LEX: 'lextale',\n        }\n    )\n    max_scanpath_length: int = 656\n\n    def __post_init__(self) -&gt; None:\n        super().__post_init__()\n        self.subject_column = 'participant_id'\n        self.raw_ia_path: Path = Path(\n            self.base_path / 'precomputed_reading_measures' / 'combined_ia.csv'\n        )\n        self.raw_fixations_path: Path = (\n            self.base_path / 'precomputed_events' / 'combined_fixations.csv'\n        )\n</code></pre>"},{"location":"reference/configs/data/#configs.data.MECOL2W2","title":"<code>MECOL2W2</code>  <code>dataclass</code>","text":"<p>               Bases: <code>DataArgs</code></p> <p>MECOL2W data.</p> Source code in <code>src/configs/data.py</code> <pre><code>@register_data\n@dataclass\nclass MECOL2W2(DataArgs):\n    \"\"\"\n    MECOL2W data.\n    \"\"\"\n\n    text_language: str = DatasetLanguage.ENGLISH\n    # can't stratify due to regression label\n    split_item_columns: list[str] = field(\n        default_factory=lambda: [\n            Fields.UNIQUE_PARAGRAPH_ID,\n        ]\n    )\n    tasks: dict[str, str] = field(\n        default_factory=lambda: {\n            PredMode.LEX: 'lextale',\n        }\n    )\n\n    max_scanpath_length: int = 802\n\n    def __post_init__(self) -&gt; None:\n        super().__post_init__()\n        self.subject_column = 'participant_id'\n        self.unique_item_column = 'unique_trial_id'\n        self.raw_ia_path: Path = Path(\n            self.base_path / 'precomputed_reading_measures' / 'combined_ia.csv'\n        )\n        self.raw_fixations_path: Path = (\n            self.base_path / 'precomputed_events' / 'combined_fixations.csv'\n        )\n</code></pre>"},{"location":"reference/configs/data/#configs.data.MECOL2_LEX","title":"<code>MECOL2_LEX</code>  <code>dataclass</code>","text":"<p>               Bases: <code>MECOL2</code></p> <p>MECOL2 Text Reading Comprehension</p> Source code in <code>src/configs/data.py</code> <pre><code>@register_data\n@dataclass\nclass MECOL2_LEX(MECOL2):\n    \"\"\"\n    MECOL2 Text Reading Comprehension\n    \"\"\"\n\n    task: PredMode = PredMode.LEX\n    target_column: str = 'lextale'\n    stratify: str = 'lextale'\n    class_names: list[str] = field(default_factory=lambda: ['lextale'])\n    max_tokens_in_word: int = 6\n</code></pre>"},{"location":"reference/configs/data/#configs.data.OneStop","title":"<code>OneStop</code>  <code>dataclass</code>","text":"<p>               Bases: <code>DataArgs</code></p> <p>OneStop data.</p> Source code in <code>src/configs/data.py</code> <pre><code>@register_data\n@dataclass\nclass OneStop(DataArgs):\n    \"\"\"\n    OneStop data.\n    \"\"\"\n\n    n_folds: int = 10\n    split_item_columns: list[str] = field(\n        default_factory=lambda: [\n            Fields.BATCH,\n            Fields.ARTICLE_ID,\n        ]\n    )\n    ia_query: str = 'practice_trial==False &amp; question_preview==False &amp; repeated_reading_trial==False'\n    fixation_query: str = 'practice_trial==False &amp; question_preview==False &amp; repeated_reading_trial==False'\n    stratify: str = Fields.IS_CORRECT\n    tasks: dict[str, str] = field(\n        default_factory=lambda: {\n            PredMode.RC: Fields.IS_CORRECT,\n        }\n    )\n    higher_level_split: str | None = Fields.BATCH\n    text_source: str = 'Guardian Articles'\n    text_language: str = DatasetLanguage.ENGLISH\n    text_domain: str = 'News'\n    text_type: str = 'paragraph'\n\n    # Not really used, kept for eval\n    additional_groupby_columns: list[str] = field(\n        default_factory=lambda: [\n            Fields.LIST,\n            Fields.HAS_PREVIEW,\n            Fields.REREAD,\n        ]\n    )\n\n    max_scanpath_length: int = 815\n    n_questions_per_item: int = 1\n\n    def __post_init__(self) -&gt; None:\n        super().__post_init__()\n        self.raw_ia_path: Path = (\n            self.base_path / 'precomputed_reading_measures' / 'ia_Paragraph.csv'\n        )\n        self.raw_fixations_path: Path = (\n            self.base_path / 'precomputed_events' / 'fixations_Paragraph.csv'\n        )\n        self.trial_level_paragraphs_path = (\n            self.base_path / 'additional_raw' / 'trial_level_paragraphs.csv'\n        )\n        self.onestopqa_path = self.base_path / 'additional_raw' / 'onestop_qa.json'\n</code></pre>"},{"location":"reference/configs/data/#configs.data.OneStop_RC","title":"<code>OneStop_RC</code>  <code>dataclass</code>","text":"<p>               Bases: <code>OneStop</code></p> <p>OneStop Is Correct</p> Source code in <code>src/configs/data.py</code> <pre><code>@register_data\n@dataclass\nclass OneStop_RC(OneStop):\n    \"\"\"\n    OneStop Is Correct\n    \"\"\"\n\n    task: PredMode = PredMode.RC\n    target_column: str = Fields.IS_CORRECT\n    class_names: list[str] = field(default_factory=lambda: ['Incorrect', 'Correct'])\n\n    max_q_len: int = 30\n    # max_seq_len: int = 280\n    max_tokens_in_word: int = 10\n</code></pre>"},{"location":"reference/configs/data/#configs.data.PoTeC","title":"<code>PoTeC</code>  <code>dataclass</code>","text":"<p>               Bases: <code>DataArgs</code></p> <p>PoTeC data.</p> Source code in <code>src/configs/data.py</code> <pre><code>@register_data\n@dataclass\nclass PoTeC(DataArgs):\n    \"\"\"\n    PoTeC data.\n    \"\"\"\n\n    text_source: str = 'German Physics &amp; Biology Textbooks'\n    text_language: str = DatasetLanguage.GERMAN\n    text_domain: str = 'Science Education'\n    text_type: str = 'paragraph'\n    stratify: str = 'DE_RC'\n\n    split_item_columns: list[str] = field(\n        default_factory=lambda: [\n            Fields.UNIQUE_PARAGRAPH_ID,\n        ]\n    )\n    tasks: dict[str, str] = field(\n        default_factory=lambda: {\n            PredMode.RC: 'RC',\n            PredMode.DE: 'DE',\n        }\n    )\n\n    additional_groupby_columns: list[str] = field(\n        default_factory=lambda: [\n            'question',\n            'DE_RC',\n        ]\n    )\n    n_questions_per_item: int = 3\n    max_scanpath_length: int = 1483\n\n    def __post_init__(self) -&gt; None:\n        super().__post_init__()\n        self.raw_ia_path: Path = Path(\n            self.base_path / 'precomputed_reading_measures' / 'combined_ia.csv'\n        )\n        self.raw_fixations_path: Path = (\n            self.base_path / 'precomputed_events' / 'combined_fixations.csv'\n        )\n</code></pre>"},{"location":"reference/configs/data/#configs.data.PoTeC_DE","title":"<code>PoTeC_DE</code>  <code>dataclass</code>","text":"<p>               Bases: <code>PoTeC</code></p> <p>PoTeC Background Knowledge</p> Source code in <code>src/configs/data.py</code> <pre><code>@register_data\n@dataclass\nclass PoTeC_DE(PoTeC):\n    \"\"\"\n    PoTeC Background Knowledge\n    \"\"\"\n\n    task: PredMode = PredMode.DE\n    target_column: str = 'DE'\n    class_names: list[str] = field(default_factory=lambda: ['Low', 'High'])\n    # max_seq_len: int = 512\n    max_tokens_in_word: int = 12\n</code></pre>"},{"location":"reference/configs/data/#configs.data.PoTeC_RC","title":"<code>PoTeC_RC</code>  <code>dataclass</code>","text":"<p>               Bases: <code>PoTeC</code></p> <p>PoTeC Text Reading Comprehension</p> Source code in <code>src/configs/data.py</code> <pre><code>@register_data\n@dataclass\nclass PoTeC_RC(PoTeC):\n    \"\"\"\n    PoTeC Text Reading Comprehension\n    \"\"\"\n\n    task: PredMode = PredMode.RC\n    target_column: str = 'RC'\n    class_names: list[str] = field(default_factory=lambda: ['Incorrect', 'Correct'])\n    max_q_len: int = 40\n    # max_seq_len: int = 350\n    max_tokens_in_word: int = 12\n</code></pre>"},{"location":"reference/configs/data/#configs.data.SBSAT","title":"<code>SBSAT</code>  <code>dataclass</code>","text":"<p>               Bases: <code>DataArgs</code></p> <p>SBSAT data.</p> Source code in <code>src/configs/data.py</code> <pre><code>@register_data\n@dataclass\nclass SBSAT(DataArgs):\n    \"\"\"\n    SBSAT data.\n    \"\"\"\n\n    text_source: str = 'SAT Reading Passages'\n    text_language: str = DatasetLanguage.ENGLISH\n    text_domain: str = 'Education'\n    text_type: str = 'paragraph'\n    stratify: str = 'RC'\n    tasks: dict[str, str] = field(\n        default_factory=lambda: {\n            PredMode.RC: 'RC',\n            PredMode.STD: 'difficulty',\n        }\n    )\n    max_scanpath_length: int = 1240\n    n_questions_per_item: int = 5\n    max_seq_len: int = 740\n\n    def __post_init__(self) -&gt; None:\n        super().__post_init__()\n        self.raw_ia_dir: Path = Path(self.base_path / 'stimuli')\n        self.raw_ia_path: Path = Path(\n            self.base_path / 'stimuli/' / 'combined_stimulus.csv'\n        )\n        self.raw_fixations_path: Path = (\n            self.base_path / 'precomputed_events/18sat_fixfinal.csv'\n        )\n</code></pre>"},{"location":"reference/configs/data/#configs.data.SBSAT_RC","title":"<code>SBSAT_RC</code>  <code>dataclass</code>","text":"<p>               Bases: <code>SBSAT</code></p> <p>SBSAT Text Reading Comprehension</p> Source code in <code>src/configs/data.py</code> <pre><code>@register_data\n@dataclass\nclass SBSAT_RC(SBSAT):\n    \"\"\"\n    SBSAT Text Reading Comprehension\n    \"\"\"\n\n    task: PredMode = PredMode.RC\n    target_column: str = 'RC'\n    class_names: list[str] = field(default_factory=lambda: ['Incorrect', 'Correct'])\n    max_q_len: int = 55\n    max_tokens_in_word: int = 12\n</code></pre>"},{"location":"reference/configs/data/#configs.data.SBSAT_STD","title":"<code>SBSAT_STD</code>  <code>dataclass</code>","text":"<p>               Bases: <code>SBSAT</code></p> <p>SBSAT Subjective Difficulty</p> Source code in <code>src/configs/data.py</code> <pre><code>@register_data\n@dataclass\nclass SBSAT_STD(SBSAT):\n    \"\"\"\n    SBSAT Subjective Difficulty\n    \"\"\"\n\n    task: PredMode = PredMode.STD\n    target_column: str = 'difficulty'\n    class_names: list[str] = field(default_factory=lambda: ['difficulty'])\n    max_tokens_in_word: int = 12\n</code></pre>"},{"location":"reference/configs/data/#configs.data.get_data_args","title":"<code>get_data_args(class_name)</code>","text":"<p>Get the data path arguments class by its name.</p> <p>Parameters:</p> Name Type Description Default <code>class_name</code> <code>str</code> <p>The name of the class.</p> required <p>Returns:</p> Name Type Description <code>DataArgs</code> <code>DataArgs | None</code> <p>An instance of the requested class.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the class name is not found.</p> Source code in <code>src/configs/data.py</code> <pre><code>def get_data_args(class_name: str) -&gt; DataArgs | None:\n    \"\"\"\n    Get the data path arguments class by its name.\n\n    Args:\n        class_name (str): The name of the class.\n\n    Returns:\n        DataArgs: An instance of the requested class.\n\n    Raises:\n        ValueError: If the class name is not found.\n    \"\"\"\n    try:\n        return globals()[class_name]()\n    except KeyError:\n        logger.error(f\"Class '{class_name}' not found in src/configs/data.py.\")\n        return None\n</code></pre>"},{"location":"reference/configs/main_config/","title":"main_config","text":"<p>Configuration file for the model, trainer, data paths and data.</p>"},{"location":"reference/configs/main_config/#configs.main_config.Args","title":"<code>Args</code>  <code>dataclass</code>","text":"<p>Configuration class for the model, trainer, data paths, and data.</p> <p>Attributes:</p> Name Type Description <code>model</code> <code>BaseModelArgs</code> <p>Configuration for the model.</p> <code>trainer</code> <code>BaseTrainer</code> <p>Configuration for the trainer.</p> <code>data</code> <code>DataArgs</code> <p>Configuration for the data.</p> <code>eval_path</code> <code>str | None</code> <p>Path for evaluation.</p> <code>hydra</code> <code>Any</code> <p>Configuration for Hydra.</p> Source code in <code>src/configs/main_config.py</code> <pre><code>@dataclass\nclass Args:\n    \"\"\"\n    Configuration class for the model, trainer, data paths, and data.\n\n    Attributes:\n        model (BaseModelArgs): Configuration for the model.\n        trainer (BaseTrainer): Configuration for the trainer.\n        data (DataArgs): Configuration for the data.\n        eval_path (str | None): Path for evaluation.\n        hydra (Any): Configuration for Hydra.\n    \"\"\"\n\n    model: BaseModelArgs = field(default_factory=BaseModelArgs)\n    trainer: BaseTrainer = field(default_factory=BaseTrainer)\n    data: DataArgs = field(default_factory=DataArgs)\n    eval_path: str | None = None\n    # https://hydra.cc/docs/1.3/configure_hydra/workdir/\n    hydra: Any = field(\n        default_factory=lambda: {\n            'run': {\n                'dir': 'outputs/${hydra:job.override_dirname}/fold_index=${data.fold_index}',\n            },\n            'sweep': {\n                'dir': 'cross_validation_runs',\n                # https://github.com/facebookresearch/hydra/issues/1786#issuecomment-1017005470\n                'subdir': '${hydra:job.override_dirname}/fold_index=${data.fold_index}',\n            },\n            'job': {\n                'config': {\n                    'override_dirname': {\n                        # Don't include fold_index and devices in the directory name\n                        'exclude_keys': [\n                            'data.fold_index',\n                            'trainer.devices',\n                        ],\n                    },\n                },\n            },\n        },\n    )\n</code></pre>"},{"location":"reference/configs/main_config/#configs.main_config.get_model","title":"<code>get_model(cfg)</code>","text":"<p>Returns a model based on the model name.</p> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <code>Args</code> <p>Configuration object containing model parameters.</p> required <p>Returns:</p> Name Type Description <code>BaseModel</code> <code>BaseModel | BaseMLModel</code> <p>An instance of the model class.</p> Source code in <code>src/configs/main_config.py</code> <pre><code>def get_model(cfg: Args) -&gt; BaseModel | BaseMLModel:\n    \"\"\"\n    Returns a model based on the model name.\n\n    Args:\n        cfg (Args): Configuration object containing model parameters.\n\n    Returns:\n        BaseModel: An instance of the model class.\n    \"\"\"\n\n    model_class = ModelFactory.get(cfg.model.base_model_name)\n    model = model_class(\n        trainer_args=cfg.trainer,\n        model_args=cfg.model,\n        data_args=cfg.data,\n    )\n\n    if getattr(cfg.trainer, 'use_torch_compile', False):\n        logger.info('Using torch.compile')\n        model = torch.compile(\n            model,\n            mode='reduce-overhead',\n        )\n\n    return model\n</code></pre>"},{"location":"reference/configs/trainers/","title":"trainers","text":"<p>This module contains dataclasses for configuring trainers.</p> <p>The module defines a hierarchy of configuration classes: - <code>BaseTrainer</code>: The root configuration class with common attributes - <code>TrainerDL</code>: Configuration for deep learning trainers, inheriting from BaseTrainer - <code>TrainerML</code>: Configuration for machine learning trainers, inheriting from BaseTrainer</p> <p>Each configuration class is defined using the <code>@dataclass</code> decorator and specifies the relevant attributes and their default values.</p> <p>The <code>@register_config</code> decorator is used to register the configuration classes with a specific group defined by <code>ConfigName.TRAINER</code>.</p>"},{"location":"reference/configs/trainers/#configs.trainers.BaseTrainer","title":"<code>BaseTrainer</code>  <code>dataclass</code>","text":"<p>Base configuration class for all trainers.</p> <p>This class defines common attributes shared by both deep learning and machine learning trainers.</p> <p>Attributes:</p> Name Type Description <code>num_workers</code> <code>int</code> <p>Number of worker processes for data loading. Default is 4.</p> <code>profiler</code> <code>str | None</code> <p>Profiler to use ('simple', 'advanced', or None). Default is None.</p> <code>precision</code> <code>Precision</code> <p>Numerical precision for training. Default is Precision.THIRTY_TWO_TRUE.</p> <code>float32_matmul_precision</code> <code>MatmulPrecisionLevel</code> <p>Matrix multiplication precision level. Default is MatmulPrecisionLevel.HIGH.</p> <code>seed</code> <code>int</code> <p>Random seed for reproducibility. Default is 42.</p> <code>devices</code> <code>Any</code> <p>Device configuration for training. Default is 1.</p> <code>run_mode</code> <code>RunModes</code> <p>Mode for running the trainer (e.g., 'train', 'test', 'debug'). Default is RunModes.TRAIN.</p> <code>wandb_job_type</code> <code>str</code> <p>Type of job for Weights &amp; Biases logging. Default is \"MISSING\".</p> <code>wandb_project</code> <code>str</code> <p>Weights &amp; Biases project name. Default is \"reading-comprehension-from-eye-movements\".</p> <code>wandb_entity</code> <code>str</code> <p>Weights &amp; Biases entity name. Default is \"EyeRead\".</p> <code>wandb_notes</code> <code>str</code> <p>Additional notes for Weights &amp; Biases logging. Default is an empty string.</p> <code>overwrite_data</code> <code>bool</code> <p>If True, overwrites the relevant TextDataSet and ETDataset. features even if they exist.</p> Source code in <code>src/configs/trainers.py</code> <pre><code>@dataclass\nclass BaseTrainer:\n    \"\"\"\n    Base configuration class for all trainers.\n\n    This class defines common attributes shared by both deep learning and machine learning trainers.\n\n    Attributes:\n        num_workers (int): Number of worker processes for data loading. Default is 4.\n        profiler (str | None): Profiler to use ('simple', 'advanced', or None). Default is None.\n        precision (Precision): Numerical precision for training.\n            Default is Precision.THIRTY_TWO_TRUE.\n        float32_matmul_precision (MatmulPrecisionLevel): Matrix multiplication precision level.\n            Default is MatmulPrecisionLevel.HIGH.\n        seed (int): Random seed for reproducibility. Default is 42.\n        devices (Any): Device configuration for training. Default is 1.\n        run_mode (RunModes): Mode for running the trainer (e.g., 'train', 'test', 'debug').\n            Default is RunModes.TRAIN.\n        wandb_job_type (str): Type of job for Weights &amp; Biases logging. Default is \"MISSING\".\n        wandb_project (str): Weights &amp; Biases project name.\n            Default is \"reading-comprehension-from-eye-movements\".\n        wandb_entity (str): Weights &amp; Biases entity name. Default is \"EyeRead\".\n        wandb_notes (str): Additional notes for Weights &amp; Biases logging.\n            Default is an empty string.\n        overwrite_data (bool): If True, overwrites the relevant TextDataSet and ETDataset.\n            features even if they exist.\n    \"\"\"\n\n    num_workers: int = 4\n    profiler: str | None = None\n    precision: Precision = Precision.THIRTY_TWO_TRUE\n    float32_matmul_precision: MatmulPrecisionLevel = MatmulPrecisionLevel.HIGH\n    seed: int = 42\n    devices: Any = 1\n    run_mode: RunModes = RunModes.TRAIN\n    wandb_job_type: str = 'MISSING'\n    wandb_project: str = 'EyeBench'\n    wandb_entity: str = 'EyeRead'\n    wandb_notes: str = ''\n    sample_m_per_class: bool = False\n    samples_per_epoch: int | None = None\n    overwrite_data: bool = False\n\n    def __post_init__(self):\n        \"\"\"\n        Post-initialization hook to adjust attributes based on the run mode.\n\n        If the run mode is set to 'debug', the number of workers is set to 0\n        and the Weights &amp; Biases job type is set to \"debug\".\n        \"\"\"\n        if self.run_mode == RunModes.DEBUG:\n            self.num_workers = 0\n            self.wandb_job_type = 'debug'\n\n        assert self.sample_m_per_class is False or (\n            self.samples_per_epoch is not None\n        ), 'samples_per_epoch must be set if sample_m_per_class is True'\n</code></pre>"},{"location":"reference/configs/trainers/#configs.trainers.BaseTrainer.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Post-initialization hook to adjust attributes based on the run mode.</p> <p>If the run mode is set to 'debug', the number of workers is set to 0 and the Weights &amp; Biases job type is set to \"debug\".</p> Source code in <code>src/configs/trainers.py</code> <pre><code>def __post_init__(self):\n    \"\"\"\n    Post-initialization hook to adjust attributes based on the run mode.\n\n    If the run mode is set to 'debug', the number of workers is set to 0\n    and the Weights &amp; Biases job type is set to \"debug\".\n    \"\"\"\n    if self.run_mode == RunModes.DEBUG:\n        self.num_workers = 0\n        self.wandb_job_type = 'debug'\n\n    assert self.sample_m_per_class is False or (\n        self.samples_per_epoch is not None\n    ), 'samples_per_epoch must be set if sample_m_per_class is True'\n</code></pre>"},{"location":"reference/configs/trainers/#configs.trainers.TrainerDL","title":"<code>TrainerDL</code>  <code>dataclass</code>","text":"<p>               Bases: <code>BaseTrainer</code></p> <p>Configuration class for deep learning trainers.</p> <p>Inherits from BaseTrainer and adds specific attributes for deep learning models.</p> <p>Attributes:</p> Name Type Description <code>learning_rate</code> <code>float</code> <p>Optimizer learning rate. Must be specified by derived classes.</p> <code>gradient_clip_val</code> <code>float | None</code> <p>Gradient clipping value. Default is None.</p> <code>accelerator</code> <code>Accelerators</code> <p>Accelerator to use (e.g., 'cpu', 'gpu', 'tpu'). Default is Accelerators.AUTO.</p> <code>log_gradients</code> <code>bool</code> <p>Whether to log gradients. Default is False.</p> <code>optimize_for_loss</code> <code>bool</code> <p>Whether to optimize for loss instead of metrics. Default is True.</p> Source code in <code>src/configs/trainers.py</code> <pre><code>@register_trainer\n@dataclass\nclass TrainerDL(BaseTrainer):\n    \"\"\"\n    Configuration class for deep learning trainers.\n\n    Inherits from BaseTrainer and adds specific attributes for deep learning models.\n\n    Attributes:\n        learning_rate (float): Optimizer learning rate. Must be specified by derived classes.\n        gradient_clip_val (float | None): Gradient clipping value. Default is None.\n        accelerator (Accelerators): Accelerator to use (e.g., 'cpu', 'gpu', 'tpu').\n            Default is Accelerators.AUTO.\n        log_gradients (bool): Whether to log gradients. Default is False.\n        optimize_for_loss (bool): Whether to optimize for loss instead of metrics. Default is True.\n    \"\"\"\n\n    learning_rate: float = 0.0003\n    gradient_clip_val: float | None = 1.0\n    accelerator: Accelerators = Accelerators.AUTO\n    log_gradients: bool = False\n    optimize_for_loss: bool = True\n    use_torch_compile: bool = False\n</code></pre>"},{"location":"reference/configs/trainers/#configs.trainers.TrainerML","title":"<code>TrainerML</code>  <code>dataclass</code>","text":"<p>               Bases: <code>BaseTrainer</code></p> <p>Configuration class for machine learning trainers. Inherits from BaseTrainer and adds specific attributes for machine learning models.</p> Source code in <code>src/configs/trainers.py</code> <pre><code>@register_trainer\n@dataclass\nclass TrainerML(BaseTrainer):\n    \"\"\"\n    Configuration class for machine learning trainers.\n    Inherits from BaseTrainer and adds specific attributes for machine learning models.\n    \"\"\"\n</code></pre>"},{"location":"reference/configs/utils/","title":"utils","text":"<p>@file: utils.py @description: Utility functions for registering configuration classes with Hydra's ConfigStore.</p>"},{"location":"reference/configs/utils/#configs.utils.register_config","title":"<code>register_config(group)</code>","text":"<p>Decorator to register a configuration class with Hydra's ConfigStore.</p> <p>Parameters:</p> Name Type Description Default <code>group</code> <code>ConfigName</code> <p>The group name to register the configuration class under.</p> required <p>Returns:</p> Name Type Description <code>Callable</code> <code>Callable</code> <p>The decorator function that registers the class.</p> Source code in <code>src/configs/utils.py</code> <pre><code>def register_config(group: ConfigName) -&gt; Callable:\n    \"\"\"\n    Decorator to register a configuration class with Hydra's ConfigStore.\n\n    Args:\n        group (ConfigName): The group name to register the configuration class under.\n\n    Returns:\n        Callable: The decorator function that registers the class.\n    \"\"\"\n\n    def decorator(cls):\n        cs = ConfigStore.instance()\n        cs.store(\n            group=group.value,\n            name=cls.__name__,\n            node=cls,\n        )\n        return cls\n\n    return decorator\n</code></pre>"},{"location":"reference/configs/utils/#configs.utils.register_data","title":"<code>register_data(cls)</code>","text":"<p>Wrapper to register a data configuration class with the specified group.</p> <p>Parameters:</p> Name Type Description Default <code>cls</code> <code>type</code> <p>The data configuration class to register.</p> required <p>Returns:</p> Name Type Description <code>type</code> <code>type[T]</code> <p>The registered data configuration class.</p> Source code in <code>src/configs/utils.py</code> <pre><code>def register_data(cls: type[T]) -&gt; type[T]:\n    \"\"\"\n    Wrapper to register a data configuration class with the specified group.\n\n    Args:\n        cls (type): The data configuration class to register.\n\n    Returns:\n        type: The registered data configuration class.\n    \"\"\"\n    return register_config(group=ConfigName.DATA)(cls)\n</code></pre>"},{"location":"reference/configs/utils/#configs.utils.register_model_config","title":"<code>register_model_config(cls)</code>","text":"<p>Wrapper to register a model configuration class with the specified group.</p> <p>Parameters:</p> Name Type Description Default <code>cls</code> <code>type</code> <p>The model configuration class to register.</p> required <p>Returns:</p> Name Type Description <code>type</code> <code>type[T]</code> <p>The registered model configuration class.</p> Source code in <code>src/configs/utils.py</code> <pre><code>def register_model_config(cls: type[T]) -&gt; type[T]:\n    \"\"\"\n    Wrapper to register a model configuration class with the specified group.\n\n    Args:\n        cls (type): The model configuration class to register.\n\n    Returns:\n        type: The registered model configuration class.\n    \"\"\"\n    return register_config(group=ConfigName.MODEL)(cls)\n</code></pre>"},{"location":"reference/configs/utils/#configs.utils.register_trainer","title":"<code>register_trainer(cls)</code>","text":"<p>Wrapper to register a trainer configuration class with the specified group.</p> <p>Parameters:</p> Name Type Description Default <code>cls</code> <code>type</code> <p>The trainer configuration class to register.</p> required <p>Returns:</p> Name Type Description <code>type</code> <code>type[T]</code> <p>The registered trainer configuration class.</p> Source code in <code>src/configs/utils.py</code> <pre><code>def register_trainer(cls: type[T]) -&gt; type[T]:\n    \"\"\"\n    Wrapper to register a trainer configuration class with the specified group.\n\n    Args:\n        cls (type): The trainer configuration class to register.\n\n    Returns:\n        type: The registered trainer configuration class.\n    \"\"\"\n    return register_config(group=ConfigName.TRAINER)(cls)\n</code></pre>"},{"location":"reference/configs/models/__init__/","title":"init","text":""},{"location":"reference/configs/models/base_model/","title":"base_model","text":"<p>This module contains dataclasses for defining model arguments and parameters.</p> <p>The module defines a hierarchy of configuration classes: - Common base configuration classes for all models (BaseModelParams, CommonBaseModelArgs) - Specialized configuration classes for deep learning models (BaseModelArgs) - Specialized configuration classes for machine learning models (MLModelArgs)</p>"},{"location":"reference/configs/models/base_model/#configs.models.base_model.BaseModelArgs","title":"<code>BaseModelArgs</code>  <code>dataclass</code>","text":"<p>Common base configuration class for model arguments shared by all models.</p> <p>This class contains the shared parameters between deep learning and machine learning models.</p> <p>Attributes:</p> Name Type Description <code>batch_size</code> <code>int</code> <p>The batch size for training.</p> <code>text_dim</code> <code>int</code> <p>The dimension of the text input.</p> <code>max_supported_seq_len</code> <code>int</code> <p>The maximum sequence length for the eye input, in tokens.</p> <code>use_fixation_report</code> <code>bool</code> <p>Whether to use fixation report.</p> <code>eyes_dim</code> <code>int | None</code> <p>The dimension of the eye features. Defined according to ia_features.</p> <code>fixation_dim</code> <code>int | None</code> <p>The dimension of the fixation features. Defined according to fixation_features + ia_features</p> <code>feature_mode</code> <code>FeatureMode</code> <p>The mode for features.</p> <code>word_features</code> <code>list[str]</code> <p>List of word features.</p> <code>eye_features</code> <code>list[str]</code> <p>List of eye features.</p> <code>ia_features</code> <code>list[str]</code> <p>List of item-level features.</p> <code>fixation_features</code> <code>list[str]</code> <p>List of fixation features.</p> <code>ia_categorical_features</code> <code>list[str]</code> <p>List of categorical features.</p> <code>compute_trial_level_features</code> <code>bool</code> <p>Whether to compute trial-level features.</p> <code>n_tokens</code> <code>int</code> <p>Number of tokens.</p> <code>eye_token_id</code> <code>int</code> <p>Eye token ID.</p> <code>sep_token_id</code> <code>int</code> <p>Separator token ID.</p> <code>is_training</code> <code>bool</code> <p>Whether the model is in training mode.</p> <code>normalization_mode</code> <code>NormalizationModes</code> <p>Mode for normalization.</p> <code>normalization_type</code> <code>Scaler</code> <p>Type of scaler for normalization.</p> <code>class_weights</code> <code>list[float] | None</code> <p>Weights for each class in the loss function. None means equal weight for all classes. Default is None.</p> <code>prepend_eye_features_to_text</code> <code>bool</code> <p>A flag indicating whether to prepend the eye data to the input. If True, the eye data will be added at the beginning of the input. Default is False.</p> <code>item_level_features_modes</code> <code>list[ItemLevelFeaturesModes]</code> <p>Modes for item-level features.</p> <code>is_ml</code> <code>bool</code> <p>Whether the model is a machine learning model or deep learning model.</p> Source code in <code>src/configs/models/base_model.py</code> <pre><code>@dataclass\nclass BaseModelArgs:\n    \"\"\"\n    Common base configuration class for model arguments shared by all models.\n\n    This class contains the shared parameters between deep learning and machine learning models.\n\n    Attributes:\n        batch_size (int): The batch size for training.\n        text_dim (int): The dimension of the text input.\n        max_supported_seq_len (int): The maximum sequence length for the eye input, in tokens.\n\n        use_fixation_report (bool): Whether to use fixation report.\n        eyes_dim (int | None): The dimension of the eye features. Defined according to ia_features.\n        fixation_dim (int | None): The dimension of the fixation features.\n            Defined according to fixation_features + ia_features\n        feature_mode (FeatureMode): The mode for features.\n        word_features (list[str]): List of word features.\n        eye_features (list[str]): List of eye features.\n        ia_features (list[str]): List of item-level features.\n        fixation_features (list[str]): List of fixation features.\n        ia_categorical_features (list[str]): List of categorical features.\n        compute_trial_level_features (bool): Whether to compute trial-level features.\n        n_tokens (int): Number of tokens.\n        eye_token_id (int): Eye token ID.\n        sep_token_id (int): Separator token ID.\n        is_training (bool): Whether the model is in training mode.\n        normalization_mode (NormalizationModes): Mode for normalization.\n        normalization_type (Scaler): Type of scaler for normalization.\n        class_weights: Weights for each class in the loss function.\n            None means equal weight for all classes. Default is None.\n        prepend_eye_features_to_text: A flag indicating whether to prepend the eye data to the input.\n            If True, the eye data will be added at the beginning of the input. Default is False.\n        item_level_features_modes: Modes for item-level features.\n        is_ml (bool): Whether the model is a machine learning model or deep learning model.\n    \"\"\"\n\n    use_class_weighted_loss: bool = True\n    class_weights: list[float] | None = None  # if use_class_weighted_loss else None\n    prepend_eye_features_to_text: bool = False\n    item_level_features_modes: list[ItemLevelFeaturesModes] = field(\n        default_factory=list\n    )\n\n    batch_size: int = MISSING\n    text_dim: int = -1\n    # query to filter longest: (participant_id != 'l31_388' | unique_paragraph_id != '3_1_Adv_4')\n    use_fixation_report: bool = MISSING\n    max_tokens_in_word: int = 15\n    eyes_dim: int = MISSING\n    fixation_dim: int = MISSING\n    use_eyes_only: bool = False\n    is_ml: bool = False\n    num_special_tokens_add: int = MISSING\n\n    feature_mode: FeatureMode = FeatureMode.EYES_WORDS\n    word_features: list[str] = field(\n        default_factory=lambda: [\n            'gpt2_surprisal',\n            'wordfreq_frequency',\n            'word_length',\n            'start_of_line',\n            'end_of_line',\n            'is_content_word',\n            'ptb_pos',\n            'left_dependents_count',\n            'right_dependents_count',\n            'distance_to_head',\n        ]\n    )\n    eye_features: list[str] = field(\n        default_factory=lambda: [\n            'IA_DWELL_TIME',\n            'IA_DWELL_TIME_%',\n            'IA_FIXATION_%',\n            'IA_FIXATION_COUNT',\n            'IA_REGRESSION_IN_COUNT',\n            'IA_REGRESSION_OUT_FULL_COUNT',\n            'IA_RUN_COUNT',\n            'IA_FIRST_FIXATION_DURATION',\n            'IA_FIRST_FIXATION_VISITED_IA_COUNT',\n            'IA_FIRST_RUN_DWELL_TIME',\n            'IA_FIRST_RUN_FIXATION_COUNT',\n            'IA_SKIP',\n            'IA_REGRESSION_PATH_DURATION',\n            'IA_REGRESSION_OUT_COUNT',\n            'IA_SELECTIVE_REGRESSION_PATH_DURATION',\n            'IA_LAST_FIXATION_DURATION',\n            'IA_LAST_RUN_DWELL_TIME',\n            'IA_LAST_RUN_FIXATION_COUNT',\n            'IA_TOP',\n            'IA_LEFT',\n            'IA_FIRST_FIX_PROGRESSIVE',\n            'normalized_ID',\n            'PARAGRAPH_RT',\n            'total_skip',\n        ]\n    )\n\n    ia_features: list[str] = MISSING\n\n    fixation_features: list[str] = field(\n        default_factory=lambda: [\n            'CURRENT_FIX_INDEX',\n            'CURRENT_FIX_DURATION',\n            'CURRENT_FIX_PUPIL',\n            'CURRENT_FIX_X',\n            'CURRENT_FIX_Y',\n            'NEXT_FIX_ANGLE',\n            'CURRENT_FIX_INTEREST_AREA_INDEX',\n            'NEXT_FIX_INTEREST_AREA_INDEX',\n            'PREVIOUS_FIX_ANGLE',\n            'NEXT_FIX_DISTANCE',\n            'PREVIOUS_FIX_DISTANCE',\n            'NEXT_SAC_AMPLITUDE',\n            'NEXT_SAC_ANGLE',\n            'NEXT_SAC_AVG_VELOCITY',\n            'NEXT_SAC_DURATION',\n            'NEXT_SAC_PEAK_VELOCITY',\n        ]\n    )\n\n    ia_categorical_features: list[str] = field(\n        default_factory=lambda: [\n            'ptb_pos',\n        ]\n    )\n\n    compute_trial_level_features: bool = False\n    n_tokens: int = 0\n    eye_token_id: int = 0\n    sep_token_id: int = 0\n    is_training: bool = False\n    full_model_name: str = ''\n    max_time_limit: str | None = None\n    sweep_hours_limit: int = 120\n    base_model_name: DLModelNames | MLModelNames = MISSING\n    normalization_mode: NormalizationModes = NormalizationModes.ALL\n    normalization_type: Scaler = Scaler.ROBUST_SCALER\n    backbone: BackboneNames | None = None\n    max_supported_seq_len: int = 512  # for roberta-based models\n\n    def __post_init__(self):\n        \"\"\"\n        Post-initialization hook to compute `eyes_dim` and `fixation_dim` based on the features.\n        \"\"\"\n        if self.feature_mode == FeatureMode.EYES_WORDS:\n            self.ia_features = self.eye_features + self.word_features\n        elif self.feature_mode == FeatureMode.EYES:\n            self.ia_features = self.eye_features\n        elif self.feature_mode == FeatureMode.WORDS:\n            self.ia_features = self.word_features\n\n        n_categorical_features = len(self.ia_categorical_features)\n        assert len(self.ia_features) &gt;= n_categorical_features, (\n            'ia_features should be greater or equal to than ia_categorical_features'\n        )\n        self.eyes_dim = len(self.ia_features) - n_categorical_features\n        self.fixation_dim = len(self.fixation_features) + self.eyes_dim\n\n        self.ia_features_to_add_to_fixation_data = self.ia_features\n\n        self.num_special_tokens_add = 6 if self.prepend_eye_features_to_text else 4\n\n    @property\n    def model_name(self) -&gt; str:\n        return self.__class__.__name__\n\n    @property\n    def max_time(self) -&gt; str | None:\n        \"\"\"\n        Returns the maximum time for training in the format of hours:minutes:seconds.\n        \"\"\"\n        # Lazy import to avoid circular dependency\n        from src.run.multi_run.search_spaces import search_space_by_model\n        from src.run.multi_run.utils import count_hyperparameter_configs\n\n        # Check if the model name is in the search space\n        if self.base_model_name not in search_space_by_model:\n            logger.warning(\n                f'Model name {self.base_model_name} not found in search space.'\n            )\n            return None\n\n        max_time_limit, _ = count_hyperparameter_configs(\n            search_space_by_model[self.base_model_name],\n            log_specific_values=False,\n            n_hours=self.sweep_hours_limit,\n        )\n\n        return max_time_limit\n\n    @staticmethod\n    def get_text_dim(backbone: BackboneNames | None) -&gt; int:\n        \"\"\"\n        Get the text dimension based on the backbone model.\n\n        Args:\n            backbone (BackboneNames | str): The backbone model name.\n\n        Returns:\n            int: The text dimension.\n        \"\"\"\n        if not backbone:\n            # print('Backbone is None. Setting text_dim to 0')\n            return 0\n        if backbone in (BackboneNames.ROBERTA_BASE, BackboneNames.XLM_ROBERTA_BASE):\n            # print(f'Backbone: {backbone}. Setting text_dim to 768')\n            return 768\n        if backbone in (BackboneNames.ROBERTA_LARGE, BackboneNames.XLM_ROBERTA_LARGE):\n            # print(f'Backbone {backbone} is recognized, setting text_dim to 1024')\n            return 1024\n        else:\n            # print(f'Backbone {backbone} not recognized, setting text_dim to 0')\n            return 0\n</code></pre>"},{"location":"reference/configs/models/base_model/#configs.models.base_model.BaseModelArgs.max_time","title":"<code>max_time</code>  <code>property</code>","text":"<p>Returns the maximum time for training in the format of hours:minutes:seconds.</p>"},{"location":"reference/configs/models/base_model/#configs.models.base_model.BaseModelArgs.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Post-initialization hook to compute <code>eyes_dim</code> and <code>fixation_dim</code> based on the features.</p> Source code in <code>src/configs/models/base_model.py</code> <pre><code>def __post_init__(self):\n    \"\"\"\n    Post-initialization hook to compute `eyes_dim` and `fixation_dim` based on the features.\n    \"\"\"\n    if self.feature_mode == FeatureMode.EYES_WORDS:\n        self.ia_features = self.eye_features + self.word_features\n    elif self.feature_mode == FeatureMode.EYES:\n        self.ia_features = self.eye_features\n    elif self.feature_mode == FeatureMode.WORDS:\n        self.ia_features = self.word_features\n\n    n_categorical_features = len(self.ia_categorical_features)\n    assert len(self.ia_features) &gt;= n_categorical_features, (\n        'ia_features should be greater or equal to than ia_categorical_features'\n    )\n    self.eyes_dim = len(self.ia_features) - n_categorical_features\n    self.fixation_dim = len(self.fixation_features) + self.eyes_dim\n\n    self.ia_features_to_add_to_fixation_data = self.ia_features\n\n    self.num_special_tokens_add = 6 if self.prepend_eye_features_to_text else 4\n</code></pre>"},{"location":"reference/configs/models/base_model/#configs.models.base_model.BaseModelArgs.get_text_dim","title":"<code>get_text_dim(backbone)</code>  <code>staticmethod</code>","text":"<p>Get the text dimension based on the backbone model.</p> <p>Parameters:</p> Name Type Description Default <code>backbone</code> <code>BackboneNames | str</code> <p>The backbone model name.</p> required <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>The text dimension.</p> Source code in <code>src/configs/models/base_model.py</code> <pre><code>@staticmethod\ndef get_text_dim(backbone: BackboneNames | None) -&gt; int:\n    \"\"\"\n    Get the text dimension based on the backbone model.\n\n    Args:\n        backbone (BackboneNames | str): The backbone model name.\n\n    Returns:\n        int: The text dimension.\n    \"\"\"\n    if not backbone:\n        # print('Backbone is None. Setting text_dim to 0')\n        return 0\n    if backbone in (BackboneNames.ROBERTA_BASE, BackboneNames.XLM_ROBERTA_BASE):\n        # print(f'Backbone: {backbone}. Setting text_dim to 768')\n        return 768\n    if backbone in (BackboneNames.ROBERTA_LARGE, BackboneNames.XLM_ROBERTA_LARGE):\n        # print(f'Backbone {backbone} is recognized, setting text_dim to 1024')\n        return 1024\n    else:\n        # print(f'Backbone {backbone} not recognized, setting text_dim to 0')\n        return 0\n</code></pre>"},{"location":"reference/configs/models/base_model/#configs.models.base_model.DLModelArgs","title":"<code>DLModelArgs</code>  <code>dataclass</code>","text":"<p>               Bases: <code>BaseModelArgs</code></p> <p>Base configuration class for deep learning model arguments.</p> <p>Attributes:</p> Name Type Description <code>backbone</code> <code>BackboneNames</code> <p>The backbone model to use. Must be specified.</p> <code>accumulate_grad_batches</code> <code>int</code> <p>The number of batches to accumulate gradients before updating the weights.</p> <code>hf_access_token</code> <code>str | None</code> <p>HuggingFace access token for private models.</p> <code>preorder</code> <code>bool</code> <p>Order the answers and convert labels according to ABCD order before model input.</p> <code>warmup_proportion</code> <code>float | None</code> <p>Proportion of training steps for learning rate warmup. Default is None.</p> <code>max_epochs</code> <code>int | None</code> <p>Maximum number of training epochs.</p> <code>early_stopping_patience</code> <code>int | None</code> <p>Number of epochs to wait for improvement before early stopping.</p> Source code in <code>src/configs/models/base_model.py</code> <pre><code>@dataclass\nclass DLModelArgs(BaseModelArgs):\n    \"\"\"\n    Base configuration class for deep learning model arguments.\n\n    Attributes:\n        backbone (BackboneNames): The backbone model to use. Must be specified.\n        accumulate_grad_batches (int): The number of batches to accumulate\n            gradients before updating the weights.\n        hf_access_token (str | None): HuggingFace access token for private models.\n        preorder (bool): Order the answers and convert labels\n            according to ABCD order before model input.\n        warmup_proportion (float | None): Proportion of training steps for learning rate warmup.\n            Default is None.\n        max_epochs (int | None): Maximum number of training epochs.\n        early_stopping_patience (int | None): Number of epochs to wait for improvement before early stopping.\n\n    \"\"\"\n\n    hf_access_token: str | None = None\n    accumulate_grad_batches: int = 1\n    preorder: bool = True\n    base_model_name: DLModelNames = MISSING\n    warmup_proportion: float | None = None\n    max_epochs: int | None = None\n    early_stopping_patience: int | None = None\n\n    def __post_init__(self):\n        \"\"\"\n        Post-initialization hook to compute dimensions and set derived parameters.\n        \"\"\"\n        super().__post_init__()\n        self.text_dim = self.get_text_dim(self.backbone)\n</code></pre>"},{"location":"reference/configs/models/base_model/#configs.models.base_model.DLModelArgs.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Post-initialization hook to compute dimensions and set derived parameters.</p> Source code in <code>src/configs/models/base_model.py</code> <pre><code>def __post_init__(self):\n    \"\"\"\n    Post-initialization hook to compute dimensions and set derived parameters.\n    \"\"\"\n    super().__post_init__()\n    self.text_dim = self.get_text_dim(self.backbone)\n</code></pre>"},{"location":"reference/configs/models/base_model/#configs.models.base_model.MLModelArgs","title":"<code>MLModelArgs</code>  <code>dataclass</code>","text":"<p>               Bases: <code>BaseModelArgs</code></p> <p>Base configuration class for machine learning model arguments.</p> <p>Attributes:</p> Name Type Description <code>preorder</code> <code>bool</code> <p>Whether to preorder the data.</p> <code>sklearn_pipeline</code> <code>Any</code> <p>The scikit-learn pipeline for the model.</p> <code>sklearn_pipeline_params</code> <code>dict</code> <p>Parameters for the scikit-learn pipeline.</p> Source code in <code>src/configs/models/base_model.py</code> <pre><code>@dataclass\nclass MLModelArgs(BaseModelArgs):\n    \"\"\"\n    Base configuration class for machine learning model arguments.\n\n    Attributes:\n        preorder (bool): Whether to preorder the data.\n        sklearn_pipeline (Any): The scikit-learn pipeline for the model.\n        sklearn_pipeline_params (dict): Parameters for the scikit-learn pipeline.\n    \"\"\"\n\n    base_model_name: MLModelNames = MISSING\n    compute_trial_level_features: bool = True\n    sklearn_pipeline: Any = MISSING\n    sklearn_pipeline_params: dict = field(default_factory=dict)\n    max_supported_seq_len: int = 1_000_000\n\n    # Additional features for ML models\n    pca_explained_variance_ratio_threshold: float = 1.0  # if 1, no PCA is done\n\n    def init_sklearn_pipeline_params(self):\n        \"\"\"\n        Initialize scikit-learn pipeline parameters.\n        Iterates over the attributes of the class and adds them to the pipeline parameters.\n        \"\"\"\n        # create sklearn pipeline params\n        # pass over the attributes of the class and add them to the pipeline params\n        for key, value in self.__dict__.items():\n            if key.startswith('sklearn_pipeline_param_'):\n                self.sklearn_pipeline_params[\n                    key.replace('sklearn_pipeline_param_', '')\n                ] = value\n\n    preorder: bool = False\n    use_eyes_only: bool = True\n</code></pre>"},{"location":"reference/configs/models/base_model/#configs.models.base_model.MLModelArgs.init_sklearn_pipeline_params","title":"<code>init_sklearn_pipeline_params()</code>","text":"<p>Initialize scikit-learn pipeline parameters. Iterates over the attributes of the class and adds them to the pipeline parameters.</p> Source code in <code>src/configs/models/base_model.py</code> <pre><code>def init_sklearn_pipeline_params(self):\n    \"\"\"\n    Initialize scikit-learn pipeline parameters.\n    Iterates over the attributes of the class and adds them to the pipeline parameters.\n    \"\"\"\n    # create sklearn pipeline params\n    # pass over the attributes of the class and add them to the pipeline params\n    for key, value in self.__dict__.items():\n        if key.startswith('sklearn_pipeline_param_'):\n            self.sklearn_pipeline_params[\n                key.replace('sklearn_pipeline_param_', '')\n            ] = value\n</code></pre>"},{"location":"reference/configs/models/dl/Ahn/","title":"Ahn","text":"<p>Ahn.py Ahn model configuration. This module defines the configuration for the Ahn model, including its parameters and specific settings for different model architectures (RNN and CNN).</p>"},{"location":"reference/configs/models/dl/Ahn/#configs.models.dl.Ahn.Ahn","title":"<code>Ahn</code>  <code>dataclass</code>","text":"<p>               Bases: <code>DLModelArgs</code></p> <p>Configuration for the Ahn model.</p> Source code in <code>src/configs/models/dl/Ahn.py</code> <pre><code>@dataclass\nclass Ahn(DLModelArgs):\n    \"\"\"\n    Configuration for the Ahn model.\n    \"\"\"\n\n    batch_size: int = 16\n    use_fixation_report: bool = True\n    use_eyes_only: bool = True\n    max_supported_seq_len: int = 1_000_000\n\n    preorder: bool = False\n\n    fixation_features: list[str] = field(\n        default_factory=lambda: [\n            'CURRENT_FIX_DURATION',\n            'CURRENT_FIX_PUPIL',\n            'CURRENT_FIX_X',\n            'CURRENT_FIX_Y',\n        ]\n    )\n    eye_features: list = field(default_factory=list)\n    word_features: list = field(default_factory=list)\n    ia_categorical_features: list = field(default_factory=list)\n    hidden_dim: int = MISSING\n    fc_dropout: float = 0.3\n    max_epochs: int = 1000\n    early_stopping_patience: int = 50\n</code></pre>"},{"location":"reference/configs/models/dl/Ahn/#configs.models.dl.Ahn.AhnCNN","title":"<code>AhnCNN</code>  <code>dataclass</code>","text":"<p>               Bases: <code>Ahn</code></p> <p>Configuration for the Ahn CNN model.</p> Source code in <code>src/configs/models/dl/Ahn.py</code> <pre><code>@register_model_config\n@dataclass\nclass AhnCNN(Ahn):\n    \"\"\"\n    Configuration for the Ahn CNN model.\n    \"\"\"\n\n    base_model_name: DLModelNames = DLModelNames.AHN_CNN_MODEL\n\n    hidden_dim: int = 40\n    conv_kernel_size: int = 3\n    pooling_kernel_size: int = 2\n    fc_hidden_dim1: int = 50\n    fc_hidden_dim2: int = 20\n</code></pre>"},{"location":"reference/configs/models/dl/Ahn/#configs.models.dl.Ahn.AhnRNN","title":"<code>AhnRNN</code>  <code>dataclass</code>","text":"<p>               Bases: <code>Ahn</code></p> <p>Configuration for the Ahn RNN model.</p> Source code in <code>src/configs/models/dl/Ahn.py</code> <pre><code>@register_model_config\n@dataclass\nclass AhnRNN(Ahn):\n    \"\"\"\n    Configuration for the Ahn RNN model.\n    \"\"\"\n\n    base_model_name: DLModelNames = DLModelNames.AHN_RNN_MODEL\n\n    hidden_dim: int = 25\n    num_lstm_layers: int = 1\n    fc_hidden_dim: int = 20\n</code></pre>"},{"location":"reference/configs/models/dl/BEyeLSTM/","title":"BEyeLSTM","text":"<p>BeyeLSTM model arguments and parameters.</p>"},{"location":"reference/configs/models/dl/BEyeLSTM/#configs.models.dl.BEyeLSTM.BEyeLSTMArgs","title":"<code>BEyeLSTMArgs</code>  <code>dataclass</code>","text":"<p>               Bases: <code>DLModelArgs</code></p> <p>Model arguments for the BEyeLSTM model.</p>"},{"location":"reference/configs/models/dl/BEyeLSTM/#configs.models.dl.BEyeLSTM.BEyeLSTMArgs--update-the-docstring-to-include-all-parameters","title":"? update the docstring to include all parameters","text":"<p>Attributes:     max_eye_len (int): The maximum sequence length for the eye input, in tokens.     batch_size (int): Batch size for training.     backbone (BackboneNames): Backbone model to use.     use_fixation_report (bool): Whether to use fixation report.     compute_trial_level_features (bool): Whether to compute trial-level features.     fixation_features (list[str]): List of fixation features.     eye_features (list[str]): List of eye features.     word_features (list[str]): List of word features.     ia_categorical_features (list[str]): List of categorical features for interest areas.     num_pos (int): Number of positions.     num_content (int): Number of content types.     fixations_dim (int): Dimension of fixations.     gsf_dim (int): Dimension of GSF.</p> Source code in <code>src/configs/models/dl/BEyeLSTM.py</code> <pre><code>@register_model_config\n@dataclass\nclass BEyeLSTMArgs(DLModelArgs):\n    \"\"\"\n    Model arguments for the BEyeLSTM model.\n    #? update the docstring to include all parameters\n    Attributes:\n        max_eye_len (int): The maximum sequence length for the eye input, in tokens.\n        batch_size (int): Batch size for training.\n        backbone (BackboneNames): Backbone model to use.\n        use_fixation_report (bool): Whether to use fixation report.\n        compute_trial_level_features (bool): Whether to compute trial-level features.\n        fixation_features (list[str]): List of fixation features.\n        eye_features (list[str]): List of eye features.\n        word_features (list[str]): List of word features.\n        ia_categorical_features (list[str]): List of categorical features for interest areas.\n        num_pos (int): Number of positions.\n        num_content (int): Number of content types.\n        fixations_dim (int): Dimension of fixations.\n        gsf_dim (int): Dimension of GSF.\n    \"\"\"\n\n    max_supported_seq_len: int = 1_000_000\n    base_model_name: DLModelNames = DLModelNames.BEYELSTM_MODEL\n    batch_size: int = 64\n    use_fixation_report: bool = True\n    use_eyes_only: bool = True\n    num_pos: int = 5\n    num_content: int = 2\n    fixations_dim: int = 4  #! Not a hyperparameter to play with\n    \"\"\"\n    Originally:\n    **35** binned values X (**13** reading features + **5** linguistic features) + **4** global features = **634**\n    Ours:\n    **44** binned values X (**11** reading features + **5** linguistic features) + **4** global features = **708**\n    \"\"\"\n    gsf_dim: int = -1\n    dropout_rate: float = 0.5  # Dropout rate of fc1 and fc2\n    embedding_dim: int = (\n        4  # The embedding dimension for categorical features (universal_pos, Content)\n    )\n    # The output dimensions for fc1,2 after each LSTM\n    lstm_block_fc1_out_dim: int = 50  # originally: 50\n    lstm_block_fc2_out_dim: int = 20  # originally: 20\n    gsf_out_dim: int = 32  # originally: 32\n    # The middle embedding size of the FC after the concat of all LSTM results and gsf (all separate layers, only the dim is shared)\n    after_cat_fc_hidden_dim: int = 32\n    hidden_dim: int = 64  # the hidden dim inside the LSTM. Originally: 25\n\n    compute_trial_level_features: bool = True\n    fixation_features: list[str] = field(\n        default_factory=lambda: [\n            'CURRENT_FIX_DURATION',\n            'CURRENT_FIX_PUPIL',\n            'CURRENT_FIX_X',\n            'CURRENT_FIX_Y',\n            'NEXT_FIX_INTEREST_AREA_INDEX',\n            'CURRENT_FIX_INTEREST_AREA_INDEX',\n            'LengthCategory',\n            'is_reg_sum',\n            'is_progressive_sum',\n            'IA_REGRESSION_IN_COUNT_sum',\n            'normalized_outgoing_regression_count',\n            'normalized_outgoing_progressive_count',\n            'normalized_incoming_regression_count',\n        ]\n    )\n    eye_features: list[str] = field(\n        default_factory=lambda: [\n            'TRIAL_IA_COUNT',\n            'IA_REGRESSION_OUT_FULL_COUNT',\n            'IA_FIXATION_COUNT',\n            'IA_REGRESSION_IN_COUNT',\n            'IA_FIRST_FIXATION_DURATION',\n            'IA_DWELL_TIME',\n        ]\n    )\n\n    word_features: list[str] = field(\n        default_factory=lambda: [\n            'is_content_word',\n            'ptb_pos',\n            'left_dependents_count',\n            'right_dependents_count',\n            'distance_to_head',\n            'head_direction',\n            'gpt2_surprisal',\n            'wordfreq_frequency',\n            'word_length',\n            # 'entity_type',\n            'universal_pos',\n        ]\n    )\n\n    ia_categorical_features: list[str] = field(\n        default_factory=lambda: [\n            'is_content_word',\n            'ptb_pos',\n            # 'entity_type',\n            'universal_pos',\n            'Head_Direction',\n            'head_direction',\n            'TRIAL_IA_COUNT',\n            'LengthCategory',\n            'IA_REGRESSION_OUT_FULL_COUNT',\n            'IA_FIXATION_COUNT',\n            'IA_REGRESSION_IN_COUNT',\n        ]\n    )\n\n    item_level_features_modes: list[ItemLevelFeaturesModes] = field(\n        default_factory=lambda: [ItemLevelFeaturesModes.BEYELSTM]\n    )\n    max_epochs: int = 1000\n    early_stopping_patience: int = 50\n</code></pre>"},{"location":"reference/configs/models/dl/BEyeLSTM/#configs.models.dl.BEyeLSTM.BEyeLSTMArgs.fixations_dim","title":"<code>fixations_dim = 4</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Originally: 35 binned values X (13 reading features + 5 linguistic features) + 4 global features = 634 Ours: 44 binned values X (11 reading features + 5 linguistic features) + 4 global features = 708</p>"},{"location":"reference/configs/models/dl/MAG/","title":"MAG","text":""},{"location":"reference/configs/models/dl/MAG/#configs.models.dl.MAG.MAG","title":"<code>MAG</code>  <code>dataclass</code>","text":"<p>               Bases: <code>DLModelArgs</code></p> <p>Model arguments for the MAG model.</p> <p>Attributes:</p> Name Type Description <code>batch_size</code> <code>int</code> <p>Batch size for training.</p> <code>accumulate_grad_batches</code> <code>int</code> <p>Number of batches to accumulate gradients over.</p> <code>backbone</code> <code>BackboneNames</code> <p>Backbone model to use.</p> <code>use_fixation_report</code> <code>bool</code> <p>Whether to use fixation report.</p> <code>freeze</code> <code>bool</code> <p>Whether to freeze the model parameters.</p> <code>Attributes</code> <code>bool</code> <code>mag_dropout</code> <code>float</code> <p>Dropout rate for the MAG module.</p> <code>mag_beta_shift</code> <code>float</code> <p>Beta shift parameter used in the MAG module.</p> <code>mag_injection_index</code> <code>int</code> <p>Index at which the MAG features are injected into the model.</p> Source code in <code>src/configs/models/dl/MAG.py</code> <pre><code>@register_model_config\n@dataclass\nclass MAG(DLModelArgs):\n    \"\"\"\n    Model arguments for the MAG model.\n\n    Attributes:\n        batch_size (int): Batch size for training.\n        accumulate_grad_batches (int): Number of batches to accumulate gradients over.\n        backbone (BackboneNames): Backbone model to use.\n        use_fixation_report (bool): Whether to use fixation report.\n        freeze (bool): Whether to freeze the model parameters.\n        Attributes:\n        mag_dropout (float): Dropout rate for the MAG module.\n        mag_beta_shift (float): Beta shift parameter used in the MAG module.\n        mag_injection_index (int): Index at which the MAG features are injected into the model.\n    \"\"\"\n\n    base_model_name: DLModelNames = DLModelNames.MAG_MODEL\n\n    mag_dropout: float = 0.5\n    mag_beta_shift: float = 1e-3\n    mag_injection_index: int = 0\n    warmup_proportion: float = 0.1\n    batch_size: int = 4\n    accumulate_grad_batches: int = 16 // batch_size\n    backbone: BackboneNames = BackboneNames.ROBERTA_LARGE\n    use_fixation_report: bool = False\n    freeze: bool = False\n\n    max_epochs: int = 10\n    early_stopping_patience: int = 3\n\n    def __post_init__(self):\n        super().__post_init__()\n        if (\n            self.backbone == BackboneNames.ROBERTA_BASE\n            and self.mag_injection_index &gt; 13\n        ):\n            raise ValueError(\n                f'Warning: MAG injection index {self.mag_injection_index} is higher than 13 for Roberta Base. Exiting.'\n            )\n</code></pre>"},{"location":"reference/configs/models/dl/PLMAS/","title":"PLMAS","text":""},{"location":"reference/configs/models/dl/PLMAS/#configs.models.dl.PLMAS.PLMASArgs","title":"<code>PLMASArgs</code>  <code>dataclass</code>","text":"<p>               Bases: <code>DLModelArgs</code></p> <p>Model arguments for the PLMAS model.</p> <p>Attributes:</p> Name Type Description <code>batch_size</code> <code>int</code> <p>The batch size for training.</p> <code>accumulate_grad_batches</code> <code>int</code> <p>Number of batches to accumulate gradients before updating the weights.</p> <code>backbone</code> <code>BackboneNames</code> <p>The backbone model to use.</p> <code>use_fixation_report</code> <code>bool</code> <p>Whether to use fixation report.</p> <code>freeze</code> <code>bool</code> <p>Whether to freeze the model parameters.</p> <code>fixation_features</code> <code>list[str]</code> <p>List of fixation features to use.</p> <code>eye_features</code> <code>list[str]</code> <p>List of eye features to use.</p> <code>ia_categorical_features</code> <code>list[str]</code> <p>List of categorical interest area features.</p> <code>lstm_num_layers</code> <code>int</code> <p>Number of LSTM layers in the model.</p> <code>lstm_dropout</code> <code>float</code> <p>Dropout rate for the LSTM layers.</p> Source code in <code>src/configs/models/dl/PLMAS.py</code> <pre><code>@register_model_config\n@dataclass\nclass PLMASArgs(DLModelArgs):\n    \"\"\"\n    Model arguments for the PLMAS model.\n\n    Attributes:\n        batch_size (int): The batch size for training.\n        accumulate_grad_batches (int): Number of batches to accumulate gradients before updating the weights.\n        backbone (BackboneNames): The backbone model to use.\n        use_fixation_report (bool): Whether to use fixation report.\n        freeze (bool): Whether to freeze the model parameters.\n        fixation_features (list[str]): List of fixation features to use.\n        eye_features (list[str]): List of eye features to use.\n        ia_categorical_features (list[str]): List of categorical interest area features.\n        lstm_num_layers (int): Number of LSTM layers in the model.\n        lstm_dropout (float): Dropout rate for the LSTM layers.\n    \"\"\"\n\n    base_model_name: DLModelNames = DLModelNames.PLMAS_MODEL\n\n    lstm_num_layers: int = 2\n    lstm_dropout: float = 0.1\n\n    batch_size: int = 16\n    accumulate_grad_batches: int = 1\n    backbone: BackboneNames = BackboneNames.ROBERTA_LARGE\n    use_fixation_report: bool = True\n    freeze: bool = True\n\n    fixation_features: list[str] = field(\n        default_factory=lambda: [\n            'CURRENT_FIX_INTEREST_AREA_INDEX',\n            'CURRENT_FIX_DURATION',\n            'CURRENT_FIX_NEAREST_INTEREST_AREA_DISTANCE',\n        ]\n    )\n    eye_features: list[str] = field(\n        default_factory=lambda: [\n            'IA_DWELL_TIME',\n        ]\n    )\n    ia_categorical_features: list[str] = field(\n        default_factory=lambda: [\n            'is_content_word',\n            'ptb_pos',\n            # 'entity_type',\n            'universal_pos',\n            'Head_Direction',\n            'TRIAL_IA_COUNT',\n            'IA_REGRESSION_OUT_FULL_COUNT',\n            'IA_FIXATION_COUNT',\n            'IA_REGRESSION_IN_COUNT',\n        ]\n    )\n    max_epochs: int = 10\n    early_stopping_patience: int = 3\n    warmup_proportion: float = 0.1\n</code></pre>"},{"location":"reference/configs/models/dl/PLMASF/","title":"PLMASF","text":""},{"location":"reference/configs/models/dl/PLMASF/#configs.models.dl.PLMASF.PLMASfArgs","title":"<code>PLMASfArgs</code>  <code>dataclass</code>","text":"<p>               Bases: <code>DLModelArgs</code></p> <p>Model arguments for the PLMASf model.</p> <p>Attributes:</p> Name Type Description <code>batch_size</code> <code>int</code> <p>The batch size for training.</p> <code>accumulate_grad_batches</code> <code>int</code> <p>The number of batches to accumulate gradients.</p> <code>backbone</code> <code>BackboneNames</code> <p>The backbone model to use.</p> <code>use_fixation_report</code> <code>bool</code> <p>Whether to use fixation report.</p> <code>freeze</code> <code>bool</code> <p>Whether to freeze the model parameters.</p> <code>fixation_features</code> <code>list[str]</code> <p>List of fixation features to use.</p> <code>eye_features</code> <code>list[str]</code> <p>List of eye features to use.</p> <code>ia_categorical_features</code> <code>list[str]</code> <p>List of categorical interest area features.</p> <code>lstm_hidden_size</code> <code>int</code> <p>Hidden size for the LSTM layers.</p> <code>lstm_num_layers</code> <code>int</code> <p>Number of LSTM layers in the model.</p> <code>lstm_dropout</code> <code>float</code> <p>Dropout rate for the LSTM layers.</p> Source code in <code>src/configs/models/dl/PLMASF.py</code> <pre><code>@register_model_config\n@dataclass\nclass PLMASfArgs(DLModelArgs):\n    \"\"\"\n    Model arguments for the PLMASf model.\n\n    Attributes:\n        batch_size (int): The batch size for training.\n        accumulate_grad_batches (int): The number of batches to accumulate gradients.\n        backbone (BackboneNames): The backbone model to use.\n        use_fixation_report (bool): Whether to use fixation report.\n        freeze (bool): Whether to freeze the model parameters.\n        fixation_features (list[str]): List of fixation features to use.\n        eye_features (list[str]): List of eye features to use.\n        ia_categorical_features (list[str]): List of categorical interest area features.\n        lstm_hidden_size (int): Hidden size for the LSTM layers.\n        lstm_num_layers (int): Number of LSTM layers in the model.\n        lstm_dropout (float): Dropout rate for the LSTM layers.\n    \"\"\"\n\n    base_model_name: DLModelNames = DLModelNames.PLMASF_MODEL\n\n    lstm_hidden_size: int = 70\n    lstm_num_layers: int = 1\n    lstm_dropout: float = 0.1\n\n    batch_size: int = 16\n    accumulate_grad_batches: int = 1\n    backbone: BackboneNames = BackboneNames.ROBERTA_LARGE\n    use_fixation_report: bool = True\n    freeze: bool = True\n\n    fixation_features: list[str] = field(\n        default_factory=lambda: [\n            'CURRENT_FIX_INTEREST_AREA_INDEX',\n            'CURRENT_FIX_DURATION',\n            'CURRENT_FIX_NEAREST_INTEREST_AREA_DISTANCE',\n            'CURRENT_FIX_Y',\n            'NEXT_SAC_DURATION',\n            'NEXT_SAC_END_X',\n            'NEXT_SAC_START_X',\n            'NEXT_SAC_END_Y',\n            'NEXT_SAC_START_Y',\n        ]\n    )\n    eye_features: list[str] = field(\n        default_factory=lambda: [\n            'IA_DWELL_TIME',\n            # 'IA_FIRST_RUN_LANDING_POSITION',\n            # 'IA_LAST_RUN_LANDING_POSITION',\n            'IA_FIRST_FIXATION_DURATION',\n        ]\n    )\n    ia_categorical_features: list[str] = field(\n        default_factory=lambda: [\n            'is_content_word',\n            'ptb_pos',\n            # 'entity_type',\n            'universal_pos',\n            'Head_Direction',\n            'TRIAL_IA_COUNT',\n            'IA_REGRESSION_OUT_FULL_COUNT',\n            'IA_FIXATION_COUNT',\n            'IA_REGRESSION_IN_COUNT',\n        ]\n    )\n\n    max_epochs: int = 10\n    early_stopping_patience: int = 3\n    warmup_proportion: float = 0.1\n</code></pre>"},{"location":"reference/configs/models/dl/PostFusion/","title":"PostFusion","text":""},{"location":"reference/configs/models/dl/PostFusion/#configs.models.dl.PostFusion.PostFusion","title":"<code>PostFusion</code>  <code>dataclass</code>","text":"<p>               Bases: <code>DLModelArgs</code></p> <p>PostFusion Model</p> <p>Attributes:</p> Name Type Description <code>batch_size</code> <code>int</code> <p>Batch size for training.</p> <code>accumulate_grad_batches</code> <code>int</code> <p>Number of batches to accumulate gradients before updating the weights.</p> <code>backbone</code> <code>BackboneNames</code> <p>Backbone model to use.</p> <code>use_fixation_report</code> <code>bool</code> <p>Whether to use fixation report.</p> <code>sep_token_id</code> <code>int</code> <p>ID of the separator token.</p> <code>is_training</code> <code>bool</code> <p>Whether the model is in training mode.</p> <code>freeze</code> <code>bool</code> <p>Whether to freeze the model.</p> <code>prepend_eye_features_to_text</code> <code>bool</code> <p>A flag indicating whether to prepend the eye data to the input. If True, the eye data will be added at the beginning of the input (otherwise no eyes).</p> <code>eye_projection_dropout</code> <code>float</code> <p>Dropout rate for the eye projection layer.</p> <code>cross_attention_dropout</code> <code>float</code> <p>Dropout rate for the cross-attention layer.</p> <code>use_attn_mask</code> <code>bool</code> <p>Whether to use an attention mask in the model.</p> Source code in <code>src/configs/models/dl/PostFusion.py</code> <pre><code>@register_model_config\n@dataclass\nclass PostFusion(DLModelArgs):\n    \"\"\"\n    PostFusion Model\n\n    Attributes:\n        batch_size (int): Batch size for training.\n        accumulate_grad_batches (int): Number of batches to accumulate gradients before updating the weights.\n        backbone (BackboneNames): Backbone model to use.\n        use_fixation_report (bool): Whether to use fixation report.\n        sep_token_id (int): ID of the separator token.\n        is_training (bool): Whether the model is in training mode.\n        freeze (bool): Whether to freeze the model.\n        prepend_eye_features_to_text (bool): A flag indicating whether to prepend the eye data to the input.\n            If True, the eye data will be added at the beginning of the input (otherwise no eyes).\n        eye_projection_dropout (float): Dropout rate for the eye projection layer.\n        cross_attention_dropout (float): Dropout rate for the cross-attention layer.\n        use_attn_mask (bool): Whether to use an attention mask in the model.\n    \"\"\"\n\n    base_model_name: DLModelNames = DLModelNames.POSTFUSION_MODEL\n\n    prepend_eye_features_to_text: bool = False\n    eye_projection_dropout: float = 0.1\n    cross_attention_dropout: float = 0.1\n    use_attn_mask: bool = True\n    batch_size: int = 4\n    accumulate_grad_batches: int = 16 // batch_size\n    backbone: BackboneNames = BackboneNames.ROBERTA_LARGE\n    use_fixation_report: bool = True\n    sep_token_id: int = 2\n    is_training: bool = False\n    freeze: bool = False\n    warmup_proportion: float = 0.1\n    max_epochs: int = 10\n    early_stopping_patience: int = 3\n</code></pre>"},{"location":"reference/configs/models/dl/RoBERTeye/","title":"RoBERTeye","text":""},{"location":"reference/configs/models/dl/RoBERTeye/#configs.models.dl.RoBERTeye.Roberta","title":"<code>Roberta</code>  <code>dataclass</code>","text":"<p>               Bases: <code>RoberteyeArgs</code></p> <p>Roberta Model (no eye data)</p> Source code in <code>src/configs/models/dl/RoBERTeye.py</code> <pre><code>@register_model_config\n@dataclass\nclass Roberta(RoberteyeArgs):\n    \"\"\"\n    Roberta Model (no eye data)\n    \"\"\"\n\n    prepend_eye_features_to_text: bool = False\n    use_fixation_report: bool = False\n</code></pre>"},{"location":"reference/configs/models/dl/RoBERTeye/#configs.models.dl.RoBERTeye.RoberteyeArgs","title":"<code>RoberteyeArgs</code>  <code>dataclass</code>","text":"<p>               Bases: <code>DLModelArgs</code></p> <p>prepend_eye_features_to_text (bool): A flag indicating whether to prepend the eye data to the input.     If True, the eye data will be added at the beginning of the input; otherwise, eye data is not used.</p> Source code in <code>src/configs/models/dl/RoBERTeye.py</code> <pre><code>@dataclass\nclass RoberteyeArgs(DLModelArgs):\n    \"\"\"\n    prepend_eye_features_to_text (bool): A flag indicating whether to prepend the eye data to the input.\n        If True, the eye data will be added at the beginning of the input; otherwise, eye data is not used.\n    \"\"\"\n\n    base_model_name: DLModelNames = DLModelNames.ROBERTEYE_MODEL\n\n    prepend_eye_features_to_text: bool = True\n    batch_size: int = 4\n    accumulate_grad_batches: int = 16 // batch_size\n    backbone: BackboneNames = BackboneNames.ROBERTA_LARGE\n    freeze: bool = False\n    eye_projection_dropout: float = 0.3\n    max_epochs: int = 10\n    early_stopping_patience: int = 3\n    warmup_proportion: float = 0.1\n    eye_projection_MAGModule: bool = False\n\n    token_type_num: int = 2\n    vocab_size: int = -1  # specified in instantiate_config\n    #! Don't change the following\n    n_tokens: int = 0\n    eye_token_id: int = 0\n    sep_token_id: int = 0\n    is_training: bool = False\n</code></pre>"},{"location":"reference/configs/models/dl/RoBERTeye/#configs.models.dl.RoBERTeye.RoberteyeFixation","title":"<code>RoberteyeFixation</code>  <code>dataclass</code>","text":"<p>               Bases: <code>RoberteyeArgs</code></p> <p>Fixation-level RoBERTeye</p> Source code in <code>src/configs/models/dl/RoBERTeye.py</code> <pre><code>@register_model_config\n@dataclass\nclass RoberteyeFixation(RoberteyeArgs):\n    \"\"\"\n    Fixation-level RoBERTeye\n    \"\"\"\n\n    use_fixation_report: bool = True\n    batch_size: int = 2\n    accumulate_grad_batches: int = 16 // batch_size\n</code></pre>"},{"location":"reference/configs/models/dl/RoBERTeye/#configs.models.dl.RoBERTeye.RoberteyeWord","title":"<code>RoberteyeWord</code>  <code>dataclass</code>","text":"<p>               Bases: <code>RoberteyeArgs</code></p> <p>Word-level RoBERTeye</p> Source code in <code>src/configs/models/dl/RoBERTeye.py</code> <pre><code>@register_model_config\n@dataclass\nclass RoberteyeWord(RoberteyeArgs):\n    \"\"\"\n    Word-level RoBERTeye\n    \"\"\"\n\n    use_fixation_report: bool = False\n</code></pre>"},{"location":"reference/configs/models/dl/__init__/","title":"init","text":""},{"location":"reference/configs/models/ml/DummyClassifier/","title":"DummyClassifier","text":""},{"location":"reference/configs/models/ml/DummyClassifier/#configs.models.ml.DummyClassifier.DummyClassifierMLArgs","title":"<code>DummyClassifierMLArgs</code>  <code>dataclass</code>","text":"<p>               Bases: <code>MLModelArgs</code></p> <p>Model arguments for the Dummy Classifier model.</p> <p>Attributes:</p> Name Type Description <code>batch_size</code> <code>int</code> <p>The batch size for training.</p> <code>use_fixation_report</code> <code>bool</code> <p>Whether to use the fixation report.</p> <code>backbone</code> <code>str</code> <p>The backbone model to use.</p> <code>item_level_features_modes</code> <code>list[ItemLevelFeaturesModes]</code> <p>The item-level features to use.</p> <code>sklearn_pipeline</code> <code>tuple</code> <p>The scikit-learn pipeline for the model.</p> <code>sklearn_pipeline_param_clf__strategy</code> <code>str</code> <p>Strategy for the dummy classifier (\"stratified\", \"most_frequent\", etc.).</p> <code>sklearn_pipeline_param_clf__random_state</code> <code>int</code> <p>Random seed for the dummy classifier.</p> Source code in <code>src/configs/models/ml/DummyClassifier.py</code> <pre><code>@register_model_config\n@dataclass\nclass DummyClassifierMLArgs(MLModelArgs):\n    \"\"\"\n    Model arguments for the Dummy Classifier model.\n\n    Attributes:\n        batch_size (int): The batch size for training.\n        use_fixation_report (bool): Whether to use the fixation report.\n        backbone (str): The backbone model to use.\n        item_level_features_modes (list[ItemLevelFeaturesModes]): The item-level features to use.\n        sklearn_pipeline (tuple): The scikit-learn pipeline for the model.\n        sklearn_pipeline_param_clf__strategy (str): Strategy for the dummy classifier\n            (\"stratified\", \"most_frequent\", etc.).\n        sklearn_pipeline_param_clf__random_state (int): Random seed for the dummy classifier.\n    \"\"\"\n\n    base_model_name: MLModelNames = MLModelNames.DUMMY_CLASSIFIER\n    item_level_features_modes: list[ItemLevelFeaturesModes] = field(\n        default_factory=lambda: [ItemLevelFeaturesModes.READING_SPEED]\n    )\n    sklearn_pipeline: tuple = (('clf', 'sklearn.dummy.DummyClassifier'),)\n    sklearn_pipeline_param_clf__strategy: str = 'most_frequent'\n    sklearn_pipeline_param_clf__random_state: int = 1\n\n    batch_size: int = 1024\n    use_fixation_report: bool = True\n    backbone: BackboneNames = BackboneNames.ROBERTA_LARGE\n</code></pre>"},{"location":"reference/configs/models/ml/DummyClassifier/#configs.models.ml.DummyClassifier.DummyRegressorMLArgs","title":"<code>DummyRegressorMLArgs</code>  <code>dataclass</code>","text":"<p>               Bases: <code>MLModelArgs</code></p> <p>Model arguments for the Dummy Regressor model.</p> <p>Attributes:</p> Name Type Description <code>batch_size</code> <code>int</code> <p>The batch size for training.</p> <code>use_fixation_report</code> <code>bool</code> <p>Whether to use the fixation report.</p> <code>backbone</code> <code>str</code> <p>The backbone model to use.</p> <code>item_level_features_modes</code> <code>list[ItemLevelFeaturesModes]</code> <p>The item-level features to use.</p> <code>sklearn_pipeline</code> <code>list</code> <p>The scikit-learn pipeline for the model.</p> <code>sklearn_pipeline_param_reg__strategy</code> <code>str</code> <p>Strategy for the dummy regressor (\"mean\", \"median\", etc.).</p> <code>sklearn_pipeline_param_reg__random_state</code> <code>int</code> <p>Random seed for the dummy regressor.</p> Source code in <code>src/configs/models/ml/DummyClassifier.py</code> <pre><code>@register_model_config\n@dataclass\nclass DummyRegressorMLArgs(MLModelArgs):\n    \"\"\"\n    Model arguments for the Dummy Regressor model.\n\n    Attributes:\n        batch_size (int): The batch size for training.\n        use_fixation_report (bool): Whether to use the fixation report.\n        backbone (str): The backbone model to use.\n        item_level_features_modes (list[ItemLevelFeaturesModes]): The item-level features to use.\n        sklearn_pipeline (list): The scikit-learn pipeline for the model.\n        sklearn_pipeline_param_reg__strategy (str): Strategy for the dummy regressor (\"mean\", \"median\", etc.).\n        sklearn_pipeline_param_reg__random_state (int): Random seed for the dummy regressor.\n    \"\"\"\n\n    base_model_name: MLModelNames = MLModelNames.DUMMY_REGRESSOR\n    item_level_features_modes: list[ItemLevelFeaturesModes] = field(\n        default_factory=lambda: [ItemLevelFeaturesModes.READING_SPEED]\n    )\n    sklearn_pipeline: tuple = (('reg', 'sklearn.dummy.DummyRegressor'),)\n    sklearn_pipeline_param_reg__strategy: str = 'mean'\n\n    batch_size: int = 1024\n    use_fixation_report: bool = True\n    backbone: BackboneNames = BackboneNames.ROBERTA_LARGE\n</code></pre>"},{"location":"reference/configs/models/ml/LogisticRegression/","title":"LogisticRegression","text":""},{"location":"reference/configs/models/ml/LogisticRegression/#configs.models.ml.LogisticRegression.LinearRegressionArgs","title":"<code>LinearRegressionArgs</code>  <code>dataclass</code>","text":"<p>               Bases: <code>MLModelArgs</code></p> <p>Model arguments for the Linear Regression model.</p> <p>Attributes:</p> Name Type Description <code>batch_size</code> <code>int</code> <p>The batch size for training.</p> <code>use_fixation_report</code> <code>bool</code> <p>Whether to use the fixation report.</p> <code>backbone</code> <code>str</code> <p>The backbone model to use.</p> <code>sklearn_pipeline</code> <code>tuple</code> <p>The scikit-learn pipeline for the model.</p> <code>sklearn_pipeline_param_regressor__fit_intercept</code> <code>bool</code> <p>Whether to calculate the intercept for this model.</p> <code>sklearn_pipeline_param_scaler__with_mean</code> <code>bool</code> <p>Whether to center data before scaling.</p> <code>sklearn_pipeline_param_scaler__with_std</code> <code>bool</code> <p>Whether to scale data to unit variance.</p> Source code in <code>src/configs/models/ml/LogisticRegression.py</code> <pre><code>@register_model_config\n@dataclass\nclass LinearRegressionArgs(MLModelArgs):\n    \"\"\"\n    Model arguments for the Linear Regression model.\n\n    Attributes:\n        batch_size (int): The batch size for training.\n        use_fixation_report (bool): Whether to use the fixation report.\n        backbone (str): The backbone model to use.\n        sklearn_pipeline (tuple): The scikit-learn pipeline for the model.\n        sklearn_pipeline_param_regressor__fit_intercept (bool): Whether to calculate the intercept for this model.\n        sklearn_pipeline_param_scaler__with_mean (bool): Whether to center data before scaling.\n        sklearn_pipeline_param_scaler__with_std (bool): Whether to scale data to unit variance.\n    \"\"\"\n\n    base_model_name: MLModelNames = MLModelNames.LINEAR_REG\n\n    sklearn_pipeline: tuple = (\n        ('scaler', 'sklearn.preprocessing.StandardScaler'),\n        ('regressor', 'sklearn.linear_model.LinearRegression'),\n    )\n    sklearn_pipeline_param_regressor__fit_intercept: bool = True\n    sklearn_pipeline_param_scaler__with_mean: bool = True\n    sklearn_pipeline_param_scaler__with_std: bool = True\n\n    batch_size: int = 1024\n    use_fixation_report: bool = True\n    backbone: BackboneNames = BackboneNames.XLM_ROBERTA_LARGE\n    item_level_features_modes: list[ItemLevelFeaturesModes] = field(\n        default_factory=lambda: [ItemLevelFeaturesModes.READING_SPEED],\n    )\n</code></pre>"},{"location":"reference/configs/models/ml/LogisticRegression/#configs.models.ml.LogisticRegression.LogisticRegressionMLArgs","title":"<code>LogisticRegressionMLArgs</code>  <code>dataclass</code>","text":"<p>               Bases: <code>MLModelArgs</code></p> <p>Model arguments for the Logistic Regression model.</p> <p>Attributes:</p> Name Type Description <code>batch_size</code> <code>int</code> <p>The batch size for training.</p> <code>use_fixation_report</code> <code>bool</code> <p>Whether to use the fixation report.</p> <code>backbone</code> <code>str</code> <p>The backbone model to use.</p> <code>sklearn_pipeline</code> <code>tuple</code> <p>The scikit-learn pipeline for the model.</p> <code>sklearn_pipeline_param_clf__C</code> <code>float</code> <p>Inverse of regularization strength.</p> <code>sklearn_pipeline_param_clf__fit_intercept</code> <code>bool</code> <p>Whether to add an intercept to the decision function.</p> <code>sklearn_pipeline_param_clf__penalty</code> <code>str</code> <p>Norm used in penalization.</p> <code>sklearn_pipeline_param_clf__solver</code> <code>str</code> <p>Optimization algorithm.</p> <code>sklearn_pipeline_param_clf__random_state</code> <code>int</code> <p>Seed for pseudo-random number generator.</p> <code>sklearn_pipeline_param_clf__max_iter</code> <code>int</code> <p>Maximum number of solver iterations.</p> <code>sklearn_pipeline_param_clf__class_weight</code> <code>str</code> <p>Class weight balancing strategy.</p> <code>sklearn_pipeline_param_scaler__with_mean</code> <code>bool</code> <p>Whether to center data before scaling.</p> <code>sklearn_pipeline_param_scaler__with_std</code> <code>bool</code> <p>Whether to scale data to unit variance.</p> Source code in <code>src/configs/models/ml/LogisticRegression.py</code> <pre><code>@register_model_config\n@dataclass\nclass LogisticRegressionMLArgs(MLModelArgs):\n    \"\"\"\n    Model arguments for the Logistic Regression model.\n\n    Attributes:\n        batch_size (int): The batch size for training.\n        use_fixation_report (bool): Whether to use the fixation report.\n        backbone (str): The backbone model to use.\n        sklearn_pipeline (tuple): The scikit-learn pipeline for the model.\n        sklearn_pipeline_param_clf__C (float): Inverse of regularization strength.\n        sklearn_pipeline_param_clf__fit_intercept (bool): Whether to add an intercept to the decision function.\n        sklearn_pipeline_param_clf__penalty (str): Norm used in penalization.\n        sklearn_pipeline_param_clf__solver (str): Optimization algorithm.\n        sklearn_pipeline_param_clf__random_state (int): Seed for pseudo-random number generator.\n        sklearn_pipeline_param_clf__max_iter (int): Maximum number of solver iterations.\n        sklearn_pipeline_param_clf__class_weight (str): Class weight balancing strategy.\n        sklearn_pipeline_param_scaler__with_mean (bool): Whether to center data before scaling.\n        sklearn_pipeline_param_scaler__with_std (bool): Whether to scale data to unit variance.\n    \"\"\"\n\n    base_model_name: MLModelNames = MLModelNames.LOGISTIC_REGRESSION\n\n    sklearn_pipeline: tuple = (\n        ('scaler', 'sklearn.preprocessing.StandardScaler'),\n        ('clf', 'sklearn.linear_model.LogisticRegression'),\n    )\n    sklearn_pipeline_param_clf__C: float = 2.0\n    sklearn_pipeline_param_clf__fit_intercept: bool = True\n    sklearn_pipeline_param_clf__penalty: str = 'l2'\n    sklearn_pipeline_param_clf__solver: str = 'lbfgs'\n    sklearn_pipeline_param_clf__random_state: int = 1\n    sklearn_pipeline_param_clf__max_iter: int = 1000\n    sklearn_pipeline_param_clf__class_weight: str = 'balanced'\n    sklearn_pipeline_param_scaler__with_mean: bool = True\n    sklearn_pipeline_param_scaler__with_std: bool = True\n\n    batch_size: int = 1024\n    use_fixation_report: bool = True\n    backbone: BackboneNames = BackboneNames.ROBERTA_LARGE\n    item_level_features_modes: list[ItemLevelFeaturesModes] = field(\n        default_factory=lambda: [ItemLevelFeaturesModes.READING_SPEED],\n    )\n</code></pre>"},{"location":"reference/configs/models/ml/RandomForest/","title":"RandomForest","text":""},{"location":"reference/configs/models/ml/RandomForest/#configs.models.ml.RandomForest.RandomForestMLArgs","title":"<code>RandomForestMLArgs</code>  <code>dataclass</code>","text":"<p>               Bases: <code>MLModelArgs</code></p> <p>Model arguments for the RandomForest model.</p> <p>Attributes:</p> Name Type Description <code>batch_size</code> <code>int</code> <p>The batch size for training.</p> <code>use_fixation_report</code> <code>bool</code> <p>Whether to use the fixation report.</p> <code>backbone</code> <code>str</code> <p>The backbone model to use.</p> <code>pca_explained_variance_ratio_threshold</code> <code>float</code> <p>Threshold for PCA explained variance ratio.</p> <code>sklearn_pipeline</code> <code>tuple</code> <p>The scikit-learn pipeline for the model.</p> <code>sklearn_pipeline_param_clf__n_estimators</code> <code>int</code> <p>Number of gradient boosted trees.</p> <code>sklearn_pipeline_param_clf__criterion</code> <code>str</code> <p>The function to measure the quality of a split.</p> <code>sklearn_pipeline_param_clf__max_depth</code> <code>int</code> <p>Maximum depth of a tree.</p> <code>sklearn_pipeline_param_clf__min_samples_split</code> <code>int | float</code> <p>The minimum number of samples required to split an internal node.</p> <code>sklearn_pipeline_param_clf__min_samples_leaf</code> <code>int | float</code> <p>The minimum number of samples required to be at a leaf node.</p> <code>sklearn_pipeline_param_clf__max_features</code> <code>str | int | float</code> <p>The number of features to consider when looking for the best split.</p> <code>sklearn_pipeline_param_clf__n_jobs</code> <code>int</code> <p>The number of jobs to run in parallel.</p> Source code in <code>src/configs/models/ml/RandomForest.py</code> <pre><code>@register_model_config\n@dataclass\nclass RandomForestMLArgs(MLModelArgs):\n    \"\"\"\n    Model arguments for the RandomForest model.\n\n    Attributes:\n        batch_size (int): The batch size for training.\n        use_fixation_report (bool): Whether to use the fixation report.\n        backbone (str): The backbone model to use.\n        pca_explained_variance_ratio_threshold (float): Threshold for PCA explained variance ratio.\n        sklearn_pipeline (tuple): The scikit-learn pipeline for the model.\n        sklearn_pipeline_param_clf__n_estimators (int): Number of gradient boosted trees.\n        sklearn_pipeline_param_clf__criterion (str): The function to measure the quality of a split.\n        sklearn_pipeline_param_clf__max_depth (int): Maximum depth of a tree.\n        sklearn_pipeline_param_clf__min_samples_split (int | float): The minimum number of samples required to split an internal node.\n        sklearn_pipeline_param_clf__min_samples_leaf (int | float): The minimum number of samples required to be at a leaf node.\n        sklearn_pipeline_param_clf__max_features (str| int | float): The number of features to consider when looking for the best split.\n        sklearn_pipeline_param_clf__n_jobs (int): The number of jobs to run in parallel.\n\n    \"\"\"\n\n    base_model_name: MLModelNames = MLModelNames.RANDOM_FOREST\n\n    sklearn_pipeline: tuple = (\n        ('scaler', 'sklearn.preprocessing.StandardScaler'),\n        ('clf', 'sklearn.ensemble.RandomForestClassifier'),\n    )\n\n    # sklearn pipeline params\n    #! note the naming convention for the parameters:\n    #! sklearn_pipeline_param_&lt;pipline_element_name&gt;__&lt;param_name&gt;\n\n    # clf params\n    sklearn_pipeline_param_clf__n_estimators: int = 1000\n    sklearn_pipeline_param_clf__max_depth: int = 6\n    sklearn_pipeline_param_clf__min_samples_split: int = 2\n    sklearn_pipeline_param_clf__min_samples_leaf: int = 1\n    sklearn_pipeline_param_clf__max_features: str = 'sqrt'\n    sklearn_pipeline_param_clf__n_jobs: int = -1\n\n    # scaler params\n    sklearn_pipeline_param_scaler__with_mean: bool = True\n    sklearn_pipeline_param_scaler__with_std: bool = True\n\n    batch_size: int = 1024\n    use_fixation_report: bool = True\n    backbone: BackboneNames = BackboneNames.ROBERTA_LARGE\n\n    item_level_features_modes: list[ItemLevelFeaturesModes] = field(\n        default_factory=lambda: [ItemLevelFeaturesModes.RF],\n    )\n</code></pre>"},{"location":"reference/configs/models/ml/RandomForest/#configs.models.ml.RandomForest.RandomForestRegressorMLArgs","title":"<code>RandomForestRegressorMLArgs</code>  <code>dataclass</code>","text":"<p>               Bases: <code>MLModelArgs</code></p> <p>Model arguments for the RandomForest regressor model.</p> <p>Attributes:</p> Name Type Description <code>batch_size</code> <code>int</code> <p>The batch size for training.</p> <code>use_fixation_report</code> <code>bool</code> <p>Whether to use the fixation report.</p> <code>backbone</code> <code>str</code> <p>The backbone model to use.</p> <code>pca_explained_variance_ratio_threshold</code> <code>float</code> <p>Threshold for PCA explained variance ratio.</p> <code>sklearn_pipeline</code> <code>tuple</code> <p>The scikit-learn pipeline for the model.</p> <code>sklearn_pipeline_param_reg__n_estimators</code> <code>int</code> <p>Number of trees in the forest.</p> <code>sklearn_pipeline_param_reg__max_depth</code> <code>int</code> <p>Maximum depth of a tree.</p> <code>sklearn_pipeline_param_reg__min_samples_split</code> <code>int | float</code> <p>The minimum number of samples required to split an internal node.</p> <code>sklearn_pipeline_param_reg__min_samples_leaf</code> <code>int | float</code> <p>The minimum number of samples required to be at a leaf node.</p> <code>sklearn_pipeline_param_reg__max_features</code> <code>str | int | float</code> <p>The number of features to consider when looking for the best split.</p> <code>sklearn_pipeline_param_reg__n_jobs</code> <code>int</code> <p>The number of jobs to run in parallel.</p> Source code in <code>src/configs/models/ml/RandomForest.py</code> <pre><code>@register_model_config\n@dataclass\nclass RandomForestRegressorMLArgs(MLModelArgs):\n    \"\"\"\n    Model arguments for the RandomForest regressor model.\n\n    Attributes:\n        batch_size (int): The batch size for training.\n        use_fixation_report (bool): Whether to use the fixation report.\n        backbone (str): The backbone model to use.\n        pca_explained_variance_ratio_threshold (float): Threshold for PCA explained variance ratio.\n        sklearn_pipeline (tuple): The scikit-learn pipeline for the model.\n        sklearn_pipeline_param_reg__n_estimators (int): Number of trees in the forest.\n        sklearn_pipeline_param_reg__max_depth (int): Maximum depth of a tree.\n        sklearn_pipeline_param_reg__min_samples_split (int | float): The minimum number of samples required to split an internal node.\n        sklearn_pipeline_param_reg__min_samples_leaf (int | float): The minimum number of samples required to be at a leaf node.\n        sklearn_pipeline_param_reg__max_features (str| int | float): The number of features to consider when looking for the best split.\n        sklearn_pipeline_param_reg__n_jobs (int): The number of jobs to run in parallel.\n    \"\"\"\n\n    base_model_name: MLModelNames = MLModelNames.RANDOM_FOREST_REG\n\n    sklearn_pipeline: tuple = (\n        ('scaler', 'sklearn.preprocessing.StandardScaler'),\n        ('reg', 'sklearn.ensemble.RandomForestRegressor'),\n    )\n\n    # sklearn pipeline params\n    #! sklearn_pipeline_param_&lt;pipline_element_name&gt;__&lt;param_name&gt;\n\n    # regressor params\n    sklearn_pipeline_param_reg__n_estimators: int = 1000\n    sklearn_pipeline_param_reg__max_depth: int = 6\n    sklearn_pipeline_param_reg__min_samples_split: int = 2\n    sklearn_pipeline_param_reg__min_samples_leaf: int = 1\n    sklearn_pipeline_param_reg__max_features: str = 'sqrt'\n    sklearn_pipeline_param_reg__n_jobs: int = -1\n\n    # scaler params\n    sklearn_pipeline_param_scaler__with_mean: bool = True\n    sklearn_pipeline_param_scaler__with_std: bool = True\n\n    batch_size: int = 1024\n    use_fixation_report: bool = True\n    backbone: BackboneNames = BackboneNames.ROBERTA_LARGE\n\n    item_level_features_modes: list[ItemLevelFeaturesModes] = field(\n        default_factory=lambda: [ItemLevelFeaturesModes.RF],\n    )\n</code></pre>"},{"location":"reference/configs/models/ml/SVM/","title":"SVM","text":""},{"location":"reference/configs/models/ml/SVM/#configs.models.ml.SVM.SupportVectorMachineMLArgs","title":"<code>SupportVectorMachineMLArgs</code>  <code>dataclass</code>","text":"<p>               Bases: <code>MLModelArgs</code></p> <p>Model arguments for the Support Vector Machine (SVM) model.</p> <p>Attributes:</p> Name Type Description <code>batch_size</code> <code>int</code> <p>The batch size for training.</p> <code>use_fixation_report</code> <code>bool</code> <p>Whether to use the fixation report.</p> <code>backbone</code> <code>str</code> <p>The backbone model to use.</p> <code>sklearn_pipeline</code> <code>tuple</code> <p>The scikit-learn pipeline for the model.</p> <code>sklearn_pipeline_param_clf__C</code> <code>float</code> <p>Regularization parameter. Inverse of regularization strength.</p> <code>sklearn_pipeline_param_clf__kernel</code> <code>str</code> <p>Specifies the kernel type to be used in the algorithm.</p> <code>sklearn_pipeline_param_clf__degree</code> <code>int</code> <p>Degree of the polynomial kernel function ('poly'). Ignored by other kernels.</p> <code>sklearn_pipeline_param_clf__gamma</code> <code>str | float</code> <p>Kernel coefficient for 'rbf', 'poly', and 'sigmoid'.</p> <code>sklearn_pipeline_param_clf__coef0</code> <code>float</code> <p>Independent term in kernel function. Relevant for 'poly' and 'sigmoid'.</p> <code>sklearn_pipeline_param_clf__shrinking</code> <code>bool</code> <p>Whether to use the shrinking heuristic.</p> <code>sklearn_pipeline_param_clf__probability</code> <code>bool</code> <p>Whether to enable probability estimates.</p> <code>sklearn_pipeline_param_clf__tol</code> <code>float</code> <p>Tolerance for stopping criterion.</p> <code>sklearn_pipeline_param_clf__random_state</code> <code>int</code> <p>Seed for shuffling the data.</p> <code>sklearn_pipeline_param_clf__class_weight</code> <code>str</code> <p>Class weights (e.g., 'balanced').</p> <code>sklearn_pipeline_param_scaler__with_mean</code> <code>bool</code> <p>If True, center the data before scaling.</p> <code>sklearn_pipeline_param_scaler__with_std</code> <code>bool</code> <p>If True, scale the data to unit variance.</p> Source code in <code>src/configs/models/ml/SVM.py</code> <pre><code>@register_model_config\n@dataclass\nclass SupportVectorMachineMLArgs(MLModelArgs):\n    \"\"\"\n    Model arguments for the Support Vector Machine (SVM) model.\n\n    Attributes:\n        batch_size (int): The batch size for training.\n        use_fixation_report (bool): Whether to use the fixation report.\n        backbone (str): The backbone model to use.\n        sklearn_pipeline (tuple): The scikit-learn pipeline for the model.\n        sklearn_pipeline_param_clf__C (float): Regularization parameter. Inverse of regularization strength.\n        sklearn_pipeline_param_clf__kernel (str): Specifies the kernel type to be used in the algorithm.\n        sklearn_pipeline_param_clf__degree (int): Degree of the polynomial kernel function ('poly'). Ignored by other kernels.\n        sklearn_pipeline_param_clf__gamma (str | float): Kernel coefficient for 'rbf', 'poly', and 'sigmoid'.\n        sklearn_pipeline_param_clf__coef0 (float): Independent term in kernel function. Relevant for 'poly' and 'sigmoid'.\n        sklearn_pipeline_param_clf__shrinking (bool): Whether to use the shrinking heuristic.\n        sklearn_pipeline_param_clf__probability (bool): Whether to enable probability estimates.\n        sklearn_pipeline_param_clf__tol (float): Tolerance for stopping criterion.\n        sklearn_pipeline_param_clf__random_state (int): Seed for shuffling the data.\n        sklearn_pipeline_param_clf__class_weight (str): Class weights (e.g., 'balanced').\n        sklearn_pipeline_param_scaler__with_mean (bool): If True, center the data before scaling.\n        sklearn_pipeline_param_scaler__with_std (bool): If True, scale the data to unit variance.\n    \"\"\"\n\n    base_model_name: MLModelNames = MLModelNames.SVM\n\n    sklearn_pipeline: tuple = (\n        ('scaler', 'sklearn.preprocessing.StandardScaler'),\n        ('clf', 'sklearn.svm.SVC'),\n    )\n    # sklearn pipeline params\n    #! note the naming convention for the parameters:\n    #! sklearn_pipeline_param_&lt;pipline_element_name&gt;__&lt;param_name&gt;\n\n    # clf params\n    sklearn_pipeline_param_clf__C: float = 100\n    sklearn_pipeline_param_clf__kernel: str = 'rbf'\n    sklearn_pipeline_param_clf__degree: int = 3  # Degree of the polynomial kernel function (\u2018poly\u2019). Must be non-negative. Ignored by all other kernels.\n    sklearn_pipeline_param_clf__gamma: str | float = 0.01\n    # sklearn_pipeline_param_clf__gamma: str = \"scale\"\n    sklearn_pipeline_param_clf__coef0: float = (\n        0.0  # It is only significant in \u2018poly\u2019 and \u2018sigmoid\u2019.\n    )\n    sklearn_pipeline_param_clf__shrinking: bool = True\n    sklearn_pipeline_param_clf__probability: bool = False\n    sklearn_pipeline_param_clf__tol: float = 0.001\n    sklearn_pipeline_param_clf__random_state: int = 1\n    sklearn_pipeline_param_clf__class_weight: str = 'balanced'\n\n    # scaler params\n    sklearn_pipeline_param_scaler__with_mean: bool = True\n    sklearn_pipeline_param_scaler__with_std: bool = True\n\n    batch_size: int = 1024\n\n    #! note logistic regression is for binary classification\n    use_fixation_report: bool = True\n    backbone: BackboneNames = BackboneNames.ROBERTA_LARGE\n    item_level_features_modes: list[ItemLevelFeaturesModes] = field(\n        default_factory=lambda: [ItemLevelFeaturesModes.SVM]\n    )\n</code></pre>"},{"location":"reference/configs/models/ml/SVM/#configs.models.ml.SVM.SupportVectorRegressorMLArgs","title":"<code>SupportVectorRegressorMLArgs</code>  <code>dataclass</code>","text":"<p>               Bases: <code>MLModelArgs</code></p> <p>Model arguments for the Support Vector Regressor (SVR) model.</p> <p>Attributes:</p> Name Type Description <code>batch_size</code> <code>int</code> <p>The batch size for training.</p> <code>use_fixation_report</code> <code>bool</code> <p>Whether to use the fixation report.</p> <code>backbone</code> <code>str</code> <p>The backbone model to use.</p> <code>sklearn_pipeline</code> <code>tuple</code> <p>The scikit-learn pipeline for the model.</p> <code>sklearn_pipeline_param_reg__C</code> <code>float</code> <p>Regularization parameter.</p> <code>sklearn_pipeline_param_reg__kernel</code> <code>str</code> <p>Specifies the kernel type to be used in the algorithm.</p> <code>sklearn_pipeline_param_reg__degree</code> <code>int</code> <p>Degree of the polynomial kernel function ('poly'). Ignored by other kernels.</p> <code>sklearn_pipeline_param_reg__gamma</code> <code>str | float</code> <p>Kernel coefficient for 'rbf', 'poly', and 'sigmoid'.</p> <code>sklearn_pipeline_param_reg__coef0</code> <code>float</code> <p>Independent term in kernel function. Relevant for 'poly' and 'sigmoid'.</p> <code>sklearn_pipeline_param_reg__shrinking</code> <code>bool</code> <p>Whether to use the shrinking heuristic.</p> <code>sklearn_pipeline_param_reg__tol</code> <code>float</code> <p>Tolerance for stopping criterion.</p> <code>sklearn_pipeline_param_reg__epsilon</code> <code>float</code> <p>Epsilon in the epsilon-SVR model.</p> <code>sklearn_pipeline_param_scaler__with_mean</code> <code>bool</code> <p>If True, center the data before scaling.</p> <code>sklearn_pipeline_param_scaler__with_std</code> <code>bool</code> <p>If True, scale the data to unit variance.</p> Source code in <code>src/configs/models/ml/SVM.py</code> <pre><code>@register_model_config\n@dataclass\nclass SupportVectorRegressorMLArgs(MLModelArgs):\n    \"\"\"\n    Model arguments for the Support Vector Regressor (SVR) model.\n\n    Attributes:\n        batch_size (int): The batch size for training.\n        use_fixation_report (bool): Whether to use the fixation report.\n        backbone (str): The backbone model to use.\n        sklearn_pipeline (tuple): The scikit-learn pipeline for the model.\n        sklearn_pipeline_param_reg__C (float): Regularization parameter.\n        sklearn_pipeline_param_reg__kernel (str): Specifies the kernel type to be used in the algorithm.\n        sklearn_pipeline_param_reg__degree (int): Degree of the polynomial kernel function ('poly'). Ignored by other kernels.\n        sklearn_pipeline_param_reg__gamma (str | float): Kernel coefficient for 'rbf', 'poly', and 'sigmoid'.\n        sklearn_pipeline_param_reg__coef0 (float): Independent term in kernel function. Relevant for 'poly' and 'sigmoid'.\n        sklearn_pipeline_param_reg__shrinking (bool): Whether to use the shrinking heuristic.\n        sklearn_pipeline_param_reg__tol (float): Tolerance for stopping criterion.\n        sklearn_pipeline_param_reg__epsilon (float): Epsilon in the epsilon-SVR model.\n        sklearn_pipeline_param_scaler__with_mean (bool): If True, center the data before scaling.\n        sklearn_pipeline_param_scaler__with_std (bool): If True, scale the data to unit variance.\n    \"\"\"\n\n    base_model_name: MLModelNames = MLModelNames.SVM_REG\n\n    sklearn_pipeline: tuple = (\n        ('scaler', 'sklearn.preprocessing.StandardScaler'),\n        ('reg', 'sklearn.svm.SVR'),\n    )\n\n    # regressor params\n    sklearn_pipeline_param_reg__C: float = 100\n    sklearn_pipeline_param_reg__kernel: str = 'rbf'\n    sklearn_pipeline_param_reg__degree: int = 3\n    sklearn_pipeline_param_reg__gamma: str | float = 0.01\n    sklearn_pipeline_param_reg__coef0: float = 0.0\n    sklearn_pipeline_param_reg__shrinking: bool = True\n    sklearn_pipeline_param_reg__tol: float = 0.001\n    sklearn_pipeline_param_reg__epsilon: float = 0.1\n\n    # scaler params\n    sklearn_pipeline_param_scaler__with_mean: bool = True\n    sklearn_pipeline_param_scaler__with_std: bool = True\n\n    batch_size: int = 1024\n    use_fixation_report: bool = True\n    backbone: BackboneNames = BackboneNames.ROBERTA_LARGE\n    item_level_features_modes: list[ItemLevelFeaturesModes] = field(\n        default_factory=lambda: [ItemLevelFeaturesModes.SVM]\n    )\n</code></pre>"},{"location":"reference/configs/models/ml/XGBoost/","title":"XGBoost","text":""},{"location":"reference/configs/models/ml/XGBoost/#configs.models.ml.XGBoost.XGBoostMLArgs","title":"<code>XGBoostMLArgs</code>  <code>dataclass</code>","text":"<p>               Bases: <code>MLModelArgs</code></p> <p>Model arguments for the XGBoost model.</p> <p>Attributes:</p> Name Type Description <code>batch_size</code> <code>int</code> <p>The batch size for training.</p> <code>use_fixation_report</code> <code>bool</code> <p>Whether to use the fixation report.</p> <code>backbone</code> <code>str</code> <p>The backbone model to use.</p> <code>pca_explained_variance_ratio_threshold</code> <code>float</code> <p>Threshold for PCA explained variance ratio.</p> <code>sklearn_pipeline</code> <code>tuple</code> <p>The scikit-learn pipeline for the model.</p> <code>sklearn_pipeline_param_clf__learning_rate</code> <code>float</code> <p>Learning rate for the XGBoost model.</p> <code>sklearn_pipeline_param_clf__min_child_weight</code> <code>int</code> <p>Minimum sum of instance weight (hessian) needed in a child.</p> <code>sklearn_pipeline_param_clf__gamma</code> <code>float</code> <p>Minimum loss reduction required to make a further partition on a leaf node of the tree.</p> <code>sklearn_pipeline_param_clf__n_estimators</code> <code>int</code> <p>Number of gradient boosted trees.</p> <code>sklearn_pipeline_param_clf__max_depth</code> <code>int</code> <p>Maximum depth of a tree.</p> <code>sklearn_pipeline_param_clf__colsample_bytree</code> <code>float</code> <p>Subsample ratio of columns when constructing each tree.</p> <code>sklearn_pipeline_param_clf__alpha</code> <code>float</code> <p>L1 regularization term on weights.</p> <code>sklearn_pipeline_param_clf__lambda</code> <code>float</code> <p>L2 regularization term on weights.</p> <code>sklearn_pipeline_param_clf__booster</code> <code>str</code> <p>Type of booster to use.</p> <code>sklearn_pipeline_params_clf__device</code> <code>str</code> <p>Device to use for training (e.g., \"gpu\").</p> <code>sklearn_pipeline_param_scaler__with_mean</code> <code>bool</code> <p>If True, center the data before scaling.</p> <code>sklearn_pipeline_param_scaler__with_std</code> <code>bool</code> <p>If True, scale the data to unit variance (or equivalently, unit standard deviation).</p> Source code in <code>src/configs/models/ml/XGBoost.py</code> <pre><code>@register_model_config\n@dataclass\nclass XGBoostMLArgs(MLModelArgs):\n    \"\"\"\n    Model arguments for the XGBoost model.\n\n    Attributes:\n        batch_size (int): The batch size for training.\n        use_fixation_report (bool): Whether to use the fixation report.\n        backbone (str): The backbone model to use.\n        pca_explained_variance_ratio_threshold (float): Threshold for PCA explained variance ratio.\n        sklearn_pipeline (tuple): The scikit-learn pipeline for the model.\n        sklearn_pipeline_param_clf__learning_rate (float): Learning rate for the XGBoost model.\n        sklearn_pipeline_param_clf__min_child_weight (int): Minimum sum of instance weight (hessian) needed in a child.\n        sklearn_pipeline_param_clf__gamma (float): Minimum loss reduction required to make a further partition on a leaf node of the tree.\n        sklearn_pipeline_param_clf__n_estimators (int): Number of gradient boosted trees.\n        sklearn_pipeline_param_clf__max_depth (int): Maximum depth of a tree.\n        sklearn_pipeline_param_clf__colsample_bytree (float): Subsample ratio of columns when constructing each tree.\n        sklearn_pipeline_param_clf__alpha (float): L1 regularization term on weights.\n        sklearn_pipeline_param_clf__lambda (float): L2 regularization term on weights.\n        sklearn_pipeline_param_clf__booster (str): Type of booster to use.\n        sklearn_pipeline_params_clf__device (str): Device to use for training (e.g., \"gpu\").\n        sklearn_pipeline_param_scaler__with_mean (bool): If True, center the data before scaling.\n        sklearn_pipeline_param_scaler__with_std (bool): If True, scale the data to unit variance (or equivalently, unit standard deviation).\n    \"\"\"\n\n    base_model_name: MLModelNames = MLModelNames.XGBOOST\n\n    sklearn_pipeline: tuple = (\n        ('scaler', 'sklearn.preprocessing.StandardScaler'),\n        ('clf', 'xgboost.XGBClassifier'),\n    )\n\n    # sklearn pipeline params\n    #! note the naming convention for the parameters:\n    #! sklearn_pipeline_param_&lt;pipline_element_name&gt;__&lt;param_name&gt;\n\n    # clf params\n    sklearn_pipeline_param_clf__learning_rate: float = 0.01\n    sklearn_pipeline_param_clf__min_child_weight: int = 1\n    sklearn_pipeline_param_clf__gamma: float = 0\n    sklearn_pipeline_param_clf__n_estimators: int = 1000\n    sklearn_pipeline_param_clf__max_depth: int = 6\n    sklearn_pipeline_param_clf__colsample_bytree: float = 1.0\n    sklearn_pipeline_param_clf__alpha: float = 0\n    sklearn_pipeline_param_clf__lambda: float = 1\n    sklearn_pipeline_param_clf__booster: str = 'gbtree'\n\n    # sklearn_pipeline_param_clf__scale_pos_weight: float = sqrt(\n    #     83.6 / 16.4\n    # )  # the ratio between 0 and 1 in the reread column of the train set of fold 0\n    sklearn_pipeline_params_clf__device: str = 'gpu'\n    # sklearn_pipeline_param_clf__shrinking: bool = True\n    # sklearn_pipeline_param_clf__probability: bool = False\n    # sklearn_pipeline_param_clf__tol: float = 0.001\n    # sklearn_pipeline_param_clf__random_state: int = 1\n    # sklearn_pipeline_param_clf__class_weight: str = \"balanced\"\n\n    # scaler params\n    sklearn_pipeline_param_scaler__with_mean: bool = True\n    sklearn_pipeline_param_scaler__with_std: bool = True\n\n    batch_size: int = 1024\n\n    #! note logistic regression is for binary classification\n    use_fixation_report: bool = True\n    backbone: BackboneNames = BackboneNames.ROBERTA_LARGE\n    item_level_features_modes: list[ItemLevelFeaturesModes] = field(\n        default_factory=lambda: [ItemLevelFeaturesModes.RF],\n    )\n</code></pre>"},{"location":"reference/configs/models/ml/XGBoost/#configs.models.ml.XGBoost.XGBoostRegressorMLArgs","title":"<code>XGBoostRegressorMLArgs</code>  <code>dataclass</code>","text":"<p>               Bases: <code>MLModelArgs</code></p> <p>Model arguments for the XGBoost regressor model.</p> <p>Attributes:</p> Name Type Description <code>batch_size</code> <code>int</code> <p>The batch size for training.</p> <code>use_fixation_report</code> <code>bool</code> <p>Whether to use the fixation report.</p> <code>backbone</code> <code>str</code> <p>The backbone model to use.</p> <code>pca_explained_variance_ratio_threshold</code> <code>float</code> <p>Threshold for PCA explained variance ratio.</p> <code>sklearn_pipeline</code> <code>tuple</code> <p>The scikit-learn pipeline for the model.</p> <code>sklearn_pipeline_param_reg__learning_rate</code> <code>float</code> <p>Learning rate for the XGBoost model.</p> <code>sklearn_pipeline_param_reg__min_child_weight</code> <code>int</code> <p>Minimum sum of instance weight (hessian) needed in a child.</p> <code>sklearn_pipeline_param_reg__gamma</code> <code>float</code> <p>Minimum loss reduction required to make a further partition on a leaf node of the tree.</p> <code>sklearn_pipeline_param_reg__n_estimators</code> <code>int</code> <p>Number of gradient boosted trees.</p> <code>sklearn_pipeline_param_reg__max_depth</code> <code>int</code> <p>Maximum depth of a tree.</p> <code>sklearn_pipeline_param_reg__colsample_bytree</code> <code>float</code> <p>Subsample ratio of columns when constructing each tree.</p> <code>sklearn_pipeline_param_reg__alpha</code> <code>float</code> <p>L1 regularization term on weights.</p> <code>sklearn_pipeline_param_reg__lambda</code> <code>float</code> <p>L2 regularization term on weights.</p> <code>sklearn_pipeline_param_reg__booster</code> <code>str</code> <p>Type of booster to use.</p> <code>sklearn_pipeline_params_reg__device</code> <code>str</code> <p>Device to use for training (e.g., \"gpu\").</p> <code>sklearn_pipeline_param_scaler__with_mean</code> <code>bool</code> <p>If True, center the data before scaling.</p> <code>sklearn_pipeline_param_scaler__with_std</code> <code>bool</code> <p>If True, scale the data to unit variance (or equivalently, unit standard deviation).</p> Source code in <code>src/configs/models/ml/XGBoost.py</code> <pre><code>@register_model_config\n@dataclass\nclass XGBoostRegressorMLArgs(MLModelArgs):\n    \"\"\"\n    Model arguments for the XGBoost regressor model.\n\n    Attributes:\n        batch_size (int): The batch size for training.\n        use_fixation_report (bool): Whether to use the fixation report.\n        backbone (str): The backbone model to use.\n        pca_explained_variance_ratio_threshold (float): Threshold for PCA explained variance ratio.\n        sklearn_pipeline (tuple): The scikit-learn pipeline for the model.\n        sklearn_pipeline_param_reg__learning_rate (float): Learning rate for the XGBoost model.\n        sklearn_pipeline_param_reg__min_child_weight (int): Minimum sum of instance weight (hessian) needed in a child.\n        sklearn_pipeline_param_reg__gamma (float): Minimum loss reduction required to make a further partition on a leaf node of the tree.\n        sklearn_pipeline_param_reg__n_estimators (int): Number of gradient boosted trees.\n        sklearn_pipeline_param_reg__max_depth (int): Maximum depth of a tree.\n        sklearn_pipeline_param_reg__colsample_bytree (float): Subsample ratio of columns when constructing each tree.\n        sklearn_pipeline_param_reg__alpha (float): L1 regularization term on weights.\n        sklearn_pipeline_param_reg__lambda (float): L2 regularization term on weights.\n        sklearn_pipeline_param_reg__booster (str): Type of booster to use.\n        sklearn_pipeline_params_reg__device (str): Device to use for training (e.g., \"gpu\").\n        sklearn_pipeline_param_scaler__with_mean (bool): If True, center the data before scaling.\n        sklearn_pipeline_param_scaler__with_std (bool): If True, scale the data to unit variance (or equivalently, unit standard deviation).\n    \"\"\"\n\n    base_model_name: MLModelNames = MLModelNames.XGBOOST_REG\n\n    sklearn_pipeline: tuple = (\n        ('scaler', 'sklearn.preprocessing.StandardScaler'),\n        ('reg', 'xgboost.XGBRegressor'),\n    )\n\n    # sklearn pipeline params\n    #! note the naming convention for the parameters:\n    #! sklearn_pipeline_param_&lt;pipline_element_name&gt;__&lt;param_name&gt;\n\n    # regressor params\n    sklearn_pipeline_param_reg__learning_rate: float = 0.01\n    sklearn_pipeline_param_reg__min_child_weight: int = 1\n    sklearn_pipeline_param_reg__gamma: float = 0\n    sklearn_pipeline_param_reg__n_estimators: int = 1000\n    sklearn_pipeline_param_reg__max_depth: int = 6\n    sklearn_pipeline_param_reg__colsample_bytree: float = 1.0\n    sklearn_pipeline_param_reg__alpha: float = 0\n    sklearn_pipeline_param_reg__lambda: float = 1\n    sklearn_pipeline_param_reg__booster: str = 'gbtree'\n    sklearn_pipeline_params_reg__device: str = 'gpu'\n\n    # scaler params\n    sklearn_pipeline_param_scaler__with_mean: bool = True\n    sklearn_pipeline_param_scaler__with_std: bool = True\n\n    batch_size: int = 1024\n\n    use_fixation_report: bool = True\n    backbone: BackboneNames = BackboneNames.ROBERTA_LARGE\n    item_level_features_modes: list[ItemLevelFeaturesModes] = field(\n        default_factory=lambda: [ItemLevelFeaturesModes.RF],\n    )\n</code></pre>"},{"location":"reference/configs/models/ml/__init__/","title":"init","text":""},{"location":"reference/data/utils/","title":"utils","text":""},{"location":"reference/data/utils/#data.utils.add_missing_categories_and_flatten","title":"<code>add_missing_categories_and_flatten(grouped_gsf_features, groupby_fields, groupby_type_)</code>","text":"<p>Add missing categories and flatten the grouped GSF features.</p> <p>Parameters:</p> Name Type Description Default <code>grouped_gsf_features</code> <code>DataFrame</code> <p>The grouped GSF features.</p> required <code>groupby_fields</code> <code>list</code> <p>The fields to group by.</p> required <code>groupby_type_</code> <code>str</code> <p>The type of grouping.</p> required <p>Returns:</p> Type Description <code>dict[str, int | float | float64]</code> <p>dict[str, int | float | np.float64]: The flattened GSF features.</p> Source code in <code>src/data/utils.py</code> <pre><code>def add_missing_categories_and_flatten(\n    grouped_gsf_features: pd.DataFrame,\n    groupby_fields: list[\n        float | int | str | np.int64 | None | np.float64 | pd._libs.missing.NAType\n    ],\n    groupby_type_: str,\n) -&gt; dict[str, int | float | np.float64]:\n    \"\"\"\n    Add missing categories and flatten the grouped GSF features.\n\n    Args:\n        grouped_gsf_features (pd.DataFrame): The grouped GSF features.\n        groupby_fields (list): The fields to group by.\n        groupby_type_ (str): The type of grouping.\n\n    Returns:\n        dict[str, int | float | np.float64]: The flattened GSF features.\n    \"\"\"\n    new_index = (\n        grouped_gsf_features.index.union(\n            pd.Index(groupby_fields),\n        )\n        .drop_duplicates()\n        .dropna()\n    )\n    if len(groupby_fields) &lt; len(new_index):\n        logger.warning(\n            f'Missing categories: {new_index.difference(groupby_fields)} in {\n                groupby_type_\n            }!',\n        )\n    grouped_gsf_features = grouped_gsf_features.reindex(\n        new_index,\n        fill_value=0,\n    )\n    grouped_df_reset = grouped_gsf_features.reset_index()\n\n    melted_ = grouped_df_reset.melt(\n        # Use the first column as the id_vars\n        id_vars=grouped_df_reset.columns[0],\n        var_name='variable',  # Name of the new variable column\n        value_name='value',  # Name of the new value column\n    )\n    # If you want to add the groupby_type_ to the feature name so have feature names\n    melted_['feature_name'] = (\n        groupby_type_ + '_' + melted_['index'].astype(str) + '_' + melted_['variable']\n    )\n    res_df = (\n        melted_[['feature_name', 'value']]\n        .set_index(\n            'feature_name',\n        )\n        .sort_index()\n    )\n    # create a dict {feature_name: value}\n    res_dict = res_df.to_dict()['value']\n    return res_dict\n</code></pre>"},{"location":"reference/data/utils/#data.utils.add_missing_features","title":"<code>add_missing_features(et_data, trial_groupby_columns, mode)</code>","text":"<p>Add and transform features in the given DataFrame.</p> <p>This function adds and transforms several features in the DataFrame. It also creates new features based on existing ones.</p> <p>Parameters:</p> Name Type Description Default <code>et_data</code> <code>DataFrame</code> <p>The input DataFrame. It should have the following columns: - ptb_pos - is_content_word - NEXT_FIX_INTEREST_AREA_INDEX - CURRENT_FIX_INTEREST_AREA_INDEX - IA_REGRESSION_IN_COUNT - IA_REGRESSION_OUT_FULL_COUNT - IA_FIXATION_COUNT</p> required <code>trial_groupby_columns</code> <code>list</code> <p>A list of column names to group by when calculating sums.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: The DataFrame with added and transformed features.</p> <code>DataFrame</code> <p>The function creates the following new features: - ptb_pos: Transformed from categorical to numerical using a mapping dictionary. - is_content_word: Converted to integer type. - is_reg: Whether the next fixation interest area index is less than the current one. - is_progressive: Whether the next fixation IA index is greater than the current one. - is_reg_sum: The sum of is_reg for each group defined by trial_groupby_columns. - is_progressive_sum:     The sum of is_progressive for each group defined by trial_groupby_columns. - IA_REGRESSION_IN_COUNT_sum:     The sum of IA_REGRESSION_IN_COUNT for each group defined by trial_groupby_columns. - normalized_outgoing_regression_count:     The ratio of IA_REGRESSION_OUT_FULL_COUNT to is_reg_sum. - normalized_outgoing_progressive_count:     The ratio of the difference between IA_FIXATION_COUNT and     IA_REGRESSION_OUT_FULL_COUNT to is_progressive_sum. - normalized_incoming_regression_count:     The ratio of IA_REGRESSION_IN_COUNT to IA_REGRESSION_IN_COUNT_sum.</p> Source code in <code>src/data/utils.py</code> <pre><code>def add_missing_features(\n    et_data: pd.DataFrame,\n    trial_groupby_columns: list[str],\n    mode: DataType,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Add and transform features in the given DataFrame.\n\n    This function adds and transforms several features in the DataFrame. It also creates\n    new features based on existing ones.\n\n    Args:\n        et_data (pd.DataFrame): The input DataFrame. It should have the following columns:\n            - ptb_pos\n            - is_content_word\n            - NEXT_FIX_INTEREST_AREA_INDEX\n            - CURRENT_FIX_INTEREST_AREA_INDEX\n            - IA_REGRESSION_IN_COUNT\n            - IA_REGRESSION_OUT_FULL_COUNT\n            - IA_FIXATION_COUNT\n        trial_groupby_columns (list): A list of column names to group by when calculating sums.\n\n    Returns:\n        pd.DataFrame: The DataFrame with added and transformed features.\n        The function creates the following new features:\n            - ptb_pos: Transformed from categorical to numerical using a mapping dictionary.\n            - is_content_word: Converted to integer type.\n            - is_reg: Whether the next fixation interest area index is less than the current one.\n            - is_progressive: Whether the next fixation IA index is greater than the current one.\n            - is_reg_sum: The sum of is_reg for each group defined by trial_groupby_columns.\n            - is_progressive_sum:\n                The sum of is_progressive for each group defined by trial_groupby_columns.\n            - IA_REGRESSION_IN_COUNT_sum:\n                The sum of IA_REGRESSION_IN_COUNT for each group defined by trial_groupby_columns.\n            - normalized_outgoing_regression_count:\n                The ratio of IA_REGRESSION_OUT_FULL_COUNT to is_reg_sum.\n            - normalized_outgoing_progressive_count:\n                The ratio of the difference between IA_FIXATION_COUNT and\n                IA_REGRESSION_OUT_FULL_COUNT to is_progressive_sum.\n            - normalized_incoming_regression_count:\n                The ratio of IA_REGRESSION_IN_COUNT to IA_REGRESSION_IN_COUNT_sum.\n            # These are used for Syntactic Clusters with\n            # Universal Dependencies PoS and Information Clusters [Berzak et al. 2017]\n            - LengthCategory:\n                The length category of the word based on the word_length column.\n            - LengthCategory_normalized_IA_DWELL_TIME:\n                IA_DWELL_TIME normalized by the mean IA_DWELL_TIME of the LengthCategory group.\n            - POS_normalized_IA_DWELL_TIME:\n                IA_DWELL_TIME normalized by the mean IA_DWELL_TIME of the universal_pos group.\n            - LengthCategory_normalized_IA_FIRST_FIXATION_DURATION:\n                IA_FIRST_FIXATION_DURATION normalized by the mean IA_FIRST_FIXATION_DURATION of the\n                LengthCategory group.\n            - POS_normalized_IA_FIRST_FIXATION_DURATION:\n                IA_FIRST_FIXATION_DURATION normalized by the mean IA_FIRST_FIXATION_DURATION of the\n                universal_pos group.\n    \"\"\"\n    # Map ptb_pos values to numbers\n    value_to_number = {'FUNC': 0, 'NOUN': 1, 'VERB': 2, 'ADJ': 3, 'UNKNOWN': 4}\n    et_data['ptb_pos'] = et_data['ptb_pos'].map(value_to_number)\n\n    # Convert is_content_word to integer\n    et_data['is_content_word'] = et_data['is_content_word'].astype('Int64')\n\n    # TODO Add reference to a paper for these bins?\n    # Define the boundaries of the bins by word length\n    bins = [0, 2, 5, 11, np.inf]  # 0-1, 2-4, 5-10, 11+\n    # Define the labels for the bins\n    labels = [0, 1, 2, 3]\n    et_data['LengthCategory'] = pd.cut(\n        et_data['word_length'],\n        bins=bins,\n        labels=labels,\n        right=False,\n    )\n\n    if mode == DataType.FIXATIONS:\n        # Add is_reg and is_progressive features\n        et_data['is_reg'] = (\n            et_data['NEXT_FIX_INTEREST_AREA_INDEX']\n            &lt; et_data['CURRENT_FIX_INTEREST_AREA_INDEX']\n        )\n        et_data['is_progressive'] = (\n            et_data['NEXT_FIX_INTEREST_AREA_INDEX']\n            &gt; et_data['CURRENT_FIX_INTEREST_AREA_INDEX']\n        )\n\n        # Calculate sums for is_reg, is_progressive, and IA_REGRESSION_IN_COUNT\n        grouped_sums = et_data.groupby(trial_groupby_columns)[\n            ['is_reg', 'is_progressive', 'IA_REGRESSION_IN_COUNT']\n        ].transform('sum')\n\n        # Add sum features\n        et_data['is_reg_sum'] = grouped_sums['is_reg']\n        et_data['is_progressive_sum'] = grouped_sums['is_progressive']\n        et_data['IA_REGRESSION_IN_COUNT_sum'] = grouped_sums['IA_REGRESSION_IN_COUNT']\n\n        # Add normalized count features\n        et_data['normalized_outgoing_regression_count'] = (\n            et_data['IA_REGRESSION_OUT_FULL_COUNT'] / et_data['is_reg_sum']\n        )\n        et_data['normalized_outgoing_progressive_count'] = (\n            et_data['IA_FIXATION_COUNT'] - et_data['IA_REGRESSION_OUT_FULL_COUNT']\n        ) / et_data['is_progressive_sum']  # approximation\n        et_data['normalized_incoming_regression_count'] = (\n            et_data['IA_REGRESSION_IN_COUNT'] / et_data['IA_REGRESSION_IN_COUNT_sum']\n        )\n        et_data = et_data.replace([np.inf, -np.inf], 0)\n        et_data.fillna(\n            {\n                'normalized_outgoing_regression_count': 0,\n                'normalized_outgoing_progressive_count': 0,\n                'normalized_incoming_regression_count': 0,\n            },\n            inplace=True,\n        )\n\n    et_data.fillna(\n        {\n            'LengthCategory_normalized_IA_DWELL_TIME': 0,\n            'universal_pos_normalized_IA_DWELL_TIME': 0,\n            'LengthCategory_normalized_IA_FIRST_FIXATION_DURATION': 0,\n            'universal_pos_normalized_IA_FIRST_FIXATION_DURATION': 0,\n        },\n        inplace=True,\n    )\n\n    return et_data\n</code></pre>"},{"location":"reference/data/utils/#data.utils.add_missing_features--these-are-used-for-syntactic-clusters-with","title":"These are used for Syntactic Clusters with","text":""},{"location":"reference/data/utils/#data.utils.add_missing_features--universal-dependencies-pos-and-information-clusters-berzak-et-al-2017","title":"Universal Dependencies PoS and Information Clusters [Berzak et al. 2017]","text":"<ul> <li>LengthCategory:     The length category of the word based on the word_length column.</li> <li>LengthCategory_normalized_IA_DWELL_TIME:     IA_DWELL_TIME normalized by the mean IA_DWELL_TIME of the LengthCategory group.</li> <li>POS_normalized_IA_DWELL_TIME:     IA_DWELL_TIME normalized by the mean IA_DWELL_TIME of the universal_pos group.</li> <li>LengthCategory_normalized_IA_FIRST_FIXATION_DURATION:     IA_FIRST_FIXATION_DURATION normalized by the mean IA_FIRST_FIXATION_DURATION of the     LengthCategory group.</li> <li>POS_normalized_IA_FIRST_FIXATION_DURATION:     IA_FIRST_FIXATION_DURATION normalized by the mean IA_FIRST_FIXATION_DURATION of the     universal_pos group.</li> </ul>"},{"location":"reference/data/utils/#data.utils.compute_fixation_trial_level_features","title":"<code>compute_fixation_trial_level_features(trial, groupby_mappings, processed_data_path)</code>","text":"<p>Compute fixation trial-level features.</p> <p>Parameters:</p> Name Type Description Default <code>trial</code> <code>DataFrame</code> <p>The trial data.</p> required <code>groupby_mappings</code> <code>list[tuple]</code> <p>The groupby mappings for categorical features.</p> required <code>processed_data_path</code> <code>Path</code> <p>The path to save the trial level feature names.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>pd.Series: The computed features.</p> Source code in <code>src/data/utils.py</code> <pre><code>def compute_fixation_trial_level_features(\n    trial: pd.DataFrame, groupby_mappings: list[tuple], processed_data_path: Path\n) -&gt; dict:\n    \"\"\"\n    Compute fixation trial-level features.\n\n    Args:\n        trial (pd.DataFrame): The trial data.\n        groupby_mappings (list[tuple]): The groupby mappings for categorical features.\n        processed_data_path (Path): The path to save the trial level feature names.\n\n    Returns:\n        pd.Series: The computed features.\n    \"\"\"\n\n    RF = {}\n    for column in numerical_fixation_trial_columns:\n        if column in trial.columns:\n            for aggregation_method in numerical_feature_aggregations:\n                key = 'fix_feature_' + aggregation_method + '_' + column\n                val = get_feature_from_list(\n                    trial[column].replace('.', np.nan).astype(float),\n                    aggregation_method,\n                )\n                RF.update({key: val})\n\n    ####### David Eyes Only ######\n    BEYELSTM = {}\n    gaze_features = get_gaze_entropy_features(\n        x_means=trial['CURRENT_FIX_X'].values,  # type: ignore\n        y_means=trial['CURRENT_FIX_Y'].values,  # type: ignore\n    )\n    BEYELSTM.update(gaze_features)\n\n    BEYELSTM['total_num_fixations'] = len(trial)\n    BEYELSTM['total_num_words'] = (\n        trial['TRIAL_IA_COUNT'].drop_duplicates().dropna().values[0]\n    )\n\n    ##### David #####\n    # Creates\n    # 'LengthCategory_normalized_IA_FIRST_FIXATION_DURATION',\n    # 'LengthCategory_normalized_IA_DWELL_TIME',\n    # 'universal_pos_normalized_IA_DWELL_TIME',\n    # 'universal_pos_normalized_IA_FIRST_FIXATION_DURATION',\n    for cluster_by in ['LengthCategory', 'universal_pos']:\n        # FutureWarning: The default of observed=False is deprecated and will be changed to True\n        # in a future version of pandas. Pass observed=False to retain current behavior or\n        # observed=True to adopt the future default and silence this warning.\n        try:\n            grouped_means = trial.groupby(cluster_by, observed=False)[\n                ['IA_DWELL_TIME', 'IA_FIRST_FIXATION_DURATION']\n            ].transform('mean')\n        except IndexError:\n            grouped_means['IA_DWELL_TIME'] = np.nan\n            grouped_means['IA_FIRST_FIXATION_DURATION'] = np.nan\n        for et_measure in ['IA_DWELL_TIME', 'IA_FIRST_FIXATION_DURATION']:\n            trial[f'{cluster_by}_normalized_{et_measure}'] = (\n                trial[et_measure] / grouped_means[et_measure]\n            )\n    # No. values in each groupby type:\n    # is_content_word 2 (in beyelstm originally 3)\n    # ptb_pos 5 (in beyelstm originally 5)\n    # entity_type 20 (in beyelstm originally 11)\n    # universal_pos 17 (in beyelstm originally 16)\n    for groupby_type_, groupby_fields in groupby_mappings:\n        # TODO This shouldn't be hardcoded here\n        if groupby_type_ == 'ptb_pos':\n            value_to_number = {\n                'FUNC': 0,\n                'NOUN': 1,\n                'VERB': 2,\n                'ADJ': 3,\n                'UNKNOWN': 4,\n            }\n            trial['ptb_pos'] = trial['ptb_pos'].map(value_to_number)\n        grouped_gsf_features = trial.groupby(groupby_type_)[gsf_features].mean()\n        melted_gsf_features = add_missing_categories_and_flatten(\n            grouped_gsf_features=grouped_gsf_features,\n            groupby_fields=groupby_fields,\n            groupby_type_=groupby_type_,\n        )\n        for feature_name, feature_value in melted_gsf_features.items():\n            BEYELSTM[feature_name] = feature_value\n\n    SVM = {}\n    # mean saccade duration -&gt; mean \"NEXT_SAC_DURATION\"\n    to_compute_features = [\n        'NEXT_SAC_DURATION',\n        'NEXT_SAC_AVG_VELOCITY',\n        'NEXT_SAC_AMPLITUDE',\n    ]\n    for feature_to_compute in to_compute_features:\n        SVM[feature_to_compute + '_mean'] = trial[feature_to_compute].mean()\n        SVM[feature_to_compute + '_max'] = trial[feature_to_compute].max()\n\n    # Diane\n    LOGISTIC = {}\n    LOGISTIC['CURRENT_FIX_DURATION_mean'] = trial['CURRENT_FIX_DURATION'].mean()\n\n    # mean forward saccade length:\n    # * \"normalized_ID_plus_1\" = \"normalized_ID\" of the next fixation (row)\n    trial['normalized_ID_plus_1'] = trial['normalized_ID'].shift(-1)\n    # * mean \"NEXT_SAC_AMPLITUDE\" where \"normalized_ID_plus_1\" &gt; \"normalized_ID\"\n    forward_saccade_length = trial[\n        trial['normalized_ID_plus_1'] &gt; trial['normalized_ID']\n    ]['NEXT_SAC_AMPLITUDE'].mean()\n    LOGISTIC['forward_saccade_length_mean'] = forward_saccade_length\n\n    # regression rate - backward saccade rate\n    # * using \"normalized_ID_plus_1\" = \"normalized_ID\" of the next fixation (row)\n    # * regression rate - % of rows where \"normalized_ID_plus_1\" &lt; \"normalized_ID\"\n    regression_rate = (\n        trial['normalized_ID_plus_1'] &lt; trial['normalized_ID']\n    ).sum() / len(trial)\n    LOGISTIC['regression_rate'] = regression_rate\n\n    features_dict = {\n        'RF': RF,\n        'BEYELSTM': BEYELSTM,\n        'SVM': SVM,\n        'LOGISTIC': LOGISTIC,\n    }\n    save_feature_names_if_do_not_exist(\n        features_dict=features_dict,\n        csv_path=processed_data_path / 'fixation_trial_level_feature_keys.csv',\n        mode=DataType.FIXATIONS,\n    )\n\n    return RF | BEYELSTM | SVM | LOGISTIC\n</code></pre>"},{"location":"reference/data/utils/#data.utils.compute_ia_trial_level_features","title":"<code>compute_ia_trial_level_features(trial, processed_data_path)</code>","text":"<p>Compute IA trial-level features.</p> <p>Parameters:</p> Name Type Description Default <code>trial</code> <code>DataFrame</code> <p>The trial data.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>pd.Series: The computed features.</p> Source code in <code>src/data/utils.py</code> <pre><code>def compute_ia_trial_level_features(\n    trial: pd.DataFrame, processed_data_path: Path\n) -&gt; dict:\n    \"\"\"\n    Compute IA trial-level features.\n\n    Args:\n        trial (pd.DataFrame): The trial data.\n\n    Returns:\n        pd.Series: The computed features.\n    \"\"\"\n\n    RF = {}\n    for column in numerical_ia_trial_columns:\n        if column in trial.columns:\n            for aggregation_method in numerical_feature_aggregations:\n                RF.update(\n                    {\n                        'ia_feature_'\n                        + aggregation_method\n                        + '_'\n                        + column: get_feature_from_list(\n                            trial[column].astype(float), aggregation_method\n                        )\n                    }\n                )\n\n    SVM = {}\n    SVM.update(\n        {\n            'skip_rate': trial['total_skip'].mean(),\n            'num_of_fixations': trial['IA_FIXATION_COUNT'].sum(),\n            'mean_TFD': trial['IA_DWELL_TIME'].mean(),\n        }\n    )\n\n    # Diane\n    # https://tmalsburg.github.io/MeziereEtAl2021MS.pdf\n    # go-past time (i.e., the sum of fixations on a word up to when it\n    # is exited to its right, including all regressions to the left of the word\n    LOGISTIC = {}\n    LOGISTIC.update(\n        {\n            'first_pass_skip_rate': trial['IA_SKIP'].mean(),\n            'mean_FFD': trial['IA_FIRST_FIXATION_DURATION'].mean(),\n            'mean_GD': trial['IA_FIRST_RUN_DWELL_TIME'].mean(),\n            'mean_TFD': trial['IA_DWELL_TIME'].mean(),\n            'mean_go_past_time': trial['IA_SELECTIVE_REGRESSION_PATH_DURATION'].mean(),\n            'reading_speed': calc_reading_speed(trial),\n        }\n    )\n\n    READING_SPEED = {'reading_speed': calc_reading_speed(trial)}\n\n    features_dict = {\n        'RF': RF,\n        'SVM': SVM,\n        'LOGISTIC': LOGISTIC,\n        'READING_SPEED': READING_SPEED,\n    }\n    save_feature_names_if_do_not_exist(\n        features_dict=features_dict,\n        csv_path=processed_data_path / 'ia_trial_level_feature_keys.csv',\n        mode=DataType.IA,\n    )\n\n    return RF | SVM | LOGISTIC | READING_SPEED\n</code></pre>"},{"location":"reference/data/utils/#data.utils.compute_trial_level_features","title":"<code>compute_trial_level_features(raw_fixation_data, raw_ia_data, trial_groupby_columns, processed_data_path)</code>","text":"<p>Compute trial-level features in parallel.</p> <p>Parameters:</p> Name Type Description Default <code>raw_fixation_data</code> <code>DataFrame | None</code> <p>The raw fixation data.</p> required <code>raw_ia_data</code> <code>DataFrame</code> <p>The raw IA data.</p> required <code>trial_groupby_columns</code> <code>list[str]</code> <p>The columns to group by for trials.</p> required <code>processed_data_path</code> <code>Path</code> <p>The path to save the trial level feature names.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: The computed trial-level features.</p> Source code in <code>src/data/utils.py</code> <pre><code>def compute_trial_level_features(\n    raw_fixation_data: pd.DataFrame | None,\n    raw_ia_data: pd.DataFrame,\n    trial_groupby_columns: list[str],\n    processed_data_path: Path,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Compute trial-level features in parallel.\n\n    Args:\n        raw_fixation_data (pd.DataFrame | None): The raw fixation data.\n        raw_ia_data (pd.DataFrame): The raw IA data.\n        trial_groupby_columns (list[str]): The columns to group by for trials.\n        processed_data_path (Path): The path to save the trial level feature names.\n\n    Returns:\n        pd.DataFrame: The computed trial-level features.\n    \"\"\"\n    groupby_mappings = [\n        (feature_name, list(raw_ia_data[feature_name].unique()))\n        for feature_name in [\n            'is_content_word',\n            'ptb_pos',\n            'entity_type',\n            'universal_pos',\n        ]\n    ]\n    logger.info(\n        f'Computing trial level features for {raw_ia_data.shape[0]} trials with {groupby_mappings} groupby mappings'\n    )\n    ia_partial = partial(\n        compute_ia_trial_level_features,\n        processed_data_path=processed_data_path,\n    )\n    logger.info('This might take a couple of minutes, please be patient...')\n    logger.info(\n        f' Number of trial groups in ia: {len(raw_ia_data.groupby(trial_groupby_columns).groups)}'\n    )\n    ia_trial_features = raw_ia_data.groupby(trial_groupby_columns).apply(ia_partial)  # type: ignore\n    ia_trial_features = pd.DataFrame(\n        list(ia_trial_features), index=ia_trial_features.index\n    ).fillna(0)\n\n    if raw_fixation_data is not None:\n        logger.info(\n            f'Computing fixation trial level features for {raw_fixation_data.shape[0]} trials with {groupby_mappings} groupby mappings'\n        )\n        logger.info('This might take a couple of minutes, please be patient...')\n        fixation_partial = partial(\n            compute_fixation_trial_level_features,\n            groupby_mappings=groupby_mappings,\n            processed_data_path=processed_data_path,\n        )\n        logger.info(\n            f'Number of trial groups in fix: {len(raw_fixation_data.groupby(trial_groupby_columns).groups)}'\n        )\n        logger.info('This might take a couple of minutes, please be patient...')\n        fixation_trial_features = raw_fixation_data.groupby(\n            trial_groupby_columns\n        ).apply(fixation_partial)  # type: ignore\n        fixation_trial_features = pd.DataFrame(\n            list(fixation_trial_features), index=fixation_trial_features.index\n        ).fillna(0)\n        trial_level_features = pd.concat(\n            [fixation_trial_features, ia_trial_features],\n            axis=1,\n        )\n    else:\n        trial_level_features = ia_trial_features\n\n    return trial_level_features\n</code></pre>"},{"location":"reference/data/utils/#data.utils.get_feature_from_list","title":"<code>get_feature_from_list(values, aggregation_function)</code>","text":"<p>creates a feature for a list of values (e.g. mean or standard deviation of values in list) Args:     values (list[int | float | np.int32 | np.float64]): list of values     aggregation_function (str): name of function to be applied to list Returns:     np.float64  | np.nan: aggregated value or np.nan if not possible</p> Source code in <code>src/data/utils.py</code> <pre><code>def get_feature_from_list(\n    values: list[float | int | float | np.int32 | np.float64] | pd.Series,\n    aggregation_function: str,\n):\n    \"\"\"\n    creates a feature for a list of values (e.g. mean or standard deviation of values in list)\n    Args:\n        values (list[int | float | np.int32 | np.float64]): list of values\n        aggregation_function (str): name of function to be applied to list\n    Returns:\n        np.float64  | np.nan: aggregated value or np.nan if not possible\n    \"\"\"\n    warnings.filterwarnings('ignore', category=RuntimeWarning)\n    if np.sum(np.isnan(values)) == len(values):\n        return np.nan\n    if aggregation_function == 'mean':\n        return np.nanmean(values)\n    elif aggregation_function == 'std':\n        return np.nanstd(values)\n    elif aggregation_function == 'median':\n        return np.nanmedian(values)\n    elif aggregation_function == 'skew':\n        not_nan_values = np.array(values)[~np.isnan(values)]\n        return skew(not_nan_values)\n    elif aggregation_function == 'kurtosis':\n        not_nan_values = np.array(values)[~np.isnan(values)]\n        return kurtosis(not_nan_values)\n    elif aggregation_function == 'max':\n        return np.nanmax(values)\n    elif aggregation_function == 'min':\n        return np.nanmin(values)\n    else:\n        return np.nan\n</code></pre>"},{"location":"reference/data/utils/#data.utils.get_gaze_entropy_features","title":"<code>get_gaze_entropy_features(x_means, y_means, x_dim=2560, y_dim=1440, patch_size=138)</code>","text":"<p>Compute gaze entropy features.</p> <p>Parameters:</p> Name Type Description Default <code>x_means</code> <code>ndarray</code> <p>The x-coordinates of fixations.</p> required <code>y_means</code> <code>ndarray</code> <p>The y-coordinates of fixations.</p> required <code>x_dim</code> <code>int</code> <p>The screen horizontal pixels. Defaults to 2560.</p> <code>2560</code> <code>y_dim</code> <code>int</code> <p>The screen vertical pixels. Defaults to 1440.</p> <code>1440</code> <code>patch_size</code> <code>int</code> <p>The size of patches to use. Defaults to 138.</p> <code>138</code> <p>Returns:</p> Type Description <code>dict[str, int | float | float64]</code> <p>dict[str, int | float | np.float64]: The gaze entropy features.</p> Source code in <code>src/data/utils.py</code> <pre><code>def get_gaze_entropy_features(\n    x_means: np.ndarray,\n    y_means: np.ndarray,\n    x_dim: int = 2560,\n    y_dim: int = 1440,\n    patch_size: int = 138,\n) -&gt; dict[str, int | float | np.float64]:\n    \"\"\"\n    Compute gaze entropy features.\n\n    Args:\n        x_means (np.ndarray): The x-coordinates of fixations.\n        y_means (np.ndarray): The y-coordinates of fixations.\n        x_dim (int, optional): The screen horizontal pixels. Defaults to 2560.\n        y_dim (int, optional): The screen vertical pixels. Defaults to 1440.\n        patch_size (int, optional): The size of patches to use. Defaults to 138.\n\n    Returns:\n        dict[str, int | float | np.float64]: The gaze entropy features.\n    \"\"\"\n\n    # Gaze entropy measures detect alcohol-induced driver impairment - ScienceDirect\n    # https://www.sciencedirect.com/science/article/abs/pii/S0376871619302789\n    # computes the gaze entropy features\n    # params:\n    #    x_means: x-coordinates of fixations\n    #    y_means: y coordinates of fixations\n    #    x_dim: screen horizontal pixels\n    #    y_dim: screen vertical pixels\n    #    patch_size: size of patches to use\n    # Based on https://github.com/aeye-lab/etra-reading-comprehension\n    def calc_patch(patch_size: int, mean: np.float64 | np.int64) -&gt; float | int:\n        return int(np.floor(mean / patch_size))\n\n    def entropy(value: float) -&gt; float:\n        return value * (np.log(value) / np.log(2))\n\n    # dictionary of visited patches\n    patch_dict = {}\n    # dictionary for patch transitions\n    trans_dict = defaultdict(list)\n    pre = None\n    for i in range(len(x_means)):\n        x_mean = x_means[i]\n        y_mean = y_means[i]\n        patch_x = calc_patch(patch_size, x_mean)\n        patch_y = calc_patch(patch_size, y_mean)\n        cur_point = f'{str(patch_x)}_{str(patch_y)}'\n        if cur_point not in patch_dict:\n            patch_dict[cur_point] = 0\n        patch_dict[cur_point] += 1\n        if pre is not None:\n            trans_dict[pre].append(cur_point)\n        pre = cur_point\n\n    # stationary gaze entropy\n    # SGE\n    sge = 0.0\n    x_max = int(x_dim / patch_size)\n    y_max = int(y_dim / patch_size)\n    fix_number = len(x_means)\n    for i in range(x_max):\n        for j in range(y_max):\n            cur_point = f'{str(i)}_{str(j)}'\n            if cur_point in patch_dict:\n                cur_prop = patch_dict[cur_point] / fix_number\n                sge += entropy(cur_prop)\n    sge = sge * -1\n\n    # gaze transition entropy\n    # GTE\n    gte = 0.0\n    for patch in trans_dict:\n        cur_patch_prop = patch_dict[patch] / fix_number\n        cur_destination_list = trans_dict[patch]\n        (values, counts) = np.unique(cur_destination_list, return_counts=True)\n        inner_sum = 0.0\n        for i in range(len(values)):\n            cur_count = counts[i]\n            cur_prob = cur_count / np.sum(counts)\n            cur_entropy = entropy(cur_prob)\n            inner_sum += cur_entropy\n        gte += cur_patch_prop * inner_sum\n    gte *= -1\n    return {'fixation_feature_SGE': sge, 'fixation_feature_GTE': gte}\n</code></pre>"},{"location":"reference/data/utils/#data.utils.load_fold_data","title":"<code>load_fold_data(fold_index, base_path, folds_folder_name, data_type, regime_name, set_name)</code>","text":"<p>Load data for a specific fold, data type, regime, and set.</p> <p>This method reads a Feather file containing the data for the specified fold index, data type, regime name, and set name.</p> <p>Parameters:</p> Name Type Description Default <code>fold_index</code> <code>int</code> <p>The index of the fold to load data for.</p> required <code>base_path</code> <code>Path</code> <p>The base path where the data is stored.</p> required <code>folds_folder_name</code> <code>str</code> <p>The name of the folder containing the folds.</p> required <code>data_type</code> <code>DataType</code> <p>The type of data to load (e.g., train, test, etc.).</p> required <code>regime_name</code> <code>SetNames</code> <p>The name of the regime (e.g., validation, training, etc.).</p> required <code>set_name</code> <code>SetNames</code> <p>The name of the set (e.g., train, test, etc.).</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A DataFrame containing the loaded data.</p> Note <p>The file path is currently hardcoded to 'data/OneStop/folds'. This should be replaced with a general path when a connection to the server is available.</p> Source code in <code>src/data/utils.py</code> <pre><code>def load_fold_data(\n    fold_index: int,\n    base_path: Path,\n    folds_folder_name: str,\n    data_type: DataType,\n    regime_name: SetNames,\n    set_name: SetNames,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Load data for a specific fold, data type, regime, and set.\n\n    This method reads a Feather file containing the data for the specified\n    fold index, data type, regime name, and set name.\n\n    Args:\n        fold_index (int): The index of the fold to load data for.\n        base_path (Path): The base path where the data is stored.\n        folds_folder_name (str): The name of the folder containing the folds.\n        data_type (DataType): The type of data to load (e.g., train, test, etc.).\n        regime_name (SetNames): The name of the regime (e.g., validation, training, etc.).\n        set_name (SetNames): The name of the set (e.g., train, test, etc.).\n\n    Returns:\n        pd.DataFrame: A DataFrame containing the loaded data.\n\n    Note:\n        The file path is currently hardcoded to 'data/OneStop/folds'. This should\n        be replaced with a general path when a connection to the server is available.\n    \"\"\"\n    df = pd.read_feather(\n        base_path\n        / folds_folder_name\n        / f'fold_{fold_index}'\n        / f'{data_type}_{set_name}_{regime_name}.feather'\n    )\n    for should_be_bool in ['total_skip', 'start_of_line', 'end_of_line']:\n        if should_be_bool in df.columns:\n            df[should_be_bool] = df[should_be_bool].astype(bool)\n    return df\n</code></pre>"},{"location":"reference/data/utils/#data.utils.save_feature_names_if_do_not_exist","title":"<code>save_feature_names_if_do_not_exist(features_dict, csv_path, mode)</code>","text":"<p>Save feature names to a CSV file if they do not already exist.</p> Source code in <code>src/data/utils.py</code> <pre><code>def save_feature_names_if_do_not_exist(\n    features_dict, csv_path: Path, mode: DataType\n) -&gt; None:\n    \"\"\"\n    Save feature names to a CSV file if they do not already exist.\n    \"\"\"\n    global_field_name = f'{mode}_TRIAL_LEVEL_FEATURE_KEYS_SAVED'\n    if global_field_name not in globals():\n        feature_rows = []\n        for feature_type, feature_dict in features_dict.items():\n            for feature_name in feature_dict.keys():\n                feature_rows.append(\n                    {'feature_name': feature_name, 'feature_type': feature_type}\n                )\n        csv_path.parent.mkdir(parents=True, exist_ok=True)\n        pd.DataFrame(feature_rows).to_csv(csv_path, index=False)\n        logger.info(f'Saved feature names to {csv_path}')\n        globals()[global_field_name] = True\n</code></pre>"},{"location":"reference/data/datamodules/base_datamodule/","title":"base_datamodule","text":"<p>Data module for creating the data.</p>"},{"location":"reference/data/datamodules/base_datamodule/#data.datamodules.base_datamodule.ETDataModule","title":"<code>ETDataModule</code>","text":"<p>               Bases: <code>LightningDataModule</code></p> <p>A PyTorch Lightning data module for the eye tracking data.</p> <p>Attributes:</p> Name Type Description <code>cfg</code> <code>Args</code> <p>The configuration object.</p> <code>text_dataset_path</code> <code>Path</code> <p>The path to the text dataset.</p> <code>train_dataset</code> <code>ETDataset</code> <p>The training dataset.</p> <code>val_datasets</code> <code>list[ETDataset]</code> <p>The validation datasets.</p> <code>test_datasets</code> <code>list[ETDataset]</code> <p>The test datasets.</p> Source code in <code>src/data/datamodules/base_datamodule.py</code> <pre><code>class ETDataModule(pl.LightningDataModule):\n    \"\"\"\n    A PyTorch Lightning data module for the eye tracking data.\n\n    Attributes:\n        cfg (Args): The configuration object.\n        text_dataset_path (Path): The path to the text dataset.\n        train_dataset (ETDataset): The training dataset.\n        val_datasets (list[ETDataset]): The validation datasets.\n        test_datasets (list[ETDataset]): The test datasets.\n    \"\"\"\n\n    def __init__(self, cfg: Args):\n        \"\"\"\n        Initialize the ETDataModule instance.\n\n        Args:\n            cfg (Args): The configuration object.\n        \"\"\"\n        super().__init__()\n        self.cfg = cfg\n\n        self.train_dataset: ETDataset\n        self.val_datasets: list[ETDataset]\n        self.test_datasets: list[ETDataset]\n\n        self.text_dataset_path = (\n            FEATURES_CACHE_FOLDER\n            / f'{cfg.data.dataset_name}_{cfg.data.task}_{cfg.model.model_name}'\n            / 'TextDataSet.pkl'\n        )\n\n        self.save_hyperparameters(asdict(self.cfg))\n\n    def setup(self, stage: str | None = None) -&gt; None:\n        \"\"\"\n        Set up the data module for training, validation, or testing.\n\n        Args:\n            stage (str | None): The stage of the setup. Can be \"fit\", \"test\", or \"predict\".\n        \"\"\"\n\n        ia_scaler = self.cfg.model.normalization_type.value()\n        fixation_scaler = self.cfg.model.normalization_type.value()\n        trial_features_scaler = self.cfg.model.normalization_type.value()\n\n        self.train_dataset = self.create_etdataset(\n            ia_scaler=ia_scaler,\n            fixation_scaler=fixation_scaler,\n            trial_features_scaler=trial_features_scaler,\n            set_name=SetNames.TRAIN,\n            regime_name=SetNames.TRAIN,\n        )\n\n        if stage in {'fit', 'predict'}:\n            self.val_datasets = [\n                self.create_etdataset(\n                    ia_scaler=self.train_dataset.ia_scaler,\n                    fixation_scaler=self.train_dataset.fixation_scaler,\n                    trial_features_scaler=self.train_dataset.trial_features_scaler,\n                    regime_name=regime_name,\n                    set_name=SetNames.VAL,\n                )\n                for regime_name in REGIMES\n            ]\n\n        if stage in {'test', 'predict'}:\n            self.test_datasets = [\n                self.create_etdataset(\n                    ia_scaler=self.train_dataset.ia_scaler,\n                    fixation_scaler=self.train_dataset.fixation_scaler,\n                    trial_features_scaler=self.train_dataset.trial_features_scaler,\n                    regime_name=regime_name,\n                    set_name=SetNames.TEST,\n                )\n                for regime_name in REGIMES\n            ]\n\n    @abstractmethod\n    def create_etdataset(\n        self,\n        ia_scaler: Scaler | None,\n        fixation_scaler: Scaler | None,\n        trial_features_scaler: Scaler | None,\n        set_name: SetNames,\n        regime_name: SetNames,\n    ) -&gt; ETDataset:\n        \"\"\"\n        Abstract method to create an ETDataset instance.\n\n        Args:\n            ia_scaler (MinMaxScaler | RobustScaler | StandardScaler): The IA scaler.\n            fixation_scaler (MinMaxScaler | RobustScaler | StandardScaler | None): Fixation scaler.\n            trial_features_scaler (MinMaxScaler | RobustScaler | StandardScaler | None):\n                The trial features scaler.\n            regime_name (SetNames): The name of the regime (e.g., unseen_subject_seen_item).\n            set_name (SetNames): The name of the set (e.g., train, test, val).\n\n        Returns:\n            ETDataset: The created ETDataset instance.\n        \"\"\"\n        raise NotImplementedError('Subclasses must implement this method.')\n\n    def create_dataloader(\n        self,\n        dataset,\n        shuffle,\n        sample_m_per_class: bool = False,\n        drop_last: bool = False,\n    ) -&gt; DataLoader:\n        \"\"\"\n        Create a DataLoader for the given dataset.\n\n        Args:\n            dataset (ETDataset): The dataset to create the DataLoader for.\n            shuffle (bool): Whether to shuffle the data.\n\n        Returns:\n            DataLoader: The created DataLoader.\n        \"\"\"\n        if sample_m_per_class:\n            sampler = samplers.MPerClassSampler(\n                labels=self.train_dataset.labels,\n                m=1,\n                length_before_new_iter=self.cfg.trainer.samples_per_epoch,\n            )\n            shuffle = None\n            logger.info(\n                f'Using MPerClassSampler with m=1 and {self.cfg.trainer.samples_per_epoch} samples per epoch. Shuffle is set to None.'\n            )\n        else:\n            sampler = None\n\n        return DataLoader(\n            dataset,\n            batch_size=self.cfg.model.batch_size,\n            num_workers=self.cfg.trainer.num_workers,\n            shuffle=shuffle,\n            pin_memory=True,\n            drop_last=drop_last,\n            sampler=sampler,\n        )\n\n    def train_dataloader(self) -&gt; DataLoader:\n        \"\"\"\n        Create the DataLoader for the training dataset.\n\n        Returns:\n            DataLoader: The DataLoader for the training dataset.\n        \"\"\"\n        return self.create_dataloader(\n            self.train_dataset,\n            shuffle=True,\n            drop_last=False,\n            sample_m_per_class=self.cfg.trainer.sample_m_per_class,\n        )\n\n    def val_dataloader(self) -&gt; list[DataLoader]:\n        \"\"\"\n        Create the DataLoader for the validation datasets.\n\n        Returns:\n            list[DataLoader]: A list of DataLoaders for the validation datasets.\n        \"\"\"\n        return [\n            self.create_dataloader(dataset, shuffle=False, drop_last=False)\n            for dataset in self.val_datasets\n        ]\n\n    def test_dataloader(self) -&gt; list[DataLoader]:\n        \"\"\"\n        Create the DataLoader for the test datasets.\n\n        Returns:\n            list[DataLoader]: A list of DataLoaders for the test datasets.\n        \"\"\"\n        return [\n            self.create_dataloader(dataset, shuffle=False, drop_last=False)\n            for dataset in self.test_datasets\n        ]\n\n    def predict_dataloader(self) -&gt; list[DataLoader]:\n        \"\"\"\n        Create the DataLoader for the prediction datasets.\n\n        Returns:\n            list[DataLoader]: A list of DataLoaders for the prediction datasets.\n        \"\"\"\n        return self.val_dataloader() + self.test_dataloader()\n\n    def prepare_data(self) -&gt; None:\n        \"\"\"\n        Prepare the data for the module.\n\n        \"\"\"\n\n        self.text_dataset_create_if_needed()\n\n    def text_dataset_create_if_needed(self) -&gt; None:\n        \"\"\"\n        If the text dataset does not exist or overwrite_data is True, create and save the text dataset.\n        \"\"\"\n\n        if self.cfg.model.use_eyes_only:\n            logger.info('Using eyes only, no text dataset will be created.')\n            return\n\n        if self.cfg.trainer.overwrite_data or not self.text_dataset_path.exists():\n            self.text_dataset_path.parent.mkdir(parents=True, exist_ok=True)\n            logger.info(f'Creating and saving textDataSet to {self.text_dataset_path}')\n            # create and save to pkl\n            text_data = TextDataSet(cfg=self.cfg)\n            with open(self.text_dataset_path, 'wb') as f:\n                pickle.dump(text_data, f)\n        else:\n            logger.info(\n                f'TextDataSet already exists at: {self.text_dataset_path} and overwrite is False'\n            )\n\n    def load_text_dataset(self) -&gt; TextDataSet:\n        \"\"\"\n        Load the text dataset from a pickle file.\n\n        Returns:\n            TextDataSet: The loaded text dataset.\n        \"\"\"\n        logger.info(f'Loading textDataSet from {self.text_dataset_path}')\n        with open(self.text_dataset_path, 'rb') as f:\n            text_data = pickle.load(f)\n        return text_data\n</code></pre>"},{"location":"reference/data/datamodules/base_datamodule/#data.datamodules.base_datamodule.ETDataModule.__init__","title":"<code>__init__(cfg)</code>","text":"<p>Initialize the ETDataModule instance.</p> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <code>Args</code> <p>The configuration object.</p> required Source code in <code>src/data/datamodules/base_datamodule.py</code> <pre><code>def __init__(self, cfg: Args):\n    \"\"\"\n    Initialize the ETDataModule instance.\n\n    Args:\n        cfg (Args): The configuration object.\n    \"\"\"\n    super().__init__()\n    self.cfg = cfg\n\n    self.train_dataset: ETDataset\n    self.val_datasets: list[ETDataset]\n    self.test_datasets: list[ETDataset]\n\n    self.text_dataset_path = (\n        FEATURES_CACHE_FOLDER\n        / f'{cfg.data.dataset_name}_{cfg.data.task}_{cfg.model.model_name}'\n        / 'TextDataSet.pkl'\n    )\n\n    self.save_hyperparameters(asdict(self.cfg))\n</code></pre>"},{"location":"reference/data/datamodules/base_datamodule/#data.datamodules.base_datamodule.ETDataModule.create_dataloader","title":"<code>create_dataloader(dataset, shuffle, sample_m_per_class=False, drop_last=False)</code>","text":"<p>Create a DataLoader for the given dataset.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>ETDataset</code> <p>The dataset to create the DataLoader for.</p> required <code>shuffle</code> <code>bool</code> <p>Whether to shuffle the data.</p> required <p>Returns:</p> Name Type Description <code>DataLoader</code> <code>DataLoader</code> <p>The created DataLoader.</p> Source code in <code>src/data/datamodules/base_datamodule.py</code> <pre><code>def create_dataloader(\n    self,\n    dataset,\n    shuffle,\n    sample_m_per_class: bool = False,\n    drop_last: bool = False,\n) -&gt; DataLoader:\n    \"\"\"\n    Create a DataLoader for the given dataset.\n\n    Args:\n        dataset (ETDataset): The dataset to create the DataLoader for.\n        shuffle (bool): Whether to shuffle the data.\n\n    Returns:\n        DataLoader: The created DataLoader.\n    \"\"\"\n    if sample_m_per_class:\n        sampler = samplers.MPerClassSampler(\n            labels=self.train_dataset.labels,\n            m=1,\n            length_before_new_iter=self.cfg.trainer.samples_per_epoch,\n        )\n        shuffle = None\n        logger.info(\n            f'Using MPerClassSampler with m=1 and {self.cfg.trainer.samples_per_epoch} samples per epoch. Shuffle is set to None.'\n        )\n    else:\n        sampler = None\n\n    return DataLoader(\n        dataset,\n        batch_size=self.cfg.model.batch_size,\n        num_workers=self.cfg.trainer.num_workers,\n        shuffle=shuffle,\n        pin_memory=True,\n        drop_last=drop_last,\n        sampler=sampler,\n    )\n</code></pre>"},{"location":"reference/data/datamodules/base_datamodule/#data.datamodules.base_datamodule.ETDataModule.create_etdataset","title":"<code>create_etdataset(ia_scaler, fixation_scaler, trial_features_scaler, set_name, regime_name)</code>  <code>abstractmethod</code>","text":"<p>Abstract method to create an ETDataset instance.</p> <p>Parameters:</p> Name Type Description Default <code>ia_scaler</code> <code>MinMaxScaler | RobustScaler | StandardScaler</code> <p>The IA scaler.</p> required <code>fixation_scaler</code> <code>MinMaxScaler | RobustScaler | StandardScaler | None</code> <p>Fixation scaler.</p> required <code>trial_features_scaler</code> <code>MinMaxScaler | RobustScaler | StandardScaler | None</code> <p>The trial features scaler.</p> required <code>regime_name</code> <code>SetNames</code> <p>The name of the regime (e.g., unseen_subject_seen_item).</p> required <code>set_name</code> <code>SetNames</code> <p>The name of the set (e.g., train, test, val).</p> required <p>Returns:</p> Name Type Description <code>ETDataset</code> <code>ETDataset</code> <p>The created ETDataset instance.</p> Source code in <code>src/data/datamodules/base_datamodule.py</code> <pre><code>@abstractmethod\ndef create_etdataset(\n    self,\n    ia_scaler: Scaler | None,\n    fixation_scaler: Scaler | None,\n    trial_features_scaler: Scaler | None,\n    set_name: SetNames,\n    regime_name: SetNames,\n) -&gt; ETDataset:\n    \"\"\"\n    Abstract method to create an ETDataset instance.\n\n    Args:\n        ia_scaler (MinMaxScaler | RobustScaler | StandardScaler): The IA scaler.\n        fixation_scaler (MinMaxScaler | RobustScaler | StandardScaler | None): Fixation scaler.\n        trial_features_scaler (MinMaxScaler | RobustScaler | StandardScaler | None):\n            The trial features scaler.\n        regime_name (SetNames): The name of the regime (e.g., unseen_subject_seen_item).\n        set_name (SetNames): The name of the set (e.g., train, test, val).\n\n    Returns:\n        ETDataset: The created ETDataset instance.\n    \"\"\"\n    raise NotImplementedError('Subclasses must implement this method.')\n</code></pre>"},{"location":"reference/data/datamodules/base_datamodule/#data.datamodules.base_datamodule.ETDataModule.load_text_dataset","title":"<code>load_text_dataset()</code>","text":"<p>Load the text dataset from a pickle file.</p> <p>Returns:</p> Name Type Description <code>TextDataSet</code> <code>TextDataSet</code> <p>The loaded text dataset.</p> Source code in <code>src/data/datamodules/base_datamodule.py</code> <pre><code>def load_text_dataset(self) -&gt; TextDataSet:\n    \"\"\"\n    Load the text dataset from a pickle file.\n\n    Returns:\n        TextDataSet: The loaded text dataset.\n    \"\"\"\n    logger.info(f'Loading textDataSet from {self.text_dataset_path}')\n    with open(self.text_dataset_path, 'rb') as f:\n        text_data = pickle.load(f)\n    return text_data\n</code></pre>"},{"location":"reference/data/datamodules/base_datamodule/#data.datamodules.base_datamodule.ETDataModule.predict_dataloader","title":"<code>predict_dataloader()</code>","text":"<p>Create the DataLoader for the prediction datasets.</p> <p>Returns:</p> Type Description <code>list[DataLoader]</code> <p>list[DataLoader]: A list of DataLoaders for the prediction datasets.</p> Source code in <code>src/data/datamodules/base_datamodule.py</code> <pre><code>def predict_dataloader(self) -&gt; list[DataLoader]:\n    \"\"\"\n    Create the DataLoader for the prediction datasets.\n\n    Returns:\n        list[DataLoader]: A list of DataLoaders for the prediction datasets.\n    \"\"\"\n    return self.val_dataloader() + self.test_dataloader()\n</code></pre>"},{"location":"reference/data/datamodules/base_datamodule/#data.datamodules.base_datamodule.ETDataModule.prepare_data","title":"<code>prepare_data()</code>","text":"<p>Prepare the data for the module.</p> Source code in <code>src/data/datamodules/base_datamodule.py</code> <pre><code>def prepare_data(self) -&gt; None:\n    \"\"\"\n    Prepare the data for the module.\n\n    \"\"\"\n\n    self.text_dataset_create_if_needed()\n</code></pre>"},{"location":"reference/data/datamodules/base_datamodule/#data.datamodules.base_datamodule.ETDataModule.setup","title":"<code>setup(stage=None)</code>","text":"<p>Set up the data module for training, validation, or testing.</p> <p>Parameters:</p> Name Type Description Default <code>stage</code> <code>str | None</code> <p>The stage of the setup. Can be \"fit\", \"test\", or \"predict\".</p> <code>None</code> Source code in <code>src/data/datamodules/base_datamodule.py</code> <pre><code>def setup(self, stage: str | None = None) -&gt; None:\n    \"\"\"\n    Set up the data module for training, validation, or testing.\n\n    Args:\n        stage (str | None): The stage of the setup. Can be \"fit\", \"test\", or \"predict\".\n    \"\"\"\n\n    ia_scaler = self.cfg.model.normalization_type.value()\n    fixation_scaler = self.cfg.model.normalization_type.value()\n    trial_features_scaler = self.cfg.model.normalization_type.value()\n\n    self.train_dataset = self.create_etdataset(\n        ia_scaler=ia_scaler,\n        fixation_scaler=fixation_scaler,\n        trial_features_scaler=trial_features_scaler,\n        set_name=SetNames.TRAIN,\n        regime_name=SetNames.TRAIN,\n    )\n\n    if stage in {'fit', 'predict'}:\n        self.val_datasets = [\n            self.create_etdataset(\n                ia_scaler=self.train_dataset.ia_scaler,\n                fixation_scaler=self.train_dataset.fixation_scaler,\n                trial_features_scaler=self.train_dataset.trial_features_scaler,\n                regime_name=regime_name,\n                set_name=SetNames.VAL,\n            )\n            for regime_name in REGIMES\n        ]\n\n    if stage in {'test', 'predict'}:\n        self.test_datasets = [\n            self.create_etdataset(\n                ia_scaler=self.train_dataset.ia_scaler,\n                fixation_scaler=self.train_dataset.fixation_scaler,\n                trial_features_scaler=self.train_dataset.trial_features_scaler,\n                regime_name=regime_name,\n                set_name=SetNames.TEST,\n            )\n            for regime_name in REGIMES\n        ]\n</code></pre>"},{"location":"reference/data/datamodules/base_datamodule/#data.datamodules.base_datamodule.ETDataModule.test_dataloader","title":"<code>test_dataloader()</code>","text":"<p>Create the DataLoader for the test datasets.</p> <p>Returns:</p> Type Description <code>list[DataLoader]</code> <p>list[DataLoader]: A list of DataLoaders for the test datasets.</p> Source code in <code>src/data/datamodules/base_datamodule.py</code> <pre><code>def test_dataloader(self) -&gt; list[DataLoader]:\n    \"\"\"\n    Create the DataLoader for the test datasets.\n\n    Returns:\n        list[DataLoader]: A list of DataLoaders for the test datasets.\n    \"\"\"\n    return [\n        self.create_dataloader(dataset, shuffle=False, drop_last=False)\n        for dataset in self.test_datasets\n    ]\n</code></pre>"},{"location":"reference/data/datamodules/base_datamodule/#data.datamodules.base_datamodule.ETDataModule.text_dataset_create_if_needed","title":"<code>text_dataset_create_if_needed()</code>","text":"<p>If the text dataset does not exist or overwrite_data is True, create and save the text dataset.</p> Source code in <code>src/data/datamodules/base_datamodule.py</code> <pre><code>def text_dataset_create_if_needed(self) -&gt; None:\n    \"\"\"\n    If the text dataset does not exist or overwrite_data is True, create and save the text dataset.\n    \"\"\"\n\n    if self.cfg.model.use_eyes_only:\n        logger.info('Using eyes only, no text dataset will be created.')\n        return\n\n    if self.cfg.trainer.overwrite_data or not self.text_dataset_path.exists():\n        self.text_dataset_path.parent.mkdir(parents=True, exist_ok=True)\n        logger.info(f'Creating and saving textDataSet to {self.text_dataset_path}')\n        # create and save to pkl\n        text_data = TextDataSet(cfg=self.cfg)\n        with open(self.text_dataset_path, 'wb') as f:\n            pickle.dump(text_data, f)\n    else:\n        logger.info(\n            f'TextDataSet already exists at: {self.text_dataset_path} and overwrite is False'\n        )\n</code></pre>"},{"location":"reference/data/datamodules/base_datamodule/#data.datamodules.base_datamodule.ETDataModule.train_dataloader","title":"<code>train_dataloader()</code>","text":"<p>Create the DataLoader for the training dataset.</p> <p>Returns:</p> Name Type Description <code>DataLoader</code> <code>DataLoader</code> <p>The DataLoader for the training dataset.</p> Source code in <code>src/data/datamodules/base_datamodule.py</code> <pre><code>def train_dataloader(self) -&gt; DataLoader:\n    \"\"\"\n    Create the DataLoader for the training dataset.\n\n    Returns:\n        DataLoader: The DataLoader for the training dataset.\n    \"\"\"\n    return self.create_dataloader(\n        self.train_dataset,\n        shuffle=True,\n        drop_last=False,\n        sample_m_per_class=self.cfg.trainer.sample_m_per_class,\n    )\n</code></pre>"},{"location":"reference/data/datamodules/base_datamodule/#data.datamodules.base_datamodule.ETDataModule.val_dataloader","title":"<code>val_dataloader()</code>","text":"<p>Create the DataLoader for the validation datasets.</p> <p>Returns:</p> Type Description <code>list[DataLoader]</code> <p>list[DataLoader]: A list of DataLoaders for the validation datasets.</p> Source code in <code>src/data/datamodules/base_datamodule.py</code> <pre><code>def val_dataloader(self) -&gt; list[DataLoader]:\n    \"\"\"\n    Create the DataLoader for the validation datasets.\n\n    Returns:\n        list[DataLoader]: A list of DataLoaders for the validation datasets.\n    \"\"\"\n    return [\n        self.create_dataloader(dataset, shuffle=False, drop_last=False)\n        for dataset in self.val_datasets\n    ]\n</code></pre>"},{"location":"reference/data/datamodules/base_datamodule/#data.datamodules.base_datamodule.ETDataModuleFast","title":"<code>ETDataModuleFast</code>","text":"<p>               Bases: <code>ETDataModule</code></p> <p>A subclass of ETDataModule that includes checks to prevent redundant data preparation and setup. Based on the solution provided in https://github.com/Lightning-AI/pytorch-lightning/issues/16005</p> <p>Attributes:</p> Name Type Description <code>prepare_data_done</code> <code>bool</code> <p>A flag indicating whether the prepare_data method has been called.</p> <code>setup_stages_done</code> <code>set</code> <p>A set storing the stages for which setup method has been called.</p> Source code in <code>src/data/datamodules/base_datamodule.py</code> <pre><code>class ETDataModuleFast(ETDataModule):\n    \"\"\"\n    A subclass of ETDataModule that includes checks to prevent redundant data preparation and setup.\n    Based on the solution provided in https://github.com/Lightning-AI/pytorch-lightning/issues/16005\n\n    Attributes:\n        prepare_data_done (bool): A flag indicating whether the prepare_data method has been called.\n        setup_stages_done (set): A set storing the stages for which setup method has been called.\n    \"\"\"\n\n    def __init__(self, *args: object, **kwargs: object) -&gt; None:\n        \"\"\"\n        Initialize the ETDataModuleFast instance.\n\n        Args:\n            *args: Variable length argument list to be passed to the ETDataModule constructor.\n            **kwargs: Arbitrary keyword arguments to be passed to the ETDataModule constructor.\n        \"\"\"\n        super().__init__(*args, **kwargs)\n        self.prepare_data_done = False\n        self.setup_stages_done = set()\n\n    def prepare_data(self) -&gt; None:\n        \"\"\"\n        Prepare data for the module. If this method has been called before, it does nothing.\n        \"\"\"\n        if not self.prepare_data_done:\n            super().prepare_data()\n            self.prepare_data_done = True\n\n    def setup(self, stage: str) -&gt; None:\n        \"\"\"\n        Set up the module for a specific stage.\n            If this method has been called before for the same stage, it does nothing.\n\n        Args:\n            stage (str): The stage for which to set up the module.\n        \"\"\"\n        if stage not in self.setup_stages_done:\n            super().setup(stage)\n            self.setup_stages_done.add(stage)\n</code></pre>"},{"location":"reference/data/datamodules/base_datamodule/#data.datamodules.base_datamodule.ETDataModuleFast.__init__","title":"<code>__init__(*args, **kwargs)</code>","text":"<p>Initialize the ETDataModuleFast instance.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <code>object</code> <p>Variable length argument list to be passed to the ETDataModule constructor.</p> <code>()</code> <code>**kwargs</code> <code>object</code> <p>Arbitrary keyword arguments to be passed to the ETDataModule constructor.</p> <code>{}</code> Source code in <code>src/data/datamodules/base_datamodule.py</code> <pre><code>def __init__(self, *args: object, **kwargs: object) -&gt; None:\n    \"\"\"\n    Initialize the ETDataModuleFast instance.\n\n    Args:\n        *args: Variable length argument list to be passed to the ETDataModule constructor.\n        **kwargs: Arbitrary keyword arguments to be passed to the ETDataModule constructor.\n    \"\"\"\n    super().__init__(*args, **kwargs)\n    self.prepare_data_done = False\n    self.setup_stages_done = set()\n</code></pre>"},{"location":"reference/data/datamodules/base_datamodule/#data.datamodules.base_datamodule.ETDataModuleFast.prepare_data","title":"<code>prepare_data()</code>","text":"<p>Prepare data for the module. If this method has been called before, it does nothing.</p> Source code in <code>src/data/datamodules/base_datamodule.py</code> <pre><code>def prepare_data(self) -&gt; None:\n    \"\"\"\n    Prepare data for the module. If this method has been called before, it does nothing.\n    \"\"\"\n    if not self.prepare_data_done:\n        super().prepare_data()\n        self.prepare_data_done = True\n</code></pre>"},{"location":"reference/data/datamodules/base_datamodule/#data.datamodules.base_datamodule.ETDataModuleFast.setup","title":"<code>setup(stage)</code>","text":"<p>Set up the module for a specific stage.     If this method has been called before for the same stage, it does nothing.</p> <p>Parameters:</p> Name Type Description Default <code>stage</code> <code>str</code> <p>The stage for which to set up the module.</p> required Source code in <code>src/data/datamodules/base_datamodule.py</code> <pre><code>def setup(self, stage: str) -&gt; None:\n    \"\"\"\n    Set up the module for a specific stage.\n        If this method has been called before for the same stage, it does nothing.\n\n    Args:\n        stage (str): The stage for which to set up the module.\n    \"\"\"\n    if stage not in self.setup_stages_done:\n        super().setup(stage)\n        self.setup_stages_done.add(stage)\n</code></pre>"},{"location":"reference/data/datamodules/copco/","title":"copco","text":""},{"location":"reference/data/datamodules/copco/#data.datamodules.copco.CopCoDataModule","title":"<code>CopCoDataModule</code>","text":"<p>               Bases: <code>ETDataModuleFast</code></p> <p>A PyTorch Lightning data module for the eye tracking data.</p> <p>Attributes:</p> Name Type Description <code>cfg</code> <code>Args</code> <p>The configuration object.</p> <code>text_dataset_path</code> <code>Path</code> <p>The path to the text dataset.</p> <code>train_dataset</code> <code>CopCoDataSet</code> <p>The training dataset.</p> <code>val_datasets</code> <code>list[CopCoDataSet]</code> <p>The validation datasets.</p> <code>test_datasets</code> <code>list[CopCoDataSet]</code> <p>The test datasets.</p> Source code in <code>src/data/datamodules/copco.py</code> <pre><code>@register_datamodule\nclass CopCoDataModule(ETDataModuleFast):\n    \"\"\"\n    A PyTorch Lightning data module for the eye tracking data.\n\n    Attributes:\n        cfg (Args): The configuration object.\n        text_dataset_path (Path): The path to the text dataset.\n        train_dataset (CopCoDataSet): The training dataset.\n        val_datasets (list[CopCoDataSet]): The validation datasets.\n        test_datasets (list[CopCoDataSet]): The test datasets.\n    \"\"\"\n\n    def create_etdataset(\n        self,\n        ia_scaler: MinMaxScaler | RobustScaler | StandardScaler | None,\n        fixation_scaler: MinMaxScaler | RobustScaler | StandardScaler | None,\n        trial_features_scaler: MinMaxScaler | RobustScaler | StandardScaler | None,\n        set_name: SetNames,\n        regime_name: SetNames,\n    ) -&gt; CopCoDataset:\n        \"\"\"\n        Create an CopCoDataSet instance for the given keys.\n\n        Args:\n            ia_scaler (MinMaxScaler | RobustScaler | StandardScaler): The IA scaler.\n            fixation_scaler (MinMaxScaler | RobustScaler | StandardScaler | None): Fixation scaler.\n            trial_features_scaler (MinMaxScaler | RobustScaler | StandardScaler | None):\n                The trial features scaler.\n            regime_name (SetNames): The name of the regime (e.g., unseen_subject_seen_item).\n            set_name (SetNames): The name of the set (e.g., train, test, val).\n\n        Returns:\n            ETDataset: The created ETDataset instance.\n        \"\"\"\n        text_data = None if self.cfg.model.use_eyes_only else self.load_text_dataset()\n\n        dataset = CopCoDataset(\n            cfg=self.cfg,\n            ia_scaler=ia_scaler,\n            fixation_scaler=fixation_scaler,\n            trial_features_scaler=trial_features_scaler,\n            regime_name=regime_name,\n            set_name=set_name,\n            text_data=text_data,\n        )\n\n        return dataset\n</code></pre>"},{"location":"reference/data/datamodules/copco/#data.datamodules.copco.CopCoDataModule.create_etdataset","title":"<code>create_etdataset(ia_scaler, fixation_scaler, trial_features_scaler, set_name, regime_name)</code>","text":"<p>Create an CopCoDataSet instance for the given keys.</p> <p>Parameters:</p> Name Type Description Default <code>ia_scaler</code> <code>MinMaxScaler | RobustScaler | StandardScaler</code> <p>The IA scaler.</p> required <code>fixation_scaler</code> <code>MinMaxScaler | RobustScaler | StandardScaler | None</code> <p>Fixation scaler.</p> required <code>trial_features_scaler</code> <code>MinMaxScaler | RobustScaler | StandardScaler | None</code> <p>The trial features scaler.</p> required <code>regime_name</code> <code>SetNames</code> <p>The name of the regime (e.g., unseen_subject_seen_item).</p> required <code>set_name</code> <code>SetNames</code> <p>The name of the set (e.g., train, test, val).</p> required <p>Returns:</p> Name Type Description <code>ETDataset</code> <code>CopCoDataset</code> <p>The created ETDataset instance.</p> Source code in <code>src/data/datamodules/copco.py</code> <pre><code>def create_etdataset(\n    self,\n    ia_scaler: MinMaxScaler | RobustScaler | StandardScaler | None,\n    fixation_scaler: MinMaxScaler | RobustScaler | StandardScaler | None,\n    trial_features_scaler: MinMaxScaler | RobustScaler | StandardScaler | None,\n    set_name: SetNames,\n    regime_name: SetNames,\n) -&gt; CopCoDataset:\n    \"\"\"\n    Create an CopCoDataSet instance for the given keys.\n\n    Args:\n        ia_scaler (MinMaxScaler | RobustScaler | StandardScaler): The IA scaler.\n        fixation_scaler (MinMaxScaler | RobustScaler | StandardScaler | None): Fixation scaler.\n        trial_features_scaler (MinMaxScaler | RobustScaler | StandardScaler | None):\n            The trial features scaler.\n        regime_name (SetNames): The name of the regime (e.g., unseen_subject_seen_item).\n        set_name (SetNames): The name of the set (e.g., train, test, val).\n\n    Returns:\n        ETDataset: The created ETDataset instance.\n    \"\"\"\n    text_data = None if self.cfg.model.use_eyes_only else self.load_text_dataset()\n\n    dataset = CopCoDataset(\n        cfg=self.cfg,\n        ia_scaler=ia_scaler,\n        fixation_scaler=fixation_scaler,\n        trial_features_scaler=trial_features_scaler,\n        regime_name=regime_name,\n        set_name=set_name,\n        text_data=text_data,\n    )\n\n    return dataset\n</code></pre>"},{"location":"reference/data/datamodules/iitbhgc/","title":"iitbhgc","text":""},{"location":"reference/data/datamodules/iitbhgc/#data.datamodules.iitbhgc.IITBHGCDataModule","title":"<code>IITBHGCDataModule</code>","text":"<p>               Bases: <code>ETDataModuleFast</code></p> <p>A PyTorch Lightning data module for the eye tracking data.</p> <p>Attributes:</p> Name Type Description <code>cfg</code> <code>Args</code> <p>The configuration object.</p> <code>text_dataset_path</code> <code>Path</code> <p>The path to the text dataset.</p> <code>train_dataset</code> <code>IITBHGCDataSet</code> <p>The training dataset.</p> <code>val_datasets</code> <code>list[IITBHGCDataSet]</code> <p>The validation datasets.</p> <code>test_datasets</code> <code>list[IITBHGCDataSet]</code> <p>The test datasets.</p> Source code in <code>src/data/datamodules/iitbhgc.py</code> <pre><code>@register_datamodule\nclass IITBHGCDataModule(ETDataModuleFast):\n    \"\"\"\n    A PyTorch Lightning data module for the eye tracking data.\n\n    Attributes:\n        cfg (Args): The configuration object.\n        text_dataset_path (Path): The path to the text dataset.\n        train_dataset (IITBHGCDataSet): The training dataset.\n        val_datasets (list[IITBHGCDataSet]): The validation datasets.\n        test_datasets (list[IITBHGCDataSet]): The test datasets.\n    \"\"\"\n\n    def create_etdataset(\n        self,\n        ia_scaler: MinMaxScaler | RobustScaler | StandardScaler | None,\n        fixation_scaler: MinMaxScaler | RobustScaler | StandardScaler | None,\n        trial_features_scaler: MinMaxScaler | RobustScaler | StandardScaler | None,\n        set_name: SetNames,\n        regime_name: SetNames,\n    ) -&gt; IITBHGCDataset:\n        \"\"\"\n        Create an IITBHGCDataSet instance for the given keys.\n\n        Args:\n            ia_scaler (MinMaxScaler | RobustScaler | StandardScaler): The IA scaler.\n            fixation_scaler (MinMaxScaler | RobustScaler | StandardScaler | None): Fixation scaler.\n            trial_features_scaler (MinMaxScaler | RobustScaler | StandardScaler | None):\n                The trial features scaler.\n            regime_name (SetNames): The name of the regime (e.g., unseen_subject_seen_item).\n            set_name (SetNames): The name of the set (e.g., train, test, val).\n\n        Returns:\n            ETDataset: The created ETDataset instance.\n        \"\"\"\n        text_data = None if self.cfg.model.use_eyes_only else self.load_text_dataset()\n\n        dataset = IITBHGCDataset(\n            cfg=self.cfg,\n            ia_scaler=ia_scaler,\n            fixation_scaler=fixation_scaler,\n            trial_features_scaler=trial_features_scaler,\n            regime_name=regime_name,\n            set_name=set_name,\n            text_data=text_data,\n        )\n\n        return dataset\n</code></pre>"},{"location":"reference/data/datamodules/iitbhgc/#data.datamodules.iitbhgc.IITBHGCDataModule.create_etdataset","title":"<code>create_etdataset(ia_scaler, fixation_scaler, trial_features_scaler, set_name, regime_name)</code>","text":"<p>Create an IITBHGCDataSet instance for the given keys.</p> <p>Parameters:</p> Name Type Description Default <code>ia_scaler</code> <code>MinMaxScaler | RobustScaler | StandardScaler</code> <p>The IA scaler.</p> required <code>fixation_scaler</code> <code>MinMaxScaler | RobustScaler | StandardScaler | None</code> <p>Fixation scaler.</p> required <code>trial_features_scaler</code> <code>MinMaxScaler | RobustScaler | StandardScaler | None</code> <p>The trial features scaler.</p> required <code>regime_name</code> <code>SetNames</code> <p>The name of the regime (e.g., unseen_subject_seen_item).</p> required <code>set_name</code> <code>SetNames</code> <p>The name of the set (e.g., train, test, val).</p> required <p>Returns:</p> Name Type Description <code>ETDataset</code> <code>IITBHGCDataset</code> <p>The created ETDataset instance.</p> Source code in <code>src/data/datamodules/iitbhgc.py</code> <pre><code>def create_etdataset(\n    self,\n    ia_scaler: MinMaxScaler | RobustScaler | StandardScaler | None,\n    fixation_scaler: MinMaxScaler | RobustScaler | StandardScaler | None,\n    trial_features_scaler: MinMaxScaler | RobustScaler | StandardScaler | None,\n    set_name: SetNames,\n    regime_name: SetNames,\n) -&gt; IITBHGCDataset:\n    \"\"\"\n    Create an IITBHGCDataSet instance for the given keys.\n\n    Args:\n        ia_scaler (MinMaxScaler | RobustScaler | StandardScaler): The IA scaler.\n        fixation_scaler (MinMaxScaler | RobustScaler | StandardScaler | None): Fixation scaler.\n        trial_features_scaler (MinMaxScaler | RobustScaler | StandardScaler | None):\n            The trial features scaler.\n        regime_name (SetNames): The name of the regime (e.g., unseen_subject_seen_item).\n        set_name (SetNames): The name of the set (e.g., train, test, val).\n\n    Returns:\n        ETDataset: The created ETDataset instance.\n    \"\"\"\n    text_data = None if self.cfg.model.use_eyes_only else self.load_text_dataset()\n\n    dataset = IITBHGCDataset(\n        cfg=self.cfg,\n        ia_scaler=ia_scaler,\n        fixation_scaler=fixation_scaler,\n        trial_features_scaler=trial_features_scaler,\n        regime_name=regime_name,\n        set_name=set_name,\n        text_data=text_data,\n    )\n\n    return dataset\n</code></pre>"},{"location":"reference/data/datamodules/mecol2/","title":"mecol2","text":""},{"location":"reference/data/datamodules/mecol2/#data.datamodules.mecol2.MECOL2DataModule","title":"<code>MECOL2DataModule</code>","text":"<p>               Bases: <code>ETDataModuleFast</code></p> <p>A PyTorch Lightning data module for the eye tracking data.</p> <p>Attributes:</p> Name Type Description <code>cfg</code> <code>Args</code> <p>The configuration object.</p> <code>text_dataset_path</code> <code>Path</code> <p>The path to the text dataset.</p> <code>train_dataset</code> <code>MECOL2DataSet</code> <p>The training dataset.</p> <code>val_datasets</code> <code>list[MECOL2DataSet]</code> <p>The validation datasets.</p> <code>test_datasets</code> <code>list[MECOL2DataSet]</code> <p>The test datasets.</p> Source code in <code>src/data/datamodules/mecol2.py</code> <pre><code>@register_datamodule\nclass MECOL2DataModule(ETDataModuleFast):\n    \"\"\"\n    A PyTorch Lightning data module for the eye tracking data.\n\n    Attributes:\n        cfg (Args): The configuration object.\n        text_dataset_path (Path): The path to the text dataset.\n        train_dataset (MECOL2DataSet): The training dataset.\n        val_datasets (list[MECOL2DataSet]): The validation datasets.\n        test_datasets (list[MECOL2DataSet]): The test datasets.\n    \"\"\"\n\n    def create_etdataset(\n        self,\n        ia_scaler: MinMaxScaler | RobustScaler | StandardScaler | None,\n        fixation_scaler: MinMaxScaler | RobustScaler | StandardScaler | None,\n        trial_features_scaler: MinMaxScaler | RobustScaler | StandardScaler | None,\n        set_name: SetNames,\n        regime_name: SetNames,\n    ) -&gt; MECOL2Dataset:\n        \"\"\"\n        Create an MECOL2DataSet instance for the given keys.\n\n        Args:\n            ia_scaler (MinMaxScaler | RobustScaler | StandardScaler): The IA scaler.\n            fixation_scaler (MinMaxScaler | RobustScaler | StandardScaler | None): Fixation scaler.\n            trial_features_scaler (MinMaxScaler | RobustScaler | StandardScaler | None):\n                The trial features scaler.\n            regime_name (SetNames): The name of the regime (e.g., unseen_subject_seen_item).\n            set_name (SetNames): The name of the set (e.g., train, test, val).\n\n        Returns:\n            ETDataset: The created ETDataset instance.\n        \"\"\"\n        text_data = None if self.cfg.model.use_eyes_only else self.load_text_dataset()\n\n        dataset = MECOL2Dataset(\n            cfg=self.cfg,\n            ia_scaler=ia_scaler,\n            fixation_scaler=fixation_scaler,\n            trial_features_scaler=trial_features_scaler,\n            regime_name=regime_name,\n            set_name=set_name,\n            text_data=text_data,\n        )\n\n        return dataset\n</code></pre>"},{"location":"reference/data/datamodules/mecol2/#data.datamodules.mecol2.MECOL2DataModule.create_etdataset","title":"<code>create_etdataset(ia_scaler, fixation_scaler, trial_features_scaler, set_name, regime_name)</code>","text":"<p>Create an MECOL2DataSet instance for the given keys.</p> <p>Parameters:</p> Name Type Description Default <code>ia_scaler</code> <code>MinMaxScaler | RobustScaler | StandardScaler</code> <p>The IA scaler.</p> required <code>fixation_scaler</code> <code>MinMaxScaler | RobustScaler | StandardScaler | None</code> <p>Fixation scaler.</p> required <code>trial_features_scaler</code> <code>MinMaxScaler | RobustScaler | StandardScaler | None</code> <p>The trial features scaler.</p> required <code>regime_name</code> <code>SetNames</code> <p>The name of the regime (e.g., unseen_subject_seen_item).</p> required <code>set_name</code> <code>SetNames</code> <p>The name of the set (e.g., train, test, val).</p> required <p>Returns:</p> Name Type Description <code>ETDataset</code> <code>MECOL2Dataset</code> <p>The created ETDataset instance.</p> Source code in <code>src/data/datamodules/mecol2.py</code> <pre><code>def create_etdataset(\n    self,\n    ia_scaler: MinMaxScaler | RobustScaler | StandardScaler | None,\n    fixation_scaler: MinMaxScaler | RobustScaler | StandardScaler | None,\n    trial_features_scaler: MinMaxScaler | RobustScaler | StandardScaler | None,\n    set_name: SetNames,\n    regime_name: SetNames,\n) -&gt; MECOL2Dataset:\n    \"\"\"\n    Create an MECOL2DataSet instance for the given keys.\n\n    Args:\n        ia_scaler (MinMaxScaler | RobustScaler | StandardScaler): The IA scaler.\n        fixation_scaler (MinMaxScaler | RobustScaler | StandardScaler | None): Fixation scaler.\n        trial_features_scaler (MinMaxScaler | RobustScaler | StandardScaler | None):\n            The trial features scaler.\n        regime_name (SetNames): The name of the regime (e.g., unseen_subject_seen_item).\n        set_name (SetNames): The name of the set (e.g., train, test, val).\n\n    Returns:\n        ETDataset: The created ETDataset instance.\n    \"\"\"\n    text_data = None if self.cfg.model.use_eyes_only else self.load_text_dataset()\n\n    dataset = MECOL2Dataset(\n        cfg=self.cfg,\n        ia_scaler=ia_scaler,\n        fixation_scaler=fixation_scaler,\n        trial_features_scaler=trial_features_scaler,\n        regime_name=regime_name,\n        set_name=set_name,\n        text_data=text_data,\n    )\n\n    return dataset\n</code></pre>"},{"location":"reference/data/datamodules/onestop/","title":"onestop","text":""},{"location":"reference/data/datamodules/onestop/#data.datamodules.onestop.OneStopDataModule","title":"<code>OneStopDataModule</code>","text":"<p>               Bases: <code>ETDataModuleFast</code></p> <p>A PyTorch Lightning data module for the eye tracking data.</p> <p>Attributes:</p> Name Type Description <code>cfg</code> <code>Args</code> <p>The configuration object.</p> <code>text_dataset_path</code> <code>Path</code> <p>The path to the text dataset.</p> <code>train_dataset</code> <code>OneStopDataSet</code> <p>The training dataset.</p> <code>val_datasets</code> <code>list[OneStopDataSet]</code> <p>The validation datasets.</p> <code>test_datasets</code> <code>list[OneStopDataSet]</code> <p>The test datasets.</p> Source code in <code>src/data/datamodules/onestop.py</code> <pre><code>@register_datamodule\nclass OneStopDataModule(ETDataModuleFast):\n    \"\"\"\n    A PyTorch Lightning data module for the eye tracking data.\n\n    Attributes:\n        cfg (Args): The configuration object.\n        text_dataset_path (Path): The path to the text dataset.\n        train_dataset (OneStopDataSet): The training dataset.\n        val_datasets (list[OneStopDataSet]): The validation datasets.\n        test_datasets (list[OneStopDataSet]): The test datasets.\n    \"\"\"\n\n    def create_etdataset(\n        self,\n        ia_scaler: MinMaxScaler | RobustScaler | StandardScaler | None,\n        fixation_scaler: MinMaxScaler | RobustScaler | StandardScaler | None,\n        trial_features_scaler: MinMaxScaler | RobustScaler | StandardScaler | None,\n        set_name: SetNames,\n        regime_name: SetNames,\n    ) -&gt; OneStopDataset:\n        \"\"\"\n        Create an OneStopDataSet instance for the given keys.\n\n        Args:\n            ia_scaler (MinMaxScaler | RobustScaler | StandardScaler): The IA scaler.\n            fixation_scaler (MinMaxScaler | RobustScaler | StandardScaler | None): Fixation scaler.\n            trial_features_scaler (MinMaxScaler | RobustScaler | StandardScaler | None):\n                The trial features scaler.\n            regime_name (SetNames): The name of the regime (e.g., unseen_subject_seen_item).\n            set_name (SetNames): The name of the set (e.g., train, test, val).\n\n        Returns:\n            ETDataset: The created ETDataset instance.\n        \"\"\"\n        text_data = None if self.cfg.model.use_eyes_only else self.load_text_dataset()\n\n        dataset = OneStopDataset(\n            cfg=self.cfg,\n            ia_scaler=ia_scaler,\n            fixation_scaler=fixation_scaler,\n            trial_features_scaler=trial_features_scaler,\n            regime_name=regime_name,\n            set_name=set_name,\n            text_data=text_data,\n        )\n\n        return dataset\n</code></pre>"},{"location":"reference/data/datamodules/onestop/#data.datamodules.onestop.OneStopDataModule.create_etdataset","title":"<code>create_etdataset(ia_scaler, fixation_scaler, trial_features_scaler, set_name, regime_name)</code>","text":"<p>Create an OneStopDataSet instance for the given keys.</p> <p>Parameters:</p> Name Type Description Default <code>ia_scaler</code> <code>MinMaxScaler | RobustScaler | StandardScaler</code> <p>The IA scaler.</p> required <code>fixation_scaler</code> <code>MinMaxScaler | RobustScaler | StandardScaler | None</code> <p>Fixation scaler.</p> required <code>trial_features_scaler</code> <code>MinMaxScaler | RobustScaler | StandardScaler | None</code> <p>The trial features scaler.</p> required <code>regime_name</code> <code>SetNames</code> <p>The name of the regime (e.g., unseen_subject_seen_item).</p> required <code>set_name</code> <code>SetNames</code> <p>The name of the set (e.g., train, test, val).</p> required <p>Returns:</p> Name Type Description <code>ETDataset</code> <code>OneStopDataset</code> <p>The created ETDataset instance.</p> Source code in <code>src/data/datamodules/onestop.py</code> <pre><code>def create_etdataset(\n    self,\n    ia_scaler: MinMaxScaler | RobustScaler | StandardScaler | None,\n    fixation_scaler: MinMaxScaler | RobustScaler | StandardScaler | None,\n    trial_features_scaler: MinMaxScaler | RobustScaler | StandardScaler | None,\n    set_name: SetNames,\n    regime_name: SetNames,\n) -&gt; OneStopDataset:\n    \"\"\"\n    Create an OneStopDataSet instance for the given keys.\n\n    Args:\n        ia_scaler (MinMaxScaler | RobustScaler | StandardScaler): The IA scaler.\n        fixation_scaler (MinMaxScaler | RobustScaler | StandardScaler | None): Fixation scaler.\n        trial_features_scaler (MinMaxScaler | RobustScaler | StandardScaler | None):\n            The trial features scaler.\n        regime_name (SetNames): The name of the regime (e.g., unseen_subject_seen_item).\n        set_name (SetNames): The name of the set (e.g., train, test, val).\n\n    Returns:\n        ETDataset: The created ETDataset instance.\n    \"\"\"\n    text_data = None if self.cfg.model.use_eyes_only else self.load_text_dataset()\n\n    dataset = OneStopDataset(\n        cfg=self.cfg,\n        ia_scaler=ia_scaler,\n        fixation_scaler=fixation_scaler,\n        trial_features_scaler=trial_features_scaler,\n        regime_name=regime_name,\n        set_name=set_name,\n        text_data=text_data,\n    )\n\n    return dataset\n</code></pre>"},{"location":"reference/data/datamodules/potec/","title":"potec","text":""},{"location":"reference/data/datamodules/potec/#data.datamodules.potec.PoTeCDataModule","title":"<code>PoTeCDataModule</code>","text":"<p>               Bases: <code>ETDataModuleFast</code></p> <p>A PyTorch Lightning data module for the eye tracking data.</p> <p>Attributes:</p> Name Type Description <code>cfg</code> <code>Args</code> <p>The configuration object.</p> <code>text_dataset_path</code> <code>Path</code> <p>The path to the text dataset.</p> <code>train_dataset</code> <code>PoTeCDataSet</code> <p>The training dataset.</p> <code>val_datasets</code> <code>list[PoTeCDataSet]</code> <p>The validation datasets.</p> <code>test_datasets</code> <code>list[PoTeCDataSet]</code> <p>The test datasets.</p> Source code in <code>src/data/datamodules/potec.py</code> <pre><code>@register_datamodule\nclass PoTeCDataModule(ETDataModuleFast):\n    \"\"\"\n    A PyTorch Lightning data module for the eye tracking data.\n\n    Attributes:\n        cfg (Args): The configuration object.\n        text_dataset_path (Path): The path to the text dataset.\n        train_dataset (PoTeCDataSet): The training dataset.\n        val_datasets (list[PoTeCDataSet]): The validation datasets.\n        test_datasets (list[PoTeCDataSet]): The test datasets.\n    \"\"\"\n\n    def create_etdataset(\n        self,\n        ia_scaler: MinMaxScaler | RobustScaler | StandardScaler | None,\n        fixation_scaler: MinMaxScaler | RobustScaler | StandardScaler | None,\n        trial_features_scaler: MinMaxScaler | RobustScaler | StandardScaler | None,\n        set_name: SetNames,\n        regime_name: SetNames,\n    ) -&gt; PoTeCDataset:\n        \"\"\"\n        Create an PoTeCDataSet instance for the given keys.\n\n        Args:\n            ia_scaler (MinMaxScaler | RobustScaler | StandardScaler): The IA scaler.\n            fixation_scaler (MinMaxScaler | RobustScaler | StandardScaler | None): Fixation scaler.\n            trial_features_scaler (MinMaxScaler | RobustScaler | StandardScaler | None):\n                The trial features scaler.\n            regime_name (SetNames): The name of the regime (e.g., unseen_subject_seen_item).\n            set_name (SetNames): The name of the set (e.g., train, test, val).\n\n        Returns:\n            ETDataset: The created ETDataset instance.\n        \"\"\"\n        text_data = None if self.cfg.model.use_eyes_only else self.load_text_dataset()\n\n        dataset = PoTeCDataset(\n            cfg=self.cfg,\n            ia_scaler=ia_scaler,\n            fixation_scaler=fixation_scaler,\n            trial_features_scaler=trial_features_scaler,\n            regime_name=regime_name,\n            set_name=set_name,\n            text_data=text_data,\n        )\n\n        return dataset\n</code></pre>"},{"location":"reference/data/datamodules/potec/#data.datamodules.potec.PoTeCDataModule.create_etdataset","title":"<code>create_etdataset(ia_scaler, fixation_scaler, trial_features_scaler, set_name, regime_name)</code>","text":"<p>Create an PoTeCDataSet instance for the given keys.</p> <p>Parameters:</p> Name Type Description Default <code>ia_scaler</code> <code>MinMaxScaler | RobustScaler | StandardScaler</code> <p>The IA scaler.</p> required <code>fixation_scaler</code> <code>MinMaxScaler | RobustScaler | StandardScaler | None</code> <p>Fixation scaler.</p> required <code>trial_features_scaler</code> <code>MinMaxScaler | RobustScaler | StandardScaler | None</code> <p>The trial features scaler.</p> required <code>regime_name</code> <code>SetNames</code> <p>The name of the regime (e.g., unseen_subject_seen_item).</p> required <code>set_name</code> <code>SetNames</code> <p>The name of the set (e.g., train, test, val).</p> required <p>Returns:</p> Name Type Description <code>ETDataset</code> <code>PoTeCDataset</code> <p>The created ETDataset instance.</p> Source code in <code>src/data/datamodules/potec.py</code> <pre><code>def create_etdataset(\n    self,\n    ia_scaler: MinMaxScaler | RobustScaler | StandardScaler | None,\n    fixation_scaler: MinMaxScaler | RobustScaler | StandardScaler | None,\n    trial_features_scaler: MinMaxScaler | RobustScaler | StandardScaler | None,\n    set_name: SetNames,\n    regime_name: SetNames,\n) -&gt; PoTeCDataset:\n    \"\"\"\n    Create an PoTeCDataSet instance for the given keys.\n\n    Args:\n        ia_scaler (MinMaxScaler | RobustScaler | StandardScaler): The IA scaler.\n        fixation_scaler (MinMaxScaler | RobustScaler | StandardScaler | None): Fixation scaler.\n        trial_features_scaler (MinMaxScaler | RobustScaler | StandardScaler | None):\n            The trial features scaler.\n        regime_name (SetNames): The name of the regime (e.g., unseen_subject_seen_item).\n        set_name (SetNames): The name of the set (e.g., train, test, val).\n\n    Returns:\n        ETDataset: The created ETDataset instance.\n    \"\"\"\n    text_data = None if self.cfg.model.use_eyes_only else self.load_text_dataset()\n\n    dataset = PoTeCDataset(\n        cfg=self.cfg,\n        ia_scaler=ia_scaler,\n        fixation_scaler=fixation_scaler,\n        trial_features_scaler=trial_features_scaler,\n        regime_name=regime_name,\n        set_name=set_name,\n        text_data=text_data,\n    )\n\n    return dataset\n</code></pre>"},{"location":"reference/data/datamodules/sbsat/","title":"sbsat","text":""},{"location":"reference/data/datamodules/sbsat/#data.datamodules.sbsat.SBSATDataModule","title":"<code>SBSATDataModule</code>","text":"<p>               Bases: <code>ETDataModuleFast</code></p> <p>A PyTorch Lightning data module for the eye tracking data.</p> <p>Attributes:</p> Name Type Description <code>cfg</code> <code>Args</code> <p>The configuration object.</p> <code>text_dataset_path</code> <code>Path</code> <p>The path to the text dataset.</p> <code>train_dataset</code> <code>SBSATDataSet</code> <p>The training dataset.</p> <code>val_datasets</code> <code>list[SBSATDataSet]</code> <p>The validation datasets.</p> <code>test_datasets</code> <code>list[SBSATDataSet]</code> <p>The test datasets.</p> Source code in <code>src/data/datamodules/sbsat.py</code> <pre><code>@register_datamodule\nclass SBSATDataModule(ETDataModuleFast):\n    \"\"\"\n    A PyTorch Lightning data module for the eye tracking data.\n\n    Attributes:\n        cfg (Args): The configuration object.\n        text_dataset_path (Path): The path to the text dataset.\n        train_dataset (SBSATDataSet): The training dataset.\n        val_datasets (list[SBSATDataSet]): The validation datasets.\n        test_datasets (list[SBSATDataSet]): The test datasets.\n    \"\"\"\n\n    def create_etdataset(\n        self,\n        ia_scaler: MinMaxScaler | RobustScaler | StandardScaler | None,\n        fixation_scaler: MinMaxScaler | RobustScaler | StandardScaler | None,\n        trial_features_scaler: MinMaxScaler | RobustScaler | StandardScaler | None,\n        set_name: SetNames,\n        regime_name: SetNames,\n    ) -&gt; SBSATDataset:\n        \"\"\"\n        Create an SBSATDataSet instance for the given keys.\n\n        Args:\n            ia_scaler (MinMaxScaler | RobustScaler | StandardScaler): The IA scaler.\n            fixation_scaler (MinMaxScaler | RobustScaler | StandardScaler | None): Fixation scaler.\n            trial_features_scaler (MinMaxScaler | RobustScaler | StandardScaler | None):\n                The trial features scaler.\n            regime_name (SetNames): The name of the regime (e.g., unseen_subject_seen_item).\n            set_name (SetNames): The name of the set (e.g., train, test, val).\n\n        Returns:\n            ETDataset: The created ETDataset instance.\n        \"\"\"\n        text_data = None if self.cfg.model.use_eyes_only else self.load_text_dataset()\n\n        dataset = SBSATDataset(\n            cfg=self.cfg,\n            ia_scaler=ia_scaler,\n            fixation_scaler=fixation_scaler,\n            trial_features_scaler=trial_features_scaler,\n            regime_name=regime_name,\n            set_name=set_name,\n            text_data=text_data,\n        )\n\n        return dataset\n</code></pre>"},{"location":"reference/data/datamodules/sbsat/#data.datamodules.sbsat.SBSATDataModule.create_etdataset","title":"<code>create_etdataset(ia_scaler, fixation_scaler, trial_features_scaler, set_name, regime_name)</code>","text":"<p>Create an SBSATDataSet instance for the given keys.</p> <p>Parameters:</p> Name Type Description Default <code>ia_scaler</code> <code>MinMaxScaler | RobustScaler | StandardScaler</code> <p>The IA scaler.</p> required <code>fixation_scaler</code> <code>MinMaxScaler | RobustScaler | StandardScaler | None</code> <p>Fixation scaler.</p> required <code>trial_features_scaler</code> <code>MinMaxScaler | RobustScaler | StandardScaler | None</code> <p>The trial features scaler.</p> required <code>regime_name</code> <code>SetNames</code> <p>The name of the regime (e.g., unseen_subject_seen_item).</p> required <code>set_name</code> <code>SetNames</code> <p>The name of the set (e.g., train, test, val).</p> required <p>Returns:</p> Name Type Description <code>ETDataset</code> <code>SBSATDataset</code> <p>The created ETDataset instance.</p> Source code in <code>src/data/datamodules/sbsat.py</code> <pre><code>def create_etdataset(\n    self,\n    ia_scaler: MinMaxScaler | RobustScaler | StandardScaler | None,\n    fixation_scaler: MinMaxScaler | RobustScaler | StandardScaler | None,\n    trial_features_scaler: MinMaxScaler | RobustScaler | StandardScaler | None,\n    set_name: SetNames,\n    regime_name: SetNames,\n) -&gt; SBSATDataset:\n    \"\"\"\n    Create an SBSATDataSet instance for the given keys.\n\n    Args:\n        ia_scaler (MinMaxScaler | RobustScaler | StandardScaler): The IA scaler.\n        fixation_scaler (MinMaxScaler | RobustScaler | StandardScaler | None): Fixation scaler.\n        trial_features_scaler (MinMaxScaler | RobustScaler | StandardScaler | None):\n            The trial features scaler.\n        regime_name (SetNames): The name of the regime (e.g., unseen_subject_seen_item).\n        set_name (SetNames): The name of the set (e.g., train, test, val).\n\n    Returns:\n        ETDataset: The created ETDataset instance.\n    \"\"\"\n    text_data = None if self.cfg.model.use_eyes_only else self.load_text_dataset()\n\n    dataset = SBSATDataset(\n        cfg=self.cfg,\n        ia_scaler=ia_scaler,\n        fixation_scaler=fixation_scaler,\n        trial_features_scaler=trial_features_scaler,\n        regime_name=regime_name,\n        set_name=set_name,\n        text_data=text_data,\n    )\n\n    return dataset\n</code></pre>"},{"location":"reference/data/datasets/TextDataSet/","title":"TextDataSet","text":""},{"location":"reference/data/datasets/TextDataSet/#data.datasets.TextDataSet.TextDataSet","title":"<code>TextDataSet</code>","text":"<p>               Bases: <code>Dataset</code></p> <p>A PyTorch dataset for text data.</p> Source code in <code>src/data/datasets/TextDataSet.py</code> <pre><code>class TextDataSet(TorchDataset):\n    \"\"\"\n    A PyTorch dataset for text data.\n    \"\"\"\n\n    def __init__(self, cfg: Args):\n        self.prediction_mode = cfg.data.task\n        valid_modes = (\n            BINARY_P_AND_Q_TASKS\n            + BINARY_PARAGRAPH_ONLY_TASKS\n            + REGRESSION_PARAGRAPH_ONLY_TASKS\n        )\n        if self.prediction_mode not in valid_modes:\n            raise ValueError(\n                f'Invalid value for PREDICTION_MODE: {self.prediction_mode}'\n            )\n        self.max_data_seq_len = cfg.data.max_seq_len\n        self.max_model_supported_len = cfg.model.max_supported_seq_len\n        self.actual_max_needed_len = min(\n            self.max_data_seq_len, self.max_model_supported_len\n        )\n        self.num_special_tokens_to_add = cfg.model.num_special_tokens_add\n        self.actual_max_seq_len = 0\n        self.max_q_len = cfg.data.max_q_len\n        assert isinstance(cfg.model, (DLModelArgs, MLModelArgs))\n        self.prepend_eye_features_to_text = cfg.model.prepend_eye_features_to_text\n        self.text_key_field = cfg.data.unique_trial_id_column\n        self.preorder = cfg.model.preorder\n\n        self.print_tokens = True\n        self.tokenizer = AutoTokenizer.from_pretrained(\n            cfg.model.backbone,  # type: ignore\n            is_split_into_words=True,\n            add_prefix_space=True,\n        )\n        eye_token = '&lt;eye&gt;'\n        self.tokenizer.add_special_tokens(\n            special_tokens_dict={'additional_special_tokens': [eye_token]},\n            replace_additional_special_tokens=False,\n        )\n        self.eye_token_id: int = self.tokenizer.convert_tokens_to_ids(eye_token)\n\n        text_data = self.prepare_text_data(data_path=cfg.data.ia_path)\n        # create a dict mapping from key column (as the dict key) to index (as the dict value)\n        text_keys = text_data[self.text_key_field].copy()\n        self.key_to_index = dict(zip(text_keys, text_keys.index))\n\n        (\n            self.text_features,\n            self.inversions_lists,\n        ) = self.convert_examples_to_features(\n            text_data,\n        )\n\n        self.text_data = text_data\n\n    def prepare_text_data(self, data_path: Path) -&gt; pd.DataFrame:\n        \"\"\"\n        Prepares the text data by loading it from a CSV file and selecting relevant columns.\n        Args:\n            data_path (Path): The path to the CSV file containing the text data.\n\n        Returns:\n            pd.DataFrame: A DataFrame containing the selected columns from the CSV file\n                after dropping duplicates.\n        \"\"\"\n        usecols = [\n            field.value\n            for field in [\n                Fields.UNIQUE_TRIAL_ID,\n                Fields.QUESTION,\n                Fields.PARAGRAPH,\n            ]\n        ]\n\n        text_data = load_raw_data(data_path)\n\n        missing_columns = [col for col in usecols if col not in text_data.columns]\n        if missing_columns:\n            logger.warning(f'Missing columns: {missing_columns}')\n        existing_columns = [col for col in usecols if col in text_data.columns]\n        logger.info(f'Using columns: {existing_columns}')\n\n        text_data = text_data[existing_columns].copy()\n        text_data = text_data.drop_duplicates(subset=self.text_key_field).reset_index(\n            drop=True\n        )\n        return text_data\n\n    def __len__(self) -&gt; int:\n        return len(self.key_to_index)\n\n    def __getitem__(self, index: int) -&gt; tuple[tuple[torch.Tensor, ...], list[int]]:\n        features = self.text_features[index]\n        inversions_list = self.inversions_lists[index]\n        return features, inversions_list\n\n    def convert_examples_to_features(\n        self,\n        examples: pd.DataFrame,\n    ) -&gt; tuple[torch.Tensor | TorchTensorDataset, list[list[int]]]:\n        # Roberta tokenization\n        \"\"\"Loads a data file into a list of `InputBatch`s.\"\"\"\n\n        # we will use the formatting proposed in \"Improving Language\n        # Understanding by Generative Pre-Training\" and suggested by\n        # @jacobdevlin-google in this issue\n        # https://github.com/google-research/bert/issues/38.\n        assert self.tokenizer.sep_token_id is not None\n        assert self.tokenizer.cls_token_id is not None\n        paragraphs_input_ids_list = []\n        paragraphs_masks_list = []\n        input_ids_list: list[list[int] | list[list[int]]] = []\n        input_masks_list: list[list[int] | list[list[int]]] = []\n        passages_length = []\n        inversions_list = []\n        full_lengths = []\n        for example in tqdm(\n            examples.itertuples(),\n            total=len(examples),\n            desc='Tokenizing',\n        ):\n            paragraph_ids, inversions, full_length = self.tokenize(\n                text=example.paragraph\n            )\n            full_lengths.append(full_length)\n            # TODO Low priority: refactor to avoid duplication of input_ids and p_input_ids\n            p_input_ids = paragraph_ids.copy()\n\n            p_input_ids.insert(0, self.tokenizer.cls_token_id)\n\n            # Zero-pad up to the sequence length.\n            p_input_mask = [1] * len(p_input_ids) + [0] * (\n                self.actual_max_needed_len - len(p_input_ids)\n            )\n            p_input_ids = p_input_ids + [1] * (\n                self.actual_max_needed_len - len(p_input_ids)\n            )\n\n            # Add the paragraph to the lists\n            paragraphs_input_ids_list.append(p_input_ids)\n            paragraphs_masks_list.append(p_input_mask)\n\n            endings_ids = self.add_tokenized_question_if_needed(example)\n            full_ending_ids = []\n            for ending_tokens in endings_ids:\n                full_ending_ids.extend(\n                    ending_tokens\n                )  # * If adding more than one ending, concatenate them. Consider adding separators.\n\n            input_ids, input_masks = self.process_example(\n                paragraph_ids, full_ending_ids\n            )\n            input_ids_list.append(input_ids)\n            input_masks_list.append(input_masks)\n\n            if self.print_tokens:\n                if isinstance(input_ids_list[0][0], list):\n                    for ids in input_ids_list[0]:\n                        logger.info(self.tokenizer.convert_ids_to_tokens(ids))\n                else:\n                    logger.info(self.tokenizer.convert_ids_to_tokens(input_ids_list[0]))\n                self.print_tokens = False\n\n            passages_length.append(len(paragraph_ids))\n            inversions_list.append(inversions)\n        if self.actual_max_needed_len &gt; self.actual_max_seq_len:\n            logger.warning(\n                f'{self.actual_max_needed_len=} while max length in practice is {self.actual_max_seq_len}.'\n            )\n\n        features = TorchTensorDataset(\n            torch.tensor(paragraphs_input_ids_list, dtype=torch.long),\n            torch.tensor(paragraphs_masks_list, dtype=torch.long),\n            torch.tensor(input_ids_list, dtype=torch.long),\n            torch.tensor(input_masks_list, dtype=torch.long),\n            torch.tensor(passages_length, dtype=torch.long),\n            torch.tensor(full_lengths, dtype=torch.long),\n        )\n\n        return features, inversions_list\n\n    def build_inputs_with_special_tokens(\n        self,\n        context_ids: list[int],\n        ending_ids: list[int],\n    ) -&gt; list[int]:\n        \"\"\"\n        Based on from RobertaTokenizer.build_inputs_with_special_tokens\n        #! Check where things break if making changes here\n        \"\"\"\n        assert self.tokenizer.cls_token_id is not None\n        assert self.tokenizer.sep_token_id is not None\n\n        cls_token_id = self.tokenizer.cls_token_id\n        sep_token_id = self.tokenizer.sep_token_id\n\n        input_ids = [cls_token_id]\n\n        if self.prepend_eye_features_to_text:\n            input_ids.extend([self.eye_token_id, sep_token_id])\n\n        input_ids += (\n            context_ids + [sep_token_id, sep_token_id] + ending_ids + [sep_token_id]\n        )\n        return input_ids\n\n    def process_example(\n        self,\n        paragraph_ids: list[int],\n        ending_ids: list[int],\n    ) -&gt; tuple[list[int], list[int]]:\n        input_ids = self.build_inputs_with_special_tokens(paragraph_ids, ending_ids)\n\n        self.verify_input_length(input_ids)\n        padding_length = self.actual_max_needed_len - len(input_ids)\n        # Update input mask and padding for the concatenated sequence\n        input_mask = [1] * len(input_ids) + [0] * padding_length\n        padding_ids = [1] * padding_length  # 1 for roberta\n        input_ids.extend(padding_ids)\n\n        return input_ids, input_mask\n\n    def add_tokenized_question_if_needed(\n        self,\n        example,\n    ) -&gt; list[list[int]]:\n        \"\"\"\n        Processing of example endings based on prediction mode.\n        \"\"\"\n        if self.prediction_mode in BINARY_P_AND_Q_TASKS:\n            endings = [f'Question: {example.question}']\n        else:\n            endings = []\n\n        endings_ids: list[list[int]] = [self.tokenize(ending)[0] for ending in endings]\n\n        return endings_ids\n\n    def verify_input_length(self, tokens: list[int]) -&gt; None:\n        assert len(tokens) &lt;= self.actual_max_needed_len, (\n            f'tokens length is {len(tokens)}, max_seq_length is {self.actual_max_needed_len}'\n        )\n\n        if len(tokens) &gt; self.actual_max_seq_len:\n            self.actual_max_seq_len = len(tokens)\n\n    def tokenize(self, text: str) -&gt; tuple[list[int], list[int], int]:\n        \"\"\"\n        Tokenizes a paragraph into a list of tokens.\n        If the tokenized text exceeds actual_max_needed_len, truncates to keep the last actual_max_needed_len tokens.\n\n        Args:\n            text (str): The paragraph to tokenize.\n\n        Returns:\n            tuple[list[str], list[int]]: The tokenized paragraph and the inversions list.\n\n        \"\"\"\n        tokens = self.tokenizer(\n            text.split(),\n            is_split_into_words=True,\n            add_special_tokens=False,\n        )\n        input_ids: list[int] = tokens['input_ids']\n        token_word_ids: list[int] = tokens.word_ids()\n        full_length = max(token_word_ids) + 1\n        # Truncate to actual_max_needed_len, keeping the last tokens\n        max_length = (\n            self.actual_max_needed_len - self.num_special_tokens_to_add - self.max_q_len\n        )\n\n        if len(input_ids) &gt; max_length:\n            input_ids = input_ids[-max_length:]\n            token_word_ids = token_word_ids[-max_length:]\n            min_id = min(token_word_ids)\n            token_word_ids = [id_ - min_id for id_ in token_word_ids]\n\n        return input_ids, token_word_ids, full_length\n</code></pre>"},{"location":"reference/data/datasets/TextDataSet/#data.datasets.TextDataSet.TextDataSet.add_tokenized_question_if_needed","title":"<code>add_tokenized_question_if_needed(example)</code>","text":"<p>Processing of example endings based on prediction mode.</p> Source code in <code>src/data/datasets/TextDataSet.py</code> <pre><code>def add_tokenized_question_if_needed(\n    self,\n    example,\n) -&gt; list[list[int]]:\n    \"\"\"\n    Processing of example endings based on prediction mode.\n    \"\"\"\n    if self.prediction_mode in BINARY_P_AND_Q_TASKS:\n        endings = [f'Question: {example.question}']\n    else:\n        endings = []\n\n    endings_ids: list[list[int]] = [self.tokenize(ending)[0] for ending in endings]\n\n    return endings_ids\n</code></pre>"},{"location":"reference/data/datasets/TextDataSet/#data.datasets.TextDataSet.TextDataSet.build_inputs_with_special_tokens","title":"<code>build_inputs_with_special_tokens(context_ids, ending_ids)</code>","text":"<p>Based on from RobertaTokenizer.build_inputs_with_special_tokens</p>"},{"location":"reference/data/datasets/TextDataSet/#data.datasets.TextDataSet.TextDataSet.build_inputs_with_special_tokens--check-where-things-break-if-making-changes-here","title":"! Check where things break if making changes here","text":"Source code in <code>src/data/datasets/TextDataSet.py</code> <pre><code>def build_inputs_with_special_tokens(\n    self,\n    context_ids: list[int],\n    ending_ids: list[int],\n) -&gt; list[int]:\n    \"\"\"\n    Based on from RobertaTokenizer.build_inputs_with_special_tokens\n    #! Check where things break if making changes here\n    \"\"\"\n    assert self.tokenizer.cls_token_id is not None\n    assert self.tokenizer.sep_token_id is not None\n\n    cls_token_id = self.tokenizer.cls_token_id\n    sep_token_id = self.tokenizer.sep_token_id\n\n    input_ids = [cls_token_id]\n\n    if self.prepend_eye_features_to_text:\n        input_ids.extend([self.eye_token_id, sep_token_id])\n\n    input_ids += (\n        context_ids + [sep_token_id, sep_token_id] + ending_ids + [sep_token_id]\n    )\n    return input_ids\n</code></pre>"},{"location":"reference/data/datasets/TextDataSet/#data.datasets.TextDataSet.TextDataSet.convert_examples_to_features","title":"<code>convert_examples_to_features(examples)</code>","text":"<p>Loads a data file into a list of <code>InputBatch</code>s.</p> Source code in <code>src/data/datasets/TextDataSet.py</code> <pre><code>def convert_examples_to_features(\n    self,\n    examples: pd.DataFrame,\n) -&gt; tuple[torch.Tensor | TorchTensorDataset, list[list[int]]]:\n    # Roberta tokenization\n    \"\"\"Loads a data file into a list of `InputBatch`s.\"\"\"\n\n    # we will use the formatting proposed in \"Improving Language\n    # Understanding by Generative Pre-Training\" and suggested by\n    # @jacobdevlin-google in this issue\n    # https://github.com/google-research/bert/issues/38.\n    assert self.tokenizer.sep_token_id is not None\n    assert self.tokenizer.cls_token_id is not None\n    paragraphs_input_ids_list = []\n    paragraphs_masks_list = []\n    input_ids_list: list[list[int] | list[list[int]]] = []\n    input_masks_list: list[list[int] | list[list[int]]] = []\n    passages_length = []\n    inversions_list = []\n    full_lengths = []\n    for example in tqdm(\n        examples.itertuples(),\n        total=len(examples),\n        desc='Tokenizing',\n    ):\n        paragraph_ids, inversions, full_length = self.tokenize(\n            text=example.paragraph\n        )\n        full_lengths.append(full_length)\n        # TODO Low priority: refactor to avoid duplication of input_ids and p_input_ids\n        p_input_ids = paragraph_ids.copy()\n\n        p_input_ids.insert(0, self.tokenizer.cls_token_id)\n\n        # Zero-pad up to the sequence length.\n        p_input_mask = [1] * len(p_input_ids) + [0] * (\n            self.actual_max_needed_len - len(p_input_ids)\n        )\n        p_input_ids = p_input_ids + [1] * (\n            self.actual_max_needed_len - len(p_input_ids)\n        )\n\n        # Add the paragraph to the lists\n        paragraphs_input_ids_list.append(p_input_ids)\n        paragraphs_masks_list.append(p_input_mask)\n\n        endings_ids = self.add_tokenized_question_if_needed(example)\n        full_ending_ids = []\n        for ending_tokens in endings_ids:\n            full_ending_ids.extend(\n                ending_tokens\n            )  # * If adding more than one ending, concatenate them. Consider adding separators.\n\n        input_ids, input_masks = self.process_example(\n            paragraph_ids, full_ending_ids\n        )\n        input_ids_list.append(input_ids)\n        input_masks_list.append(input_masks)\n\n        if self.print_tokens:\n            if isinstance(input_ids_list[0][0], list):\n                for ids in input_ids_list[0]:\n                    logger.info(self.tokenizer.convert_ids_to_tokens(ids))\n            else:\n                logger.info(self.tokenizer.convert_ids_to_tokens(input_ids_list[0]))\n            self.print_tokens = False\n\n        passages_length.append(len(paragraph_ids))\n        inversions_list.append(inversions)\n    if self.actual_max_needed_len &gt; self.actual_max_seq_len:\n        logger.warning(\n            f'{self.actual_max_needed_len=} while max length in practice is {self.actual_max_seq_len}.'\n        )\n\n    features = TorchTensorDataset(\n        torch.tensor(paragraphs_input_ids_list, dtype=torch.long),\n        torch.tensor(paragraphs_masks_list, dtype=torch.long),\n        torch.tensor(input_ids_list, dtype=torch.long),\n        torch.tensor(input_masks_list, dtype=torch.long),\n        torch.tensor(passages_length, dtype=torch.long),\n        torch.tensor(full_lengths, dtype=torch.long),\n    )\n\n    return features, inversions_list\n</code></pre>"},{"location":"reference/data/datasets/TextDataSet/#data.datasets.TextDataSet.TextDataSet.prepare_text_data","title":"<code>prepare_text_data(data_path)</code>","text":"<p>Prepares the text data by loading it from a CSV file and selecting relevant columns. Args:     data_path (Path): The path to the CSV file containing the text data.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A DataFrame containing the selected columns from the CSV file after dropping duplicates.</p> Source code in <code>src/data/datasets/TextDataSet.py</code> <pre><code>def prepare_text_data(self, data_path: Path) -&gt; pd.DataFrame:\n    \"\"\"\n    Prepares the text data by loading it from a CSV file and selecting relevant columns.\n    Args:\n        data_path (Path): The path to the CSV file containing the text data.\n\n    Returns:\n        pd.DataFrame: A DataFrame containing the selected columns from the CSV file\n            after dropping duplicates.\n    \"\"\"\n    usecols = [\n        field.value\n        for field in [\n            Fields.UNIQUE_TRIAL_ID,\n            Fields.QUESTION,\n            Fields.PARAGRAPH,\n        ]\n    ]\n\n    text_data = load_raw_data(data_path)\n\n    missing_columns = [col for col in usecols if col not in text_data.columns]\n    if missing_columns:\n        logger.warning(f'Missing columns: {missing_columns}')\n    existing_columns = [col for col in usecols if col in text_data.columns]\n    logger.info(f'Using columns: {existing_columns}')\n\n    text_data = text_data[existing_columns].copy()\n    text_data = text_data.drop_duplicates(subset=self.text_key_field).reset_index(\n        drop=True\n    )\n    return text_data\n</code></pre>"},{"location":"reference/data/datasets/TextDataSet/#data.datasets.TextDataSet.TextDataSet.tokenize","title":"<code>tokenize(text)</code>","text":"<p>Tokenizes a paragraph into a list of tokens. If the tokenized text exceeds actual_max_needed_len, truncates to keep the last actual_max_needed_len tokens.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The paragraph to tokenize.</p> required <p>Returns:</p> Type Description <code>tuple[list[int], list[int], int]</code> <p>tuple[list[str], list[int]]: The tokenized paragraph and the inversions list.</p> Source code in <code>src/data/datasets/TextDataSet.py</code> <pre><code>def tokenize(self, text: str) -&gt; tuple[list[int], list[int], int]:\n    \"\"\"\n    Tokenizes a paragraph into a list of tokens.\n    If the tokenized text exceeds actual_max_needed_len, truncates to keep the last actual_max_needed_len tokens.\n\n    Args:\n        text (str): The paragraph to tokenize.\n\n    Returns:\n        tuple[list[str], list[int]]: The tokenized paragraph and the inversions list.\n\n    \"\"\"\n    tokens = self.tokenizer(\n        text.split(),\n        is_split_into_words=True,\n        add_special_tokens=False,\n    )\n    input_ids: list[int] = tokens['input_ids']\n    token_word_ids: list[int] = tokens.word_ids()\n    full_length = max(token_word_ids) + 1\n    # Truncate to actual_max_needed_len, keeping the last tokens\n    max_length = (\n        self.actual_max_needed_len - self.num_special_tokens_to_add - self.max_q_len\n    )\n\n    if len(input_ids) &gt; max_length:\n        input_ids = input_ids[-max_length:]\n        token_word_ids = token_word_ids[-max_length:]\n        min_id = min(token_word_ids)\n        token_word_ids = [id_ - min_id for id_ in token_word_ids]\n\n    return input_ids, token_word_ids, full_length\n</code></pre>"},{"location":"reference/data/datasets/base_dataset/","title":"base_dataset","text":""},{"location":"reference/data/datasets/base_dataset/#data.datasets.base_dataset.ETDataset","title":"<code>ETDataset</code>","text":"<p>               Bases: <code>Dataset</code></p> <p>A base class for eye tracking datasets.</p> <p>Attributes:</p> Name Type Description <code>set_name</code> <code>SetNames</code> <p>The name of the set (e.g., train, test, val).</p> <code>regime_name</code> <code>SetNames</code> <p>The name of the regime (e.g., unseen_subject_seen_item).</p> Source code in <code>src/data/datasets/base_dataset.py</code> <pre><code>class ETDataset(TorchDataset):\n    \"\"\"\n    A base class for eye tracking datasets.\n\n    Attributes:\n        set_name (SetNames): The name of the set (e.g., train, test, val).\n        regime_name (SetNames): The name of the regime (e.g., unseen_subject_seen_item).\n    \"\"\"\n\n    def __init__(\n        self,\n        cfg: Args,\n        set_name: SetNames,\n        regime_name: SetNames,\n        ia_scaler: MinMaxScaler | RobustScaler | StandardScaler | None = None,\n        fixation_scaler: MinMaxScaler | RobustScaler | StandardScaler | None = None,\n        trial_features_scaler: MinMaxScaler\n        | RobustScaler\n        | StandardScaler\n        | None = None,\n        text_data: TextDataSet | None = None,\n    ):\n        super().__init__()\n        self.set_name = set_name\n        self.regime_name = regime_name\n        self.ia_scaler = ia_scaler\n        self.fixation_scaler = fixation_scaler\n        self.trial_features_scaler = trial_features_scaler\n        self.use_fixation_data = cfg.model.use_fixation_report\n        self.ia_feature_cols = cfg.model.ia_features\n        self.fixation_feature_cols = (\n            (\n                cfg.model.fixation_features\n                + cfg.model.ia_features_to_add_to_fixation_data\n            )\n            if self.use_fixation_data\n            else []\n        )\n        assert isinstance(cfg.model, (DLModelArgs, MLModelArgs))\n        self.ia_categorical_features = cfg.model.ia_categorical_features\n        self.compute_trial_level_features = cfg.model.compute_trial_level_features\n        self.max_data_seq_len = cfg.data.max_seq_len\n        self.max_model_supported_len = cfg.model.max_supported_seq_len\n        self.actual_max_needed_len = min(\n            self.max_data_seq_len, self.max_model_supported_len\n        )\n        self.max_scanpath_len = cfg.data.max_scanpath_length\n        self.max_tokens_in_word = cfg.data.max_tokens_in_word\n        self.normalize = cfg.model.normalization_mode\n        self.prediction_mode = cfg.data.task\n        self.base_model_name = cfg.model.base_model_name\n        self.model_name = cfg.model.model_name\n        self.prepend_eye_features_to_text = cfg.model.prepend_eye_features_to_text\n        self.item_level_features_modes = cfg.model.item_level_features_modes\n        self.print_first_nan_occurrences = True\n        self.actual_max_tokens_in_word = 0\n        self.data_name = cfg.data.dataset_name\n        self.folds_folder_name = cfg.data.folds_folder_name\n        if text_data is not None:\n            self.n_tokens = len(text_data.tokenizer)\n            self.eye_token_id = text_data.eye_token_id\n            self.sep_token_id = text_data.tokenizer.sep_token_id\n        self.target_column = cfg.data.target_column\n        self.is_reg = len(list(cfg.data.class_names)) == 1\n        self.trial_groupby_columns = cfg.data.groupby_columns\n\n        (\n            self.features,\n            self.labels,\n            self.grouped_ia_data,\n            self.grouped_fixation_data,\n            self.grouped_raw_fixation_scanpath_ia_labels,\n            self.trial_level_features,\n            self.trial_level_feature_names,\n            self.ordered_key_list,\n            self.ia_scaler,\n            self.fixation_scaler,\n            self.trial_features_scaler,\n        ) = ETDataset.cache_or_load_feature(\n            cache_file_path=self.create_features_identifier(cfg=cfg),\n            overwrite_feature=cfg.trainer.overwrite_data,\n            create_feature_func=self.prepare_data,\n            create_feature_func_args=dict(\n                text_data=text_data,\n                cfg=cfg,\n            ),\n        )\n\n    @staticmethod\n    def organize_label_counts(\n        labels: list[int], label_names: list[str]\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        Organize label counts into a DataFrame.\n\n        Args:\n            labels (list): The labels to organize.\n            label_names (str): The label names.\n\n        Returns:\n            pd.DataFrame: The organized label counts.\n        \"\"\"\n        label_counts = np.unique(labels, return_counts=True)\n        label_counts = pd.DataFrame(label_counts, index=['label', 'count']).T\n        label_counts['percent'] = (\n            label_counts['count'] / label_counts['count'].sum() * 100\n        )\n\n        label_counts['percent'] = (\n            label_counts['percent']\n            .astype(\n                float,\n            )\n            .round(2)\n        )\n        label_counts.attrs['name'] = label_names\n        return label_counts\n\n    @staticmethod\n    def normalize_features(\n        x: pd.DataFrame | pd.Series,\n        normalize: NormalizationModes,\n        scaler: MinMaxScaler | RobustScaler | StandardScaler,\n    ) -&gt; np.ndarray:\n        \"\"\"\n        Normalize features based on the specified mode.\n\n        Args:\n            x (pd.DataFrame | pd.Series): The features to normalize.\n            normalize (NormalizationModes): The normalization mode.\n            scaler (MinMaxScaler | RobustScaler | StandardScaler): The scaler to use.\n        Returns:\n            np.ndarray: The normalized features.\n        \"\"\"\n\n        if normalize == NormalizationModes.NONE:\n            return x.to_numpy()\n        x_input = pd.DataFrame(x).T if isinstance(x, pd.Series) else x\n        if normalize == NormalizationModes.ALL:\n            normalized_x = scaler.transform(x_input)\n        elif normalize == NormalizationModes.TRIAL:\n            normalized_x = scaler.fit_transform(x_input)\n        else:\n            raise ValueError(\n                f'Invalid value for normalize: {normalize}, type: {type(normalize)}',\n            )\n        return normalized_x\n\n    @staticmethod\n    def cache_or_load_feature(\n        cache_file_path: Path,\n        overwrite_feature: bool,\n        create_feature_func: Callable,\n        create_feature_func_args: dict[str, Any],\n    ) -&gt; tuple:\n        \"\"\"\n        Cache or load a feature from disk.\n\n        Args:\n            cache_file_path (Path): The path to the cache file.\n            overwrite_feature (bool): Whether to overwrite existing feature.\n            create_feature_func (Callable): The function to create the feature.\n            create_feature_func_args (dict): The arguments for the feature creation function.\n\n        Returns:\n            np.ndarray |\n            pd.DataFrame |\n            torch.utils.data.dataset.TensorDataset |\n            tuple[torch.utils.data.dataset.TensorDataset, torch.Tensor]\n                The cached or loaded feature.\n        \"\"\"\n        if overwrite_feature or not cache_file_path.exists():\n            cache_file_path.parent.mkdir(parents=True, exist_ok=True)\n            logger.info(f'Caching features to {cache_file_path}')\n            feature = create_feature_func(**create_feature_func_args)\n            joblib.dump(feature, cache_file_path, compress=('zlib', 3))\n        else:\n            logger.info(f'Loading features from {cache_file_path}')\n            feature = joblib.load(cache_file_path)\n            if type(feature) not in [\n                np.ndarray,\n                pd.DataFrame,\n                torch.utils.data.dataset.TensorDataset,\n                tuple,\n            ]:\n                raise ValueError(\n                    'Feature is not a numpy array / pytorch tensor / pandas dataframe',\n                )\n\n        return feature\n\n    @staticmethod\n    def fit_scaler_if_not_fitted(\n        scaler: MinMaxScaler | RobustScaler | StandardScaler,\n        raw_data: pd.DataFrame,\n        set_name: SetNames,\n        feature_columns: list[str] | None = None,\n        ia_categorical: list[str] = [],\n    ) -&gt; MinMaxScaler | RobustScaler | StandardScaler:\n        \"\"\"\n        Fit a scaler if it is not already fitted.\n\n        Args:\n            scaler (Union[MinMaxScaler, RobustScaler, StandardScaler]):\n                The scaler to fit.\n            raw_data (pd.DataFrame): The raw data to fit the scaler on.\n            feature_columns (Optional[list[str]], optional): The feature columns to use.\n                Defaults to None.\n\n        Returns:\n            Union[MinMaxScaler, RobustScaler, StandardScaler]: The fitted scaler.\n        \"\"\"\n        try:\n            check_is_fitted(scaler)\n        except NotFittedError as exc:\n            if set_name != SetNames.TRAIN:\n                raise ValueError(\n                    f\"Scaler {scaler} is not fitted and set_name is not 'train'.\",\n                ) from exc\n            # TODO Move feature selection out of this function\n            if not feature_columns:\n                feature_columns = raw_data.columns.to_list()\n\n            numeric_only_df = raw_data[feature_columns].drop(\n                columns=ia_categorical,\n                errors='ignore',\n            )\n            non_numeric = numeric_only_df.select_dtypes(\n                exclude=['number', 'bool'],\n            )\n            if not non_numeric.empty:\n                raise ValueError(\n                    f'Non-numeric columns found in {set_name} set: {non_numeric.columns}',\n                ) from exc\n\n            scaler.fit(numeric_only_df)\n            logger.info(f'Fitted {scaler} on {numeric_only_df.columns}')\n        return scaler\n\n    def __len__(self) -&gt; int:\n        \"\"\"\n        Get the number of unique groups in the dataset.\n\n        Returns:\n            int: The number of unique groups in the dataset.\n        \"\"\"\n        return len(self.grouped_ia_data.groups)\n\n    def __getitem__(\n        self,\n        idx: int | np.integer,\n    ) -&gt; tuple[dict[str, torch.Tensor], torch.Tensor, tuple, list[str]]:\n        \"\"\"\n        Get an item from the dataset.\n\n        Args:\n            idx (int): The index of the item.\n\n        Returns:\n            tuple: A tuple containing the features, labels,\n                ordered key list, and trial groupby columns.\n        \"\"\"\n        # TODO I think torch dataset is faster and takes less storage than this.\n        # Find a way to use it while keeping the names. Maybe store the names in\n        # a list as they do not change.\n        example_feats = {name: tensor[idx] for name, tensor in self.features.items()}\n\n        return (\n            example_feats,\n            self.labels[idx],\n            self.ordered_key_list[idx],\n            self.trial_groupby_columns,\n        )\n\n    def convert_examples_to_features(\n        self,\n        text_data: TextDataSet | None,\n    ) -&gt; Tuple[dict[str, torch.Tensor], torch.Tensor]:\n        \"\"\"\n        Convert the examples in the dataset to features.\n\n        Args:\n            text_data (TextDataSet | None): The text data.\n\n        Returns:\n            dict[str, torch.Tensor]: A dictionary containing the converted features.\n        \"\"\"\n\n        features = {}\n\n        if self.compute_trial_level_features:\n            features.update(self.extract_trial_level_features())\n\n        if self.use_fixation_data:\n            features.update(self.get_fixation_features(text_data=text_data))\n\n        if self.ia_feature_cols:\n            features.update(self.get_ia_features(text_data=text_data))\n\n        if text_data:\n            features.update(self.get_text_features(text_data, features))\n\n        labels = self.get_labels()\n\n        return features, labels\n\n    def get_ia_features(self, text_data: TextDataSet | None) -&gt; dict[str, torch.Tensor]:\n        \"\"\"\n        Generate a list of normalized eye data for all trials.\n\n        Returns:\n        list[np.ndarray]: A list of normalized eye data.\n        \"\"\"\n        eyes_list = []\n        for grouped_data_key in tqdm(self.ordered_key_list, desc='IA features'):\n            trial = self.grouped_ia_data.get_group(grouped_data_key)\n            eyes, _, _ = self.get_eye_data(trial=trial, text_data=text_data)\n            eyes_list.append(eyes)\n\n        return {'eyes': torch.tensor(np.array(eyes_list), dtype=torch.float32)}\n\n    def group_to_length(\n        self,\n        lst: list[int],\n        col_pad_to_len: int,\n        row_pad_to_len: int,\n        inv_list_to_token_word_attn_mask: bool = False,\n    ) -&gt; torch.Tensor:\n        \"\"\"\n        Pad a list of values to a predefined length.\n\n        Example: [1, 1, 1, 2, 2, 3, 3, 3, 3] -&gt;\n            [tensor([[0, 1, 2, -1], [3, 4, -1, -1], [5, 6, 7, 8]])]\n        Three words, first word has 3 tokens, second word has 2 tokens, third word has 4 tokens.\n        Input list assumed to be sorted.\n        Used to represent token to word mapping.\n        I.e., in input, each token (index in lst) is mapped to a word index (value in lst),\n        in output each word index (row) is mapped to a token index (values in row).\n\n        Args:\n            lst (list): The list of values to pad.\n            col_pad_to_len (int): The length to pad to number of cols.\n            row_pad_to_len (int): The length to pad to number of rows.\n            inv_list_to_token_word_attn_mask (bool, optional):\n                Whether to convert the list to a token-word attention mask. Defaults to False.\n\n        Returns:\n            torch.Tensor: A tensor containing the padded values.\n        \"\"\"\n        # Group the list by the values, and convert to a tensor\n        # Example: [1, 1, 1, 2, 2, 3, 3, 3, 3] -&gt;\n        #  [tensor([0, 1, 2]), tensor([3, 4]), tensor([5, 6, 7, 8])\n        grouped_lst = [\n            torch.tensor(data=list(group))\n            for _, group in itertools.groupby(\n                iterable=range(len(lst)),\n                key=lambda x: lst[x],\n            )\n        ]\n\n        if inv_list_to_token_word_attn_mask:\n            # [1, 1, 1, 2, 2, 3, 3, 3, 3] -&gt; [tensor([0, 1, 2]), tensor([3, 4]), tensor([5, 6, 7, 8])\n            #     Before attending previous and next word:\n            #     [[1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n            #      [1, 0, 0, 0, 1, 1, 0, 0, 0, 0],\n            #      [1, 0, 0, 0, 0, 0, 1, 1, 1, 1],\n            #      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n            #      ...\n            #      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n\n            #      After attending previous and next word:\n            #     [[1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n            #      [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n            #      [1, 0, 0, 0, 1, 1, 1, 1, 1, 1],\n            #      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n            #      ...\n            #      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n            # Note: the first column is used to attend to the [CLS] token.\n\n            size = (\n                self.actual_max_needed_len\n            )  # ! Can be reduced to maximal num of *words* in a paragraph (not tokens)\n            matrix = torch.zeros(size + 1, size)\n            for i, row in enumerate(grouped_lst):\n                matrix[i, 0] = 1\n                matrix[i, row + 1] = 1\n                # Add attention to the previous and next word\n                if i &gt; 0:\n                    matrix[i, grouped_lst[i - 1] + 1] = 1\n                if i &lt; len(grouped_lst) - 1:\n                    matrix[i, grouped_lst[i + 1] + 1] = 1\n            return matrix\n\n        current_max_tokens_in_word = max(len(group) for group in grouped_lst)\n        if current_max_tokens_in_word &gt; self.actual_max_tokens_in_word:\n            self.actual_max_tokens_in_word: int = current_max_tokens_in_word\n\n        # Add padding\n        # Example:\n        # [\n        #   tensor([0, 1, 2]),    -&gt; tensor([0, 1, 2, -2])\n        #   tensor([3, 4]),        -&gt; tensor([3, 4, -1, -2])\n        #   tensor([5, 6, 7, 8]),  -&gt; tensor([5, 6, 7, 8])\n        # ]\n        padded_tensor = pad_sequence(\n            sequences=grouped_lst,\n            batch_first=True,\n            padding_value=-2,\n        )\n\n        num_padding_cols = max(0, col_pad_to_len - padded_tensor.size(dim=1))\n        padding = torch.full(\n            size=(padded_tensor.size(dim=0), num_padding_cols),\n            fill_value=-2,\n        )\n        padded_tensor = torch.cat(tensors=(padded_tensor, padding), dim=1)\n\n        # Calculate the number of rows needed to reach the predefined length\n        num_padding_rows = max(0, row_pad_to_len - padded_tensor.size(dim=0))\n\n        # Create a tensor of padding values\n        padding = torch.full(\n            size=(num_padding_rows, padded_tensor.size(dim=1)),\n            fill_value=-2,\n        )\n\n        # Concatenate the padding to the padded_tensor\n        padded_tensor = torch.cat(tensors=(padded_tensor, padding), dim=0)\n        padded_tensor += 1\n        return padded_tensor\n\n    def create_features_identifier(\n        self,\n        cfg: Args,\n    ) -&gt; Path:\n        \"\"\"\n        Create an identifier for features.\n\n        Args:\n            cfg (Args): The configuration.\n\n        Returns:\n            str: The features identifier.\n        \"\"\"\n\n        return (\n            FEATURES_CACHE_FOLDER\n            / (f'{self.data_name}_{self.prediction_mode}_{self.model_name}')\n            / f'fold_{cfg.data.fold_index}'\n            / f'{self.regime_name}_{self.set_name}.pkl'\n        )\n\n    def get_trial_text_data(self, text_data: TextDataSet, trial_info: pd.Series):\n        \"\"\"\n        Get the text data for a trial.\n\n        Args:\n            text_data (TextDataSet): The text data.\n            key (str): The key for the trial.\n        Returns:\n            tuple: The text data for the trial.\n        \"\"\"\n\n        key_ = trial_info[text_data.text_key_field]\n        text_index = text_data.key_to_index[key_]\n        (\n            (\n                p_input_ids,\n                p_input_masks,\n                input_ids,\n                input_mask,\n                passage_length,\n                full_length,\n            ),\n            inversions_list,\n        ) = text_data[text_index]\n\n        return (\n            p_input_ids,\n            p_input_masks,\n            input_ids,\n            input_mask,\n            passage_length,\n            full_length,\n            inversions_list,\n        )\n\n    def get_text_features(\n        self, text_data: TextDataSet, features: dict[str, torch.Tensor]\n    ) -&gt; dict[str, torch.Tensor]:\n        input_ids_list = []\n        input_masks_list = []\n        p_input_ids_list = []\n        grouped_inversions = []\n        eyes_list = []\n        for idx in tqdm(range(len(self.ordered_key_list)), desc='Text features'):\n            grouped_data_key = self.ordered_key_list[idx]\n            trial = self.grouped_ia_data.get_group(grouped_data_key)\n            (\n                p_input_ids,\n                _,\n                input_ids,\n                input_mask,\n                _,\n                _,\n                inversions_list,\n            ) = self.get_trial_text_data(text_data=text_data, trial_info=trial.iloc[0])\n\n            input_ids_list.append(input_ids)\n            input_ids_unsqueeze = input_ids\n            input_mask_unsqueeze = input_mask\n            if len(input_ids.shape) == 1:\n                input_ids_unsqueeze = input_ids_unsqueeze.unsqueeze(dim=0)\n                input_mask_unsqueeze = input_mask_unsqueeze.unsqueeze(dim=0)\n\n            p_input_ids_list.append(p_input_ids)\n\n            eyes, eye_seq_len, pad_length = self.get_eye_data(\n                trial=trial, text_data=text_data, inversions_list=inversions_list\n            )  # TODO recomputes eyes\n\n            inv_list_to_token_word_attn_mask = (\n                self.base_model_name == DLModelNames.POSTFUSION_MODEL\n            )\n\n            group_inversions = self.group_to_length(\n                lst=inversions_list,\n                col_pad_to_len=self.max_tokens_in_word,\n                row_pad_to_len=self.actual_max_needed_len,\n                inv_list_to_token_word_attn_mask=inv_list_to_token_word_attn_mask,\n            )\n            if self.use_fixation_data:\n                scanpath = features['scanpath'][idx, :]\n                actual_scanpath = scanpath[scanpath &gt;= 0]  # Remove padding values\n\n                group_inversions = group_inversions[actual_scanpath]\n\n                # Pad group_inversions to max_scanpath_len\n                num_padding_rows = self.max_scanpath_len - group_inversions.size(dim=0)\n                if num_padding_rows &gt; 0:\n                    padding = torch.full(\n                        size=(num_padding_rows, group_inversions.size(dim=1)),\n                        fill_value=-1,\n                    )\n                    group_inversions = torch.cat(\n                        tensors=(group_inversions, padding), dim=0\n                    )\n\n            eyes_list.append(eyes)\n\n            if self.prepend_eye_features_to_text:\n                if self.use_fixation_data:\n                    pad_len = features['fixation_pads'][idx]\n                    seq_len = self.max_scanpath_len - pad_len\n\n                else:\n                    seq_len = eye_seq_len\n                    pad_len = pad_length\n\n                ones = np.ones(seq_len)\n                zeroes = np.zeros(pad_len)\n                eye_mask = np.concatenate((ones, zeroes), axis=0)\n\n                axis = 0\n                input_mask = torch.from_numpy(\n                    np.concatenate((eye_mask, input_mask), axis=axis),\n                )\n\n            input_masks_list.append(input_mask)\n            grouped_inversions.append(group_inversions)\n\n        if self.max_tokens_in_word &gt; self.actual_max_tokens_in_word:\n            logger.warning(\n                f'{self.actual_max_tokens_in_word=} but using {self.max_tokens_in_word=}'\n            )\n        result = {\n            'input_ids': torch.stack(input_ids_list),\n            'input_masks': torch.stack(input_masks_list),\n            'grouped_inversions': torch.stack(grouped_inversions),\n            'p_input_ids': torch.stack(p_input_ids_list),\n            'eyes': torch.tensor(np.array(eyes_list), dtype=torch.float32),\n        }\n\n        return result\n\n    def get_eye_data(\n        self, trial: pd.DataFrame, text_data: TextDataSet | None, inversions_list=None\n    ) -&gt; Tuple[np.ndarray, int, int]:\n        \"\"\"\n        Extract and normalize eye data from a trial.\n\n        Args:\n        trial (pd.DataFrame): The trial data.\n        text_data (TextDataSet): The text data.\n        inversions_list (list, optional): The list of inversions. Defaults to None.\n\n        Returns:\n        np.ndarray: The normalized eye data.\n        \"\"\"\n        if text_data:\n            (\n                _,\n                _,\n                _,\n                _,\n                _,\n                _,\n                inversions_list,\n            ) = self.get_trial_text_data(text_data=text_data, trial_info=trial.iloc[0])\n            length_in_words = max(inversions_list) + 1\n            trial = trial.tail(length_in_words).copy()\n\n        eyes = trial[self.ia_feature_cols].drop(\n            columns=self.ia_categorical_features,\n            errors='ignore',\n        )\n\n        eyes = ETDataset.normalize_features(\n            eyes,\n            normalize=self.normalize,\n            scaler=self.ia_scaler,\n        )\n        num_pre_eye_tokens = 0  # TODO hardcoded value\n        if not self.prepend_eye_features_to_text and inversions_list:\n            aligned_eyes = [eyes[inv_idx, :] for inv_idx in inversions_list]\n            eyes = np.stack(aligned_eyes)\n            num_pre_eye_tokens = 1\n\n        eye_seq_len, eyes_dim = eyes.shape\n        eyes_pad_left = np.zeros((num_pre_eye_tokens, eyes_dim))\n        pad_length = self.actual_max_needed_len - eye_seq_len - num_pre_eye_tokens\n        if pad_length &lt; 0:\n            logger.error(\n                f'Eye data length {eye_seq_len} exceeds max eye length {self.actual_max_needed_len}'\n            )\n        eyes_pad_right = np.zeros((pad_length, eyes_dim))\n        eyes = np.concatenate((eyes_pad_left, eyes, eyes_pad_right))\n        eyes = np.nan_to_num(eyes, nan=0.0)  # TODO this shouldn't be needed\n        return eyes, eye_seq_len, pad_length\n\n    def get_labels(self) -&gt; torch.Tensor:\n        labels_list = []\n        for grouped_data_key in tqdm(self.ordered_key_list, desc='Label'):\n            trial = self.grouped_ia_data.get_group(grouped_data_key)\n            assert trial[self.target_column].nunique() == 1, (\n                f'Label {self.target_column} is not the same for all rows in {grouped_data_key}'\n            )\n            y = trial.iloc[0][self.target_column]\n\n            labels_list.append(y)\n        return torch.tensor(\n            labels_list, dtype=torch.float32 if self.is_reg else torch.long\n        )\n\n    def prepare_data(\n        self,\n        cfg: Args,\n        text_data: TextDataSet | None,\n    ) -&gt; tuple:\n        # Define a partial function for loading dataframes\n        load_data_partial = partial(\n            load_fold_data,\n            fold_index=cfg.data.fold_index,\n            base_path=cfg.data.base_path,\n            folds_folder_name=cfg.data.folds_folder_name,\n            set_name=self.set_name,\n            regime_name=self.regime_name,\n        )\n\n        ia_data = load_data_partial(data_type=DataType.IA)\n        if cfg.data.task != PredMode.RC and cfg.data.n_questions_per_item &gt; 1:\n            before = len(ia_data)\n            ia_data = (\n                ia_data[ia_data['question_index'].isin([1, 'tq_1'])]\n                .drop(columns=['question_index'])\n                .copy()\n            )\n            logger.info(\n                f'Kept {len(ia_data)} out of {before} ({(len(ia_data) / before) * 100})% in ia_data'\n            )\n        filtered_ia = ia_data[\n            list(set(self.trial_groupby_columns + self.ia_feature_cols))\n        ].copy()\n        if filtered_ia.columns[filtered_ia.isna().any()].tolist():\n            warnings.warn(\n                f'{\n                    filtered_ia.columns[filtered_ia.isna().any()].tolist()\n                }. Forward filling and backward filling.',\n            )\n        filtered_ia = filtered_ia.ffill().bfill()\n        self.grouped_ia_data = filtered_ia.groupby(self.trial_groupby_columns)\n        self.ordered_key_list = list(self.grouped_ia_data.groups)\n        if self.ia_feature_cols:\n            # filtered_ia = remove_nan_values(filtered_ia)\n            self.ia_scaler = self.fit_scaler_if_not_fitted(\n                scaler=self.ia_scaler,\n                raw_data=filtered_ia,\n                set_name=self.set_name,\n                feature_columns=self.ia_feature_cols,\n                ia_categorical=self.ia_categorical_features,\n            )\n        else:\n            self.ia_scaler = None\n\n        if self.use_fixation_data:\n            fixation_data = load_data_partial(data_type=DataType.FIXATIONS)\n            if cfg.data.task != PredMode.RC and cfg.data.n_questions_per_item &gt; 1:\n                before = len(fixation_data)\n                fixation_data = (\n                    fixation_data[fixation_data['question_index'].isin([1, 'tq_1'])]\n                    .drop(columns=['question_index'])\n                    .copy()\n                )\n                logger.info(f'Removed {len(fixation_data) / before} % duplicate rows')\n            filtered_fixations = fixation_data[\n                list(\n                    set(\n                        self.trial_groupby_columns\n                        + self.fixation_feature_cols\n                        + [Fields.FIXATION_REPORT_IA_ID_COL_NAME]\n                    )\n                )\n            ].copy()\n\n            if filtered_fixations.columns[filtered_fixations.isna().any()].tolist():\n                warnings.warn(\n                    f'{\n                        filtered_fixations.columns[\n                            filtered_fixations.isna().any()\n                        ].tolist()\n                    }. Forward filling and backward filling.',\n                )\n            filtered_fixations = filtered_fixations.ffill().bfill()\n\n            # filtered_fixations = remove_nan_values(filtered_fixations)\n\n            self.grouped_fixation_data = filtered_fixations[\n                self.trial_groupby_columns + self.fixation_feature_cols\n            ].groupby(\n                self.trial_groupby_columns\n            )  # TODO add a check that fixation, ia and trial keys are the same\n            raw_fixation_scanpath_ia_labels = filtered_fixations[\n                self.trial_groupby_columns + [Fields.FIXATION_REPORT_IA_ID_COL_NAME]\n            ]\n            self.grouped_raw_fixation_scanpath_ia_labels = (\n                raw_fixation_scanpath_ia_labels.groupby(self.trial_groupby_columns)\n            )\n            self.fixation_scaler = self.fit_scaler_if_not_fitted(\n                scaler=self.fixation_scaler,\n                raw_data=filtered_fixations,\n                set_name=self.set_name,\n                feature_columns=self.fixation_feature_cols,\n                ia_categorical=self.ia_categorical_features,\n            )\n        else:\n            self.grouped_fixation_data = None\n            self.grouped_raw_fixation_scanpath_ia_labels = None\n            self.fixation_scaler = None\n\n        if cfg.model.compute_trial_level_features:\n            trial_level_data = load_data_partial(data_type=DataType.TRIAL_LEVEL)\n            assert trial_level_data is not None\n            ia_feature_names = pd.read_csv(\n                cfg.data.processed_data_path / 'ia_trial_level_feature_keys.csv'\n            )\n            fixation_feature_names = pd.read_csv(\n                cfg.data.processed_data_path / 'fixation_trial_level_feature_keys.csv'\n            )\n            feature_names = pd.concat(\n                [ia_feature_names, fixation_feature_names],\n                axis=0,\n            )\n            self.trial_level_feature_names = (\n                feature_names[\n                    feature_names['feature_type'].isin(self.item_level_features_modes)\n                ]['feature_name']\n                .drop_duplicates()\n                .tolist()\n            )\n            logger.info(\n                f'Using {len(self.trial_level_feature_names)} trial level features.'\n            )\n            trial_level_data = trial_level_data[self.trial_level_feature_names]\n            # keep only trials whose unique_trial_id ends with '1'\n            if cfg.data.task != PredMode.RC and cfg.data.n_questions_per_item &gt; 1:\n                unique_ids = trial_level_data.index.get_level_values(\n                    level='unique_trial_id'\n                ).astype(str)\n                mask = unique_ids.str.endswith('1')\n                before = len(trial_level_data)\n                trial_level_data = trial_level_data[mask].copy()\n                logger.info(\n                    f'Removed {len(trial_level_data) / before} % duplicate rows in trial_level_data'\n                )\n            self.trial_level_features = trial_level_data\n            self.trial_features_scaler = self.fit_scaler_if_not_fitted(\n                scaler=self.trial_features_scaler,\n                raw_data=self.trial_level_features,\n                feature_columns=self.trial_level_feature_names,\n                set_name=self.set_name,\n                ia_categorical=self.ia_categorical_features,\n            )\n        else:\n            self.trial_level_features = None\n            self.trial_level_feature_names = None\n            self.trial_features_scaler = None\n\n        features, labels = self.convert_examples_to_features(text_data)\n\n        return (\n            features,\n            labels,\n            self.grouped_ia_data,\n            self.grouped_fixation_data,\n            self.grouped_raw_fixation_scanpath_ia_labels,\n            self.trial_level_features,\n            self.trial_level_feature_names,\n            self.ordered_key_list,\n            self.ia_scaler,\n            self.fixation_scaler,\n            self.trial_features_scaler,\n        )\n\n    def get_fixation_features(\n        self, text_data: TextDataSet | None\n    ) -&gt; dict[str, torch.Tensor]:\n        \"\"\"\n        Convert the examples in the dataset to fixation features.\n\n        Returns:\n            tuple: A tuple containing the fixation features, pads, scanpath, and scanpath pads.\n        \"\"\"\n        fixation_list = []\n        pads_list = []\n        scanpath_list = []\n        scanpath_pads_list = []\n        for grouped_data_key in tqdm(self.ordered_key_list, desc='Fixation features'):\n            # Get the data group associated with the given index.\n            trial = self.grouped_fixation_data.get_group(\n                grouped_data_key,\n            ).copy()\n\n            scanpath = self.grouped_raw_fixation_scanpath_ia_labels[\n                Fields.FIXATION_REPORT_IA_ID_COL_NAME\n            ].get_group(grouped_data_key)\n            if text_data:\n                (\n                    _,\n                    _,\n                    _,\n                    _,\n                    _,\n                    full_length,\n                    inversions_list,\n                ) = self.get_trial_text_data(\n                    text_data=text_data,\n                    trial_info=trial.iloc[0],\n                )\n                truncated_words = full_length - (max(inversions_list) + 1)\n                trial = trial[\n                    (\n                        trial[Fields.FIXATION_REPORT_IA_ID_COL_NAME]\n                        &gt; int(truncated_words)\n                    )\n                    | (trial[Fields.FIXATION_REPORT_IA_ID_COL_NAME] == -1)\n                ].copy()\n                scanpath = scanpath[\n                    (scanpath &gt; int(truncated_words)) | (scanpath == -1)\n                ].copy()\n                # decrease by truncated_words for all that are not -1\n                scanpath = scanpath.apply(\n                    lambda x: x - int(truncated_words) if x != -1 else x\n                )\n                for col in (\n                    Fields.FIXATION_REPORT_IA_ID_COL_NAME,\n                    'NEXT_FIX_INTEREST_AREA_INDEX',\n                ):\n                    if col in trial.columns:\n                        trial[col] = trial[col].apply(\n                            lambda x: x - int(truncated_words) if x != -1 else x\n                        )\n\n            fixation = trial[self.fixation_feature_cols].drop(\n                columns=self.ia_categorical_features,\n                errors='ignore',\n            )\n\n            fixation = ETDataset.normalize_features(\n                fixation,\n                normalize=self.normalize,\n                scaler=self.fixation_scaler,\n            )\n\n            if self.compute_trial_level_features:\n                # concat back the \"is_content_word\" and \"ptb_pos\" columns from trial\n                # TODO BEyeLSTM specific code, save as different variable?\n                fixation = np.concatenate(\n                    (\n                        fixation,\n                        trial[['is_content_word', 'ptb_pos']].to_numpy(),\n                    ),  # ! Order matters here!\n                    axis=1,\n                )\n\n            pad_length = self.max_scanpath_len - len(fixation)\n            fixation_dim = fixation.shape[1]\n\n            fixation_padding = np.zeros((pad_length, fixation_dim))\n            fixation = np.concatenate((fixation, fixation_padding))\n            # fixation = fixation[: self.max_scanpath_len] # TODO Do we want this here? Was for PoTeC only\n            # pad the scanpath with -1\n            scanpath_padding = np.full(pad_length, SCANPATH_PADDING_VAL)\n            scanpath = np.concatenate((scanpath, scanpath_padding))\n            # scanpath = scanpath[: self.max_scanpath_len] # TODO Do we want this here? Was for PoTeC only\n\n            fixation_list.append(fixation)\n            pads_list.append(pad_length)\n            scanpath_list.append(scanpath)\n            scanpath_pads_list.append(pad_length)\n\n        # TODO sure we want to fillna here?\n        fixation_list = [pd.DataFrame(fix_list).fillna(0) for fix_list in fixation_list]\n        ret = {\n            'fixation_features': torch.tensor(\n                np.array(fixation_list).astype(float), dtype=torch.float32\n            ),\n            'fixation_pads': torch.tensor(pads_list, dtype=torch.long),\n            'scanpath': torch.tensor(np.array(scanpath_list), dtype=torch.long),\n            'scanpath_pads': torch.tensor(scanpath_pads_list, dtype=torch.long),\n        }\n        return ret\n\n    def extract_trial_level_features(self) -&gt; dict[str, torch.Tensor]:\n        trial_level_features_list = []\n        trial_level_features = self.trial_level_features.copy()\n        trial_level_features = trial_level_features.drop(\n            columns=self.ia_categorical_features,\n            errors='ignore',\n        )\n\n        for grouped_data_key in tqdm(\n            self.ordered_key_list, desc='Trial level features'\n        ):\n            trial_features = trial_level_features.loc[grouped_data_key]\n\n            trial_features = ETDataset.normalize_features(\n                trial_features,\n                normalize=self.normalize,\n                scaler=self.trial_features_scaler,\n            )\n            trial_level_features_list.append(trial_features)\n\n        return {\n            'trial_level_features': torch.tensor(\n                np.array(trial_level_features_list),\n                dtype=torch.float32,\n            )\n        }\n</code></pre>"},{"location":"reference/data/datasets/base_dataset/#data.datasets.base_dataset.ETDataset.__getitem__","title":"<code>__getitem__(idx)</code>","text":"<p>Get an item from the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>idx</code> <code>int</code> <p>The index of the item.</p> required <p>Returns:</p> Name Type Description <code>tuple</code> <code>tuple[dict[str, Tensor], Tensor, tuple, list[str]]</code> <p>A tuple containing the features, labels, ordered key list, and trial groupby columns.</p> Source code in <code>src/data/datasets/base_dataset.py</code> <pre><code>def __getitem__(\n    self,\n    idx: int | np.integer,\n) -&gt; tuple[dict[str, torch.Tensor], torch.Tensor, tuple, list[str]]:\n    \"\"\"\n    Get an item from the dataset.\n\n    Args:\n        idx (int): The index of the item.\n\n    Returns:\n        tuple: A tuple containing the features, labels,\n            ordered key list, and trial groupby columns.\n    \"\"\"\n    # TODO I think torch dataset is faster and takes less storage than this.\n    # Find a way to use it while keeping the names. Maybe store the names in\n    # a list as they do not change.\n    example_feats = {name: tensor[idx] for name, tensor in self.features.items()}\n\n    return (\n        example_feats,\n        self.labels[idx],\n        self.ordered_key_list[idx],\n        self.trial_groupby_columns,\n    )\n</code></pre>"},{"location":"reference/data/datasets/base_dataset/#data.datasets.base_dataset.ETDataset.__len__","title":"<code>__len__()</code>","text":"<p>Get the number of unique groups in the dataset.</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>The number of unique groups in the dataset.</p> Source code in <code>src/data/datasets/base_dataset.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"\n    Get the number of unique groups in the dataset.\n\n    Returns:\n        int: The number of unique groups in the dataset.\n    \"\"\"\n    return len(self.grouped_ia_data.groups)\n</code></pre>"},{"location":"reference/data/datasets/base_dataset/#data.datasets.base_dataset.ETDataset.cache_or_load_feature","title":"<code>cache_or_load_feature(cache_file_path, overwrite_feature, create_feature_func, create_feature_func_args)</code>  <code>staticmethod</code>","text":"<p>Cache or load a feature from disk.</p> <p>Parameters:</p> Name Type Description Default <code>cache_file_path</code> <code>Path</code> <p>The path to the cache file.</p> required <code>overwrite_feature</code> <code>bool</code> <p>Whether to overwrite existing feature.</p> required <code>create_feature_func</code> <code>Callable</code> <p>The function to create the feature.</p> required <code>create_feature_func_args</code> <code>dict</code> <p>The arguments for the feature creation function.</p> required <p>Returns:</p> Type Description <code>tuple</code> <p>np.ndarray |</p> <code>tuple</code> <p>pd.DataFrame |</p> <code>tuple</code> <p>torch.utils.data.dataset.TensorDataset |</p> <code>tuple</code> <p>tuple[torch.utils.data.dataset.TensorDataset, torch.Tensor] The cached or loaded feature.</p> Source code in <code>src/data/datasets/base_dataset.py</code> <pre><code>@staticmethod\ndef cache_or_load_feature(\n    cache_file_path: Path,\n    overwrite_feature: bool,\n    create_feature_func: Callable,\n    create_feature_func_args: dict[str, Any],\n) -&gt; tuple:\n    \"\"\"\n    Cache or load a feature from disk.\n\n    Args:\n        cache_file_path (Path): The path to the cache file.\n        overwrite_feature (bool): Whether to overwrite existing feature.\n        create_feature_func (Callable): The function to create the feature.\n        create_feature_func_args (dict): The arguments for the feature creation function.\n\n    Returns:\n        np.ndarray |\n        pd.DataFrame |\n        torch.utils.data.dataset.TensorDataset |\n        tuple[torch.utils.data.dataset.TensorDataset, torch.Tensor]\n            The cached or loaded feature.\n    \"\"\"\n    if overwrite_feature or not cache_file_path.exists():\n        cache_file_path.parent.mkdir(parents=True, exist_ok=True)\n        logger.info(f'Caching features to {cache_file_path}')\n        feature = create_feature_func(**create_feature_func_args)\n        joblib.dump(feature, cache_file_path, compress=('zlib', 3))\n    else:\n        logger.info(f'Loading features from {cache_file_path}')\n        feature = joblib.load(cache_file_path)\n        if type(feature) not in [\n            np.ndarray,\n            pd.DataFrame,\n            torch.utils.data.dataset.TensorDataset,\n            tuple,\n        ]:\n            raise ValueError(\n                'Feature is not a numpy array / pytorch tensor / pandas dataframe',\n            )\n\n    return feature\n</code></pre>"},{"location":"reference/data/datasets/base_dataset/#data.datasets.base_dataset.ETDataset.convert_examples_to_features","title":"<code>convert_examples_to_features(text_data)</code>","text":"<p>Convert the examples in the dataset to features.</p> <p>Parameters:</p> Name Type Description Default <code>text_data</code> <code>TextDataSet | None</code> <p>The text data.</p> required <p>Returns:</p> Type Description <code>Tuple[dict[str, Tensor], Tensor]</code> <p>dict[str, torch.Tensor]: A dictionary containing the converted features.</p> Source code in <code>src/data/datasets/base_dataset.py</code> <pre><code>def convert_examples_to_features(\n    self,\n    text_data: TextDataSet | None,\n) -&gt; Tuple[dict[str, torch.Tensor], torch.Tensor]:\n    \"\"\"\n    Convert the examples in the dataset to features.\n\n    Args:\n        text_data (TextDataSet | None): The text data.\n\n    Returns:\n        dict[str, torch.Tensor]: A dictionary containing the converted features.\n    \"\"\"\n\n    features = {}\n\n    if self.compute_trial_level_features:\n        features.update(self.extract_trial_level_features())\n\n    if self.use_fixation_data:\n        features.update(self.get_fixation_features(text_data=text_data))\n\n    if self.ia_feature_cols:\n        features.update(self.get_ia_features(text_data=text_data))\n\n    if text_data:\n        features.update(self.get_text_features(text_data, features))\n\n    labels = self.get_labels()\n\n    return features, labels\n</code></pre>"},{"location":"reference/data/datasets/base_dataset/#data.datasets.base_dataset.ETDataset.create_features_identifier","title":"<code>create_features_identifier(cfg)</code>","text":"<p>Create an identifier for features.</p> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <code>Args</code> <p>The configuration.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>Path</code> <p>The features identifier.</p> Source code in <code>src/data/datasets/base_dataset.py</code> <pre><code>def create_features_identifier(\n    self,\n    cfg: Args,\n) -&gt; Path:\n    \"\"\"\n    Create an identifier for features.\n\n    Args:\n        cfg (Args): The configuration.\n\n    Returns:\n        str: The features identifier.\n    \"\"\"\n\n    return (\n        FEATURES_CACHE_FOLDER\n        / (f'{self.data_name}_{self.prediction_mode}_{self.model_name}')\n        / f'fold_{cfg.data.fold_index}'\n        / f'{self.regime_name}_{self.set_name}.pkl'\n    )\n</code></pre>"},{"location":"reference/data/datasets/base_dataset/#data.datasets.base_dataset.ETDataset.fit_scaler_if_not_fitted","title":"<code>fit_scaler_if_not_fitted(scaler, raw_data, set_name, feature_columns=None, ia_categorical=[])</code>  <code>staticmethod</code>","text":"<p>Fit a scaler if it is not already fitted.</p> <p>Parameters:</p> Name Type Description Default <code>scaler</code> <code>Union[MinMaxScaler, RobustScaler, StandardScaler]</code> <p>The scaler to fit.</p> required <code>raw_data</code> <code>DataFrame</code> <p>The raw data to fit the scaler on.</p> required <code>feature_columns</code> <code>Optional[list[str]]</code> <p>The feature columns to use. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>MinMaxScaler | RobustScaler | StandardScaler</code> <p>Union[MinMaxScaler, RobustScaler, StandardScaler]: The fitted scaler.</p> Source code in <code>src/data/datasets/base_dataset.py</code> <pre><code>@staticmethod\ndef fit_scaler_if_not_fitted(\n    scaler: MinMaxScaler | RobustScaler | StandardScaler,\n    raw_data: pd.DataFrame,\n    set_name: SetNames,\n    feature_columns: list[str] | None = None,\n    ia_categorical: list[str] = [],\n) -&gt; MinMaxScaler | RobustScaler | StandardScaler:\n    \"\"\"\n    Fit a scaler if it is not already fitted.\n\n    Args:\n        scaler (Union[MinMaxScaler, RobustScaler, StandardScaler]):\n            The scaler to fit.\n        raw_data (pd.DataFrame): The raw data to fit the scaler on.\n        feature_columns (Optional[list[str]], optional): The feature columns to use.\n            Defaults to None.\n\n    Returns:\n        Union[MinMaxScaler, RobustScaler, StandardScaler]: The fitted scaler.\n    \"\"\"\n    try:\n        check_is_fitted(scaler)\n    except NotFittedError as exc:\n        if set_name != SetNames.TRAIN:\n            raise ValueError(\n                f\"Scaler {scaler} is not fitted and set_name is not 'train'.\",\n            ) from exc\n        # TODO Move feature selection out of this function\n        if not feature_columns:\n            feature_columns = raw_data.columns.to_list()\n\n        numeric_only_df = raw_data[feature_columns].drop(\n            columns=ia_categorical,\n            errors='ignore',\n        )\n        non_numeric = numeric_only_df.select_dtypes(\n            exclude=['number', 'bool'],\n        )\n        if not non_numeric.empty:\n            raise ValueError(\n                f'Non-numeric columns found in {set_name} set: {non_numeric.columns}',\n            ) from exc\n\n        scaler.fit(numeric_only_df)\n        logger.info(f'Fitted {scaler} on {numeric_only_df.columns}')\n    return scaler\n</code></pre>"},{"location":"reference/data/datasets/base_dataset/#data.datasets.base_dataset.ETDataset.get_eye_data","title":"<code>get_eye_data(trial, text_data, inversions_list=None)</code>","text":"<p>Extract and normalize eye data from a trial.</p> <p>Args: trial (pd.DataFrame): The trial data. text_data (TextDataSet): The text data. inversions_list (list, optional): The list of inversions. Defaults to None.</p> <p>Returns: np.ndarray: The normalized eye data.</p> Source code in <code>src/data/datasets/base_dataset.py</code> <pre><code>def get_eye_data(\n    self, trial: pd.DataFrame, text_data: TextDataSet | None, inversions_list=None\n) -&gt; Tuple[np.ndarray, int, int]:\n    \"\"\"\n    Extract and normalize eye data from a trial.\n\n    Args:\n    trial (pd.DataFrame): The trial data.\n    text_data (TextDataSet): The text data.\n    inversions_list (list, optional): The list of inversions. Defaults to None.\n\n    Returns:\n    np.ndarray: The normalized eye data.\n    \"\"\"\n    if text_data:\n        (\n            _,\n            _,\n            _,\n            _,\n            _,\n            _,\n            inversions_list,\n        ) = self.get_trial_text_data(text_data=text_data, trial_info=trial.iloc[0])\n        length_in_words = max(inversions_list) + 1\n        trial = trial.tail(length_in_words).copy()\n\n    eyes = trial[self.ia_feature_cols].drop(\n        columns=self.ia_categorical_features,\n        errors='ignore',\n    )\n\n    eyes = ETDataset.normalize_features(\n        eyes,\n        normalize=self.normalize,\n        scaler=self.ia_scaler,\n    )\n    num_pre_eye_tokens = 0  # TODO hardcoded value\n    if not self.prepend_eye_features_to_text and inversions_list:\n        aligned_eyes = [eyes[inv_idx, :] for inv_idx in inversions_list]\n        eyes = np.stack(aligned_eyes)\n        num_pre_eye_tokens = 1\n\n    eye_seq_len, eyes_dim = eyes.shape\n    eyes_pad_left = np.zeros((num_pre_eye_tokens, eyes_dim))\n    pad_length = self.actual_max_needed_len - eye_seq_len - num_pre_eye_tokens\n    if pad_length &lt; 0:\n        logger.error(\n            f'Eye data length {eye_seq_len} exceeds max eye length {self.actual_max_needed_len}'\n        )\n    eyes_pad_right = np.zeros((pad_length, eyes_dim))\n    eyes = np.concatenate((eyes_pad_left, eyes, eyes_pad_right))\n    eyes = np.nan_to_num(eyes, nan=0.0)  # TODO this shouldn't be needed\n    return eyes, eye_seq_len, pad_length\n</code></pre>"},{"location":"reference/data/datasets/base_dataset/#data.datasets.base_dataset.ETDataset.get_fixation_features","title":"<code>get_fixation_features(text_data)</code>","text":"<p>Convert the examples in the dataset to fixation features.</p> <p>Returns:</p> Name Type Description <code>tuple</code> <code>dict[str, Tensor]</code> <p>A tuple containing the fixation features, pads, scanpath, and scanpath pads.</p> Source code in <code>src/data/datasets/base_dataset.py</code> <pre><code>def get_fixation_features(\n    self, text_data: TextDataSet | None\n) -&gt; dict[str, torch.Tensor]:\n    \"\"\"\n    Convert the examples in the dataset to fixation features.\n\n    Returns:\n        tuple: A tuple containing the fixation features, pads, scanpath, and scanpath pads.\n    \"\"\"\n    fixation_list = []\n    pads_list = []\n    scanpath_list = []\n    scanpath_pads_list = []\n    for grouped_data_key in tqdm(self.ordered_key_list, desc='Fixation features'):\n        # Get the data group associated with the given index.\n        trial = self.grouped_fixation_data.get_group(\n            grouped_data_key,\n        ).copy()\n\n        scanpath = self.grouped_raw_fixation_scanpath_ia_labels[\n            Fields.FIXATION_REPORT_IA_ID_COL_NAME\n        ].get_group(grouped_data_key)\n        if text_data:\n            (\n                _,\n                _,\n                _,\n                _,\n                _,\n                full_length,\n                inversions_list,\n            ) = self.get_trial_text_data(\n                text_data=text_data,\n                trial_info=trial.iloc[0],\n            )\n            truncated_words = full_length - (max(inversions_list) + 1)\n            trial = trial[\n                (\n                    trial[Fields.FIXATION_REPORT_IA_ID_COL_NAME]\n                    &gt; int(truncated_words)\n                )\n                | (trial[Fields.FIXATION_REPORT_IA_ID_COL_NAME] == -1)\n            ].copy()\n            scanpath = scanpath[\n                (scanpath &gt; int(truncated_words)) | (scanpath == -1)\n            ].copy()\n            # decrease by truncated_words for all that are not -1\n            scanpath = scanpath.apply(\n                lambda x: x - int(truncated_words) if x != -1 else x\n            )\n            for col in (\n                Fields.FIXATION_REPORT_IA_ID_COL_NAME,\n                'NEXT_FIX_INTEREST_AREA_INDEX',\n            ):\n                if col in trial.columns:\n                    trial[col] = trial[col].apply(\n                        lambda x: x - int(truncated_words) if x != -1 else x\n                    )\n\n        fixation = trial[self.fixation_feature_cols].drop(\n            columns=self.ia_categorical_features,\n            errors='ignore',\n        )\n\n        fixation = ETDataset.normalize_features(\n            fixation,\n            normalize=self.normalize,\n            scaler=self.fixation_scaler,\n        )\n\n        if self.compute_trial_level_features:\n            # concat back the \"is_content_word\" and \"ptb_pos\" columns from trial\n            # TODO BEyeLSTM specific code, save as different variable?\n            fixation = np.concatenate(\n                (\n                    fixation,\n                    trial[['is_content_word', 'ptb_pos']].to_numpy(),\n                ),  # ! Order matters here!\n                axis=1,\n            )\n\n        pad_length = self.max_scanpath_len - len(fixation)\n        fixation_dim = fixation.shape[1]\n\n        fixation_padding = np.zeros((pad_length, fixation_dim))\n        fixation = np.concatenate((fixation, fixation_padding))\n        # fixation = fixation[: self.max_scanpath_len] # TODO Do we want this here? Was for PoTeC only\n        # pad the scanpath with -1\n        scanpath_padding = np.full(pad_length, SCANPATH_PADDING_VAL)\n        scanpath = np.concatenate((scanpath, scanpath_padding))\n        # scanpath = scanpath[: self.max_scanpath_len] # TODO Do we want this here? Was for PoTeC only\n\n        fixation_list.append(fixation)\n        pads_list.append(pad_length)\n        scanpath_list.append(scanpath)\n        scanpath_pads_list.append(pad_length)\n\n    # TODO sure we want to fillna here?\n    fixation_list = [pd.DataFrame(fix_list).fillna(0) for fix_list in fixation_list]\n    ret = {\n        'fixation_features': torch.tensor(\n            np.array(fixation_list).astype(float), dtype=torch.float32\n        ),\n        'fixation_pads': torch.tensor(pads_list, dtype=torch.long),\n        'scanpath': torch.tensor(np.array(scanpath_list), dtype=torch.long),\n        'scanpath_pads': torch.tensor(scanpath_pads_list, dtype=torch.long),\n    }\n    return ret\n</code></pre>"},{"location":"reference/data/datasets/base_dataset/#data.datasets.base_dataset.ETDataset.get_ia_features","title":"<code>get_ia_features(text_data)</code>","text":"<p>Generate a list of normalized eye data for all trials.</p> <p>Returns: list[np.ndarray]: A list of normalized eye data.</p> Source code in <code>src/data/datasets/base_dataset.py</code> <pre><code>def get_ia_features(self, text_data: TextDataSet | None) -&gt; dict[str, torch.Tensor]:\n    \"\"\"\n    Generate a list of normalized eye data for all trials.\n\n    Returns:\n    list[np.ndarray]: A list of normalized eye data.\n    \"\"\"\n    eyes_list = []\n    for grouped_data_key in tqdm(self.ordered_key_list, desc='IA features'):\n        trial = self.grouped_ia_data.get_group(grouped_data_key)\n        eyes, _, _ = self.get_eye_data(trial=trial, text_data=text_data)\n        eyes_list.append(eyes)\n\n    return {'eyes': torch.tensor(np.array(eyes_list), dtype=torch.float32)}\n</code></pre>"},{"location":"reference/data/datasets/base_dataset/#data.datasets.base_dataset.ETDataset.get_trial_text_data","title":"<code>get_trial_text_data(text_data, trial_info)</code>","text":"<p>Get the text data for a trial.</p> <p>Parameters:</p> Name Type Description Default <code>text_data</code> <code>TextDataSet</code> <p>The text data.</p> required <code>key</code> <code>str</code> <p>The key for the trial.</p> required <p>Returns:     tuple: The text data for the trial.</p> Source code in <code>src/data/datasets/base_dataset.py</code> <pre><code>def get_trial_text_data(self, text_data: TextDataSet, trial_info: pd.Series):\n    \"\"\"\n    Get the text data for a trial.\n\n    Args:\n        text_data (TextDataSet): The text data.\n        key (str): The key for the trial.\n    Returns:\n        tuple: The text data for the trial.\n    \"\"\"\n\n    key_ = trial_info[text_data.text_key_field]\n    text_index = text_data.key_to_index[key_]\n    (\n        (\n            p_input_ids,\n            p_input_masks,\n            input_ids,\n            input_mask,\n            passage_length,\n            full_length,\n        ),\n        inversions_list,\n    ) = text_data[text_index]\n\n    return (\n        p_input_ids,\n        p_input_masks,\n        input_ids,\n        input_mask,\n        passage_length,\n        full_length,\n        inversions_list,\n    )\n</code></pre>"},{"location":"reference/data/datasets/base_dataset/#data.datasets.base_dataset.ETDataset.group_to_length","title":"<code>group_to_length(lst, col_pad_to_len, row_pad_to_len, inv_list_to_token_word_attn_mask=False)</code>","text":"<p>Pad a list of values to a predefined length.</p> [1, 1, 1, 2, 2, 3, 3, 3, 3] -&gt; <p>[tensor([[0, 1, 2, -1], [3, 4, -1, -1], [5, 6, 7, 8]])]</p> <p>Three words, first word has 3 tokens, second word has 2 tokens, third word has 4 tokens. Input list assumed to be sorted. Used to represent token to word mapping. I.e., in input, each token (index in lst) is mapped to a word index (value in lst), in output each word index (row) is mapped to a token index (values in row).</p> <p>Parameters:</p> Name Type Description Default <code>lst</code> <code>list</code> <p>The list of values to pad.</p> required <code>col_pad_to_len</code> <code>int</code> <p>The length to pad to number of cols.</p> required <code>row_pad_to_len</code> <code>int</code> <p>The length to pad to number of rows.</p> required <code>inv_list_to_token_word_attn_mask</code> <code>bool</code> <p>Whether to convert the list to a token-word attention mask. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: A tensor containing the padded values.</p> Source code in <code>src/data/datasets/base_dataset.py</code> <pre><code>def group_to_length(\n    self,\n    lst: list[int],\n    col_pad_to_len: int,\n    row_pad_to_len: int,\n    inv_list_to_token_word_attn_mask: bool = False,\n) -&gt; torch.Tensor:\n    \"\"\"\n    Pad a list of values to a predefined length.\n\n    Example: [1, 1, 1, 2, 2, 3, 3, 3, 3] -&gt;\n        [tensor([[0, 1, 2, -1], [3, 4, -1, -1], [5, 6, 7, 8]])]\n    Three words, first word has 3 tokens, second word has 2 tokens, third word has 4 tokens.\n    Input list assumed to be sorted.\n    Used to represent token to word mapping.\n    I.e., in input, each token (index in lst) is mapped to a word index (value in lst),\n    in output each word index (row) is mapped to a token index (values in row).\n\n    Args:\n        lst (list): The list of values to pad.\n        col_pad_to_len (int): The length to pad to number of cols.\n        row_pad_to_len (int): The length to pad to number of rows.\n        inv_list_to_token_word_attn_mask (bool, optional):\n            Whether to convert the list to a token-word attention mask. Defaults to False.\n\n    Returns:\n        torch.Tensor: A tensor containing the padded values.\n    \"\"\"\n    # Group the list by the values, and convert to a tensor\n    # Example: [1, 1, 1, 2, 2, 3, 3, 3, 3] -&gt;\n    #  [tensor([0, 1, 2]), tensor([3, 4]), tensor([5, 6, 7, 8])\n    grouped_lst = [\n        torch.tensor(data=list(group))\n        for _, group in itertools.groupby(\n            iterable=range(len(lst)),\n            key=lambda x: lst[x],\n        )\n    ]\n\n    if inv_list_to_token_word_attn_mask:\n        # [1, 1, 1, 2, 2, 3, 3, 3, 3] -&gt; [tensor([0, 1, 2]), tensor([3, 4]), tensor([5, 6, 7, 8])\n        #     Before attending previous and next word:\n        #     [[1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n        #      [1, 0, 0, 0, 1, 1, 0, 0, 0, 0],\n        #      [1, 0, 0, 0, 0, 0, 1, 1, 1, 1],\n        #      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n        #      ...\n        #      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n\n        #      After attending previous and next word:\n        #     [[1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n        #      [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n        #      [1, 0, 0, 0, 1, 1, 1, 1, 1, 1],\n        #      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n        #      ...\n        #      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n        # Note: the first column is used to attend to the [CLS] token.\n\n        size = (\n            self.actual_max_needed_len\n        )  # ! Can be reduced to maximal num of *words* in a paragraph (not tokens)\n        matrix = torch.zeros(size + 1, size)\n        for i, row in enumerate(grouped_lst):\n            matrix[i, 0] = 1\n            matrix[i, row + 1] = 1\n            # Add attention to the previous and next word\n            if i &gt; 0:\n                matrix[i, grouped_lst[i - 1] + 1] = 1\n            if i &lt; len(grouped_lst) - 1:\n                matrix[i, grouped_lst[i + 1] + 1] = 1\n        return matrix\n\n    current_max_tokens_in_word = max(len(group) for group in grouped_lst)\n    if current_max_tokens_in_word &gt; self.actual_max_tokens_in_word:\n        self.actual_max_tokens_in_word: int = current_max_tokens_in_word\n\n    # Add padding\n    # Example:\n    # [\n    #   tensor([0, 1, 2]),    -&gt; tensor([0, 1, 2, -2])\n    #   tensor([3, 4]),        -&gt; tensor([3, 4, -1, -2])\n    #   tensor([5, 6, 7, 8]),  -&gt; tensor([5, 6, 7, 8])\n    # ]\n    padded_tensor = pad_sequence(\n        sequences=grouped_lst,\n        batch_first=True,\n        padding_value=-2,\n    )\n\n    num_padding_cols = max(0, col_pad_to_len - padded_tensor.size(dim=1))\n    padding = torch.full(\n        size=(padded_tensor.size(dim=0), num_padding_cols),\n        fill_value=-2,\n    )\n    padded_tensor = torch.cat(tensors=(padded_tensor, padding), dim=1)\n\n    # Calculate the number of rows needed to reach the predefined length\n    num_padding_rows = max(0, row_pad_to_len - padded_tensor.size(dim=0))\n\n    # Create a tensor of padding values\n    padding = torch.full(\n        size=(num_padding_rows, padded_tensor.size(dim=1)),\n        fill_value=-2,\n    )\n\n    # Concatenate the padding to the padded_tensor\n    padded_tensor = torch.cat(tensors=(padded_tensor, padding), dim=0)\n    padded_tensor += 1\n    return padded_tensor\n</code></pre>"},{"location":"reference/data/datasets/base_dataset/#data.datasets.base_dataset.ETDataset.normalize_features","title":"<code>normalize_features(x, normalize, scaler)</code>  <code>staticmethod</code>","text":"<p>Normalize features based on the specified mode.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>DataFrame | Series</code> <p>The features to normalize.</p> required <code>normalize</code> <code>NormalizationModes</code> <p>The normalization mode.</p> required <code>scaler</code> <code>MinMaxScaler | RobustScaler | StandardScaler</code> <p>The scaler to use.</p> required <p>Returns:     np.ndarray: The normalized features.</p> Source code in <code>src/data/datasets/base_dataset.py</code> <pre><code>@staticmethod\ndef normalize_features(\n    x: pd.DataFrame | pd.Series,\n    normalize: NormalizationModes,\n    scaler: MinMaxScaler | RobustScaler | StandardScaler,\n) -&gt; np.ndarray:\n    \"\"\"\n    Normalize features based on the specified mode.\n\n    Args:\n        x (pd.DataFrame | pd.Series): The features to normalize.\n        normalize (NormalizationModes): The normalization mode.\n        scaler (MinMaxScaler | RobustScaler | StandardScaler): The scaler to use.\n    Returns:\n        np.ndarray: The normalized features.\n    \"\"\"\n\n    if normalize == NormalizationModes.NONE:\n        return x.to_numpy()\n    x_input = pd.DataFrame(x).T if isinstance(x, pd.Series) else x\n    if normalize == NormalizationModes.ALL:\n        normalized_x = scaler.transform(x_input)\n    elif normalize == NormalizationModes.TRIAL:\n        normalized_x = scaler.fit_transform(x_input)\n    else:\n        raise ValueError(\n            f'Invalid value for normalize: {normalize}, type: {type(normalize)}',\n        )\n    return normalized_x\n</code></pre>"},{"location":"reference/data/datasets/base_dataset/#data.datasets.base_dataset.ETDataset.organize_label_counts","title":"<code>organize_label_counts(labels, label_names)</code>  <code>staticmethod</code>","text":"<p>Organize label counts into a DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>labels</code> <code>list</code> <p>The labels to organize.</p> required <code>label_names</code> <code>str</code> <p>The label names.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: The organized label counts.</p> Source code in <code>src/data/datasets/base_dataset.py</code> <pre><code>@staticmethod\ndef organize_label_counts(\n    labels: list[int], label_names: list[str]\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Organize label counts into a DataFrame.\n\n    Args:\n        labels (list): The labels to organize.\n        label_names (str): The label names.\n\n    Returns:\n        pd.DataFrame: The organized label counts.\n    \"\"\"\n    label_counts = np.unique(labels, return_counts=True)\n    label_counts = pd.DataFrame(label_counts, index=['label', 'count']).T\n    label_counts['percent'] = (\n        label_counts['count'] / label_counts['count'].sum() * 100\n    )\n\n    label_counts['percent'] = (\n        label_counts['percent']\n        .astype(\n            float,\n        )\n        .round(2)\n    )\n    label_counts.attrs['name'] = label_names\n    return label_counts\n</code></pre>"},{"location":"reference/data/datasets/copco/","title":"copco","text":"<p>Data module for creating the data.</p>"},{"location":"reference/data/datasets/copco/#data.datasets.copco.CopCoDataset","title":"<code>CopCoDataset</code>","text":"<p>               Bases: <code>ETDataset</code></p> <p>Dataset for CopCo; inherits ETDataset and doesn't require a custom init.</p> Source code in <code>src/data/datasets/copco.py</code> <pre><code>class CopCoDataset(ETDataset):\n    \"\"\"Dataset for CopCo; inherits ETDataset and doesn't require a custom __init__.\"\"\"\n</code></pre>"},{"location":"reference/data/datasets/iitbhgc/","title":"iitbhgc","text":"<p>Data module for creating the data.</p>"},{"location":"reference/data/datasets/iitbhgc/#data.datasets.iitbhgc.IITBHGCDataset","title":"<code>IITBHGCDataset</code>","text":"<p>               Bases: <code>ETDataset</code></p> <p>A PyTorch dataset for eye movement features.</p> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <code>Args</code> <p>The configuration object.</p> required <code>text_data</code> <code>TextDataSet</code> <p>The text data.</p> <code>None</code> <code>ia_scaler</code> <code>Union[MinMaxScaler, RobustScaler, StandardScaler]</code> <p>Scaler for IA features.</p> required <code>fixation_scaler</code> <code>Union[MinMaxScaler, RobustScaler, StandardScaler, None]</code> <p>Scaler for fixation features.</p> required <code>trial_features_scaler</code> <code>Union[MinMaxScaler, RobustScaler, StandardScaler, None]</code> <p>The scaler for the trial features.</p> required <code>regime_name</code> <code>str</code> <p>The regime name. Defaults to \"\".</p> required <code>set_name</code> <code>str</code> <p>The set name. Defaults to \"\".</p> required Source code in <code>src/data/datasets/iitbhgc.py</code> <pre><code>class IITBHGCDataset(ETDataset):\n    \"\"\"\n    A PyTorch dataset for eye movement features.\n\n    Args:\n        cfg (Args): The configuration object.\n        text_data (TextDataSet): The text data.\n        ia_scaler (Union[MinMaxScaler, RobustScaler, StandardScaler]): Scaler for IA features.\n        fixation_scaler (Union[MinMaxScaler, RobustScaler, StandardScaler, None]):\n            Scaler for fixation features.\n        trial_features_scaler (Union[MinMaxScaler, RobustScaler, StandardScaler, None]):\n            The scaler for the trial features.\n        regime_name (str, optional): The regime name. Defaults to \"\".\n        set_name (str, optional): The set name. Defaults to \"\".\n    \"\"\"\n\n    def __init__(\n        self,\n        cfg: Args,\n        ia_scaler: MinMaxScaler | RobustScaler | StandardScaler | None,\n        fixation_scaler: MinMaxScaler | RobustScaler | StandardScaler | None,\n        trial_features_scaler: MinMaxScaler | RobustScaler | StandardScaler | None,\n        regime_name: SetNames,\n        set_name: SetNames,\n        text_data: TextDataSet | None = None,\n    ):\n        super().__init__(\n            cfg=cfg,\n            set_name=set_name,\n            regime_name=regime_name,\n            ia_scaler=ia_scaler,\n            fixation_scaler=fixation_scaler,\n            trial_features_scaler=trial_features_scaler,\n            text_data=text_data,\n        )\n\n    def extract_trial_level_features(self) -&gt; dict[str, torch.Tensor]:\n        trial_level_features_list = []\n        trial_level_features = self.trial_level_features.copy()\n        trial_level_features = trial_level_features.drop(\n            columns=self.ia_categorical_features,\n            errors='ignore',\n        )\n\n        for grouped_data_key in tqdm(\n            self.ordered_key_list, desc='Trial level features'\n        ):\n            try:\n                trial_features = trial_level_features.loc[grouped_data_key]\n            except KeyError:\n                e1, e2, e3, e4 = (\n                    grouped_data_key[0],\n                    grouped_data_key[1],\n                    grouped_data_key[2],\n                    grouped_data_key[3],\n                )\n                trial_features = trial_level_features.loc[(e2, e3, e4, e1)]\n\n            trial_features = ETDataset.normalize_features(\n                trial_features,\n                normalize=self.normalize,\n                scaler=self.trial_features_scaler,\n            )\n            trial_level_features_list.append(trial_features)\n\n        return {\n            'trial_level_features': torch.tensor(\n                np.array(trial_level_features_list),\n                dtype=torch.float32,\n            )\n        }\n</code></pre>"},{"location":"reference/data/datasets/mecol2/","title":"mecol2","text":"<p>Data module for creating the data.</p>"},{"location":"reference/data/datasets/mecol2/#data.datasets.mecol2.MECOL2Dataset","title":"<code>MECOL2Dataset</code>","text":"<p>               Bases: <code>ETDataset</code></p> <p>A PyTorch dataset for eye movement features.</p> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <code>Args</code> <p>The configuration object.</p> required <code>text_data</code> <code>TextDataSet</code> <p>The text data.</p> <code>None</code> <code>ia_scaler</code> <code>Union[MinMaxScaler, RobustScaler, StandardScaler]</code> <p>Scaler for IA features.</p> required <code>fixation_scaler</code> <code>Union[MinMaxScaler, RobustScaler, StandardScaler, None]</code> <p>Scaler for fixation features.</p> required <code>trial_features_scaler</code> <code>Union[MinMaxScaler, RobustScaler, StandardScaler, None]</code> <p>The scaler for the trial features.</p> required <code>regime_name</code> <code>str</code> <p>The regime name. Defaults to \"\".</p> required <code>set_name</code> <code>str</code> <p>The set name. Defaults to \"\".</p> required Source code in <code>src/data/datasets/mecol2.py</code> <pre><code>class MECOL2Dataset(ETDataset):\n    \"\"\"\n    A PyTorch dataset for eye movement features.\n\n    Args:\n        cfg (Args): The configuration object.\n        text_data (TextDataSet): The text data.\n        ia_scaler (Union[MinMaxScaler, RobustScaler, StandardScaler]): Scaler for IA features.\n        fixation_scaler (Union[MinMaxScaler, RobustScaler, StandardScaler, None]):\n            Scaler for fixation features.\n        trial_features_scaler (Union[MinMaxScaler, RobustScaler, StandardScaler, None]):\n            The scaler for the trial features.\n        regime_name (str, optional): The regime name. Defaults to \"\".\n        set_name (str, optional): The set name. Defaults to \"\".\n    \"\"\"\n\n    def __init__(\n        self,\n        cfg: Args,\n        ia_scaler: MinMaxScaler | RobustScaler | StandardScaler | None,\n        fixation_scaler: MinMaxScaler | RobustScaler | StandardScaler | None,\n        trial_features_scaler: MinMaxScaler | RobustScaler | StandardScaler | None,\n        regime_name: SetNames,\n        set_name: SetNames,\n        text_data: TextDataSet | None = None,\n    ):\n        super().__init__(\n            cfg=cfg,\n            set_name=set_name,\n            regime_name=regime_name,\n            ia_scaler=ia_scaler,\n            fixation_scaler=fixation_scaler,\n            trial_features_scaler=trial_features_scaler,\n            text_data=text_data,\n        )\n\n    def extract_trial_level_features(self) -&gt; dict[str, torch.Tensor]:\n        trial_level_features_list = []\n        trial_level_features = self.trial_level_features.copy()\n        trial_level_features = trial_level_features.drop(\n            columns=self.ia_categorical_features,\n            errors='ignore',\n        )\n\n        for grouped_data_key in tqdm(\n            self.ordered_key_list, desc='Trial level features'\n        ):\n            try:\n                trial_features = trial_level_features.loc[grouped_data_key]\n            except:  # noqa: E722\n                e1, e2, e3, e4 = grouped_data_key\n                trial_features = trial_level_features.loc[(e4, e1, e2, e3)]\n\n            trial_features = ETDataset.normalize_features(\n                trial_features,\n                normalize=self.normalize,\n                scaler=self.trial_features_scaler,\n            )\n            trial_level_features_list.append(trial_features)\n\n        return {\n            'trial_level_features': torch.tensor(\n                np.array(trial_level_features_list),\n                dtype=torch.float32,\n            )\n        }\n</code></pre>"},{"location":"reference/data/datasets/onestop/","title":"onestop","text":"<p>Data module for creating the data.</p>"},{"location":"reference/data/datasets/onestop/#data.datasets.onestop.OneStopDataset","title":"<code>OneStopDataset</code>","text":"<p>               Bases: <code>ETDataset</code></p> <p>Dataset for OneStop; inherits ETDataset and doesn't require a custom init.</p> Source code in <code>src/data/datasets/onestop.py</code> <pre><code>class OneStopDataset(ETDataset):\n    \"\"\"Dataset for OneStop; inherits ETDataset and doesn't require a custom __init__.\"\"\"\n</code></pre>"},{"location":"reference/data/datasets/potec/","title":"potec","text":"<p>Data module for creating the data.</p>"},{"location":"reference/data/datasets/potec/#data.datasets.potec.PoTeCDataset","title":"<code>PoTeCDataset</code>","text":"<p>               Bases: <code>ETDataset</code></p> <p>A PyTorch dataset for eye movement features.</p> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <code>Args</code> <p>The configuration object.</p> required <code>text_data</code> <code>TextDataSet</code> <p>The text data.</p> <code>None</code> <code>ia_scaler</code> <code>Union[MinMaxScaler, RobustScaler, StandardScaler]</code> <p>Scaler for IA features.</p> required <code>fixation_scaler</code> <code>Union[MinMaxScaler, RobustScaler, StandardScaler, None]</code> <p>Scaler for fixation features.</p> required <code>trial_features_scaler</code> <code>Union[MinMaxScaler, RobustScaler, StandardScaler, None]</code> <p>The scaler for the trial features.</p> required <code>regime_name</code> <code>str</code> <p>The regime name. Defaults to \"\".</p> required <code>set_name</code> <code>str</code> <p>The set name. Defaults to \"\".</p> required Source code in <code>src/data/datasets/potec.py</code> <pre><code>class PoTeCDataset(ETDataset):\n    \"\"\"\n    A PyTorch dataset for eye movement features.\n\n    Args:\n        cfg (Args): The configuration object.\n        text_data (TextDataSet): The text data.\n        ia_scaler (Union[MinMaxScaler, RobustScaler, StandardScaler]): Scaler for IA features.\n        fixation_scaler (Union[MinMaxScaler, RobustScaler, StandardScaler, None]):\n            Scaler for fixation features.\n        trial_features_scaler (Union[MinMaxScaler, RobustScaler, StandardScaler, None]):\n            The scaler for the trial features.\n        regime_name (str, optional): The regime name. Defaults to \"\".\n        set_name (str, optional): The set name. Defaults to \"\".\n    \"\"\"\n\n    def __init__(\n        self,\n        cfg: Args,\n        ia_scaler: MinMaxScaler | RobustScaler | StandardScaler | None,\n        fixation_scaler: MinMaxScaler | RobustScaler | StandardScaler | None,\n        trial_features_scaler: MinMaxScaler | RobustScaler | StandardScaler | None,\n        regime_name: SetNames,\n        set_name: SetNames,\n        text_data: TextDataSet | None = None,\n    ):\n        cfg.data.groupby_columns = list(\n            dict.fromkeys(cfg.data.groupby_columns)\n        )  # TODO: Is this needed??\n\n        super().__init__(\n            cfg=cfg,\n            set_name=set_name,\n            regime_name=regime_name,\n            ia_scaler=ia_scaler,\n            fixation_scaler=fixation_scaler,\n            trial_features_scaler=trial_features_scaler,\n            text_data=text_data,\n        )\n\n    def extract_trial_level_features(self) -&gt; dict[str, torch.Tensor]:\n        trial_level_features_list = []\n        trial_level_features = self.trial_level_features.copy()\n        trial_level_features = trial_level_features.drop(\n            columns=self.ia_categorical_features,\n            errors='ignore',\n        )\n\n        for grouped_data_key in tqdm(\n            self.ordered_key_list, desc='Trial level features'\n        ):\n            try:\n                trial_features = trial_level_features.loc[grouped_data_key]  # type: ignore\n            except KeyError:\n                e1, e2, e3, e4, e5, e6, e7 = grouped_data_key\n                trial_features = trial_level_features.loc[(e1, e2, e3, e4, e5, e7, e6)]  # type: ignore\n\n            trial_features = ETDataset.normalize_features(\n                trial_features,\n                normalize=self.normalize,\n                scaler=self.trial_features_scaler,\n            )\n            trial_level_features_list.append(trial_features)\n\n        return {\n            'trial_level_features': torch.tensor(\n                np.array(trial_level_features_list),\n                dtype=torch.float32,\n            )\n        }\n</code></pre>"},{"location":"reference/data/datasets/sbsat/","title":"sbsat","text":"<p>Data module for creating the data.</p>"},{"location":"reference/data/datasets/sbsat/#data.datasets.sbsat.SBSATDataset","title":"<code>SBSATDataset</code>","text":"<p>               Bases: <code>ETDataset</code></p> <p>A PyTorch dataset for eye movement features.</p> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <code>Args</code> <p>The configuration object.</p> required <code>text_data</code> <code>TextDataSet</code> <p>The text data.</p> <code>None</code> <code>ia_scaler</code> <code>Union[MinMaxScaler, RobustScaler, StandardScaler]</code> <p>Scaler for IA features.</p> required <code>fixation_scaler</code> <code>Union[MinMaxScaler, RobustScaler, StandardScaler, None]</code> <p>Scaler for fixation features.</p> required <code>trial_features_scaler</code> <code>Union[MinMaxScaler, RobustScaler, StandardScaler, None]</code> <p>The scaler for the trial features.</p> required <code>regime_name</code> <code>str</code> <p>The regime name. Defaults to \"\".</p> required <code>set_name</code> <code>str</code> <p>The set name. Defaults to \"\".</p> required Source code in <code>src/data/datasets/sbsat.py</code> <pre><code>class SBSATDataset(ETDataset):\n    \"\"\"\n    A PyTorch dataset for eye movement features.\n\n    Args:\n        cfg (Args): The configuration object.\n        text_data (TextDataSet): The text data.\n        ia_scaler (Union[MinMaxScaler, RobustScaler, StandardScaler]): Scaler for IA features.\n        fixation_scaler (Union[MinMaxScaler, RobustScaler, StandardScaler, None]):\n            Scaler for fixation features.\n        trial_features_scaler (Union[MinMaxScaler, RobustScaler, StandardScaler, None]):\n            The scaler for the trial features.\n        regime_name (str, optional): The regime name. Defaults to \"\".\n        set_name (str, optional): The set name. Defaults to \"\".\n    \"\"\"\n\n    def __init__(\n        self,\n        cfg: Args,\n        ia_scaler: MinMaxScaler | RobustScaler | StandardScaler | None,\n        fixation_scaler: MinMaxScaler | RobustScaler | StandardScaler | None,\n        trial_features_scaler: MinMaxScaler | RobustScaler | StandardScaler | None,\n        regime_name: SetNames,\n        set_name: SetNames,\n        text_data: TextDataSet | None = None,\n    ):\n        super().__init__(\n            cfg=cfg,\n            set_name=set_name,\n            regime_name=regime_name,\n            ia_scaler=ia_scaler,\n            fixation_scaler=fixation_scaler,\n            trial_features_scaler=trial_features_scaler,\n            text_data=text_data,\n        )\n\n    def extract_trial_level_features(self) -&gt; dict[str, torch.Tensor]:\n        trial_level_features_list = []\n        trial_level_features = self.trial_level_features.copy()\n        trial_level_features = trial_level_features.drop(\n            columns=self.ia_categorical_features,\n            errors='ignore',\n        )\n\n        for grouped_data_key in tqdm(\n            self.ordered_key_list, desc='Trial level features'\n        ):\n            try:\n                trial_features = trial_level_features.loc[grouped_data_key]\n            except KeyError:\n                e1, e2, e3, e4, e5, e6 = (\n                    grouped_data_key[0],\n                    grouped_data_key[1],\n                    grouped_data_key[2],\n                    grouped_data_key[3],\n                    grouped_data_key[4],\n                    grouped_data_key[5],\n                )  # e1: unique item, e2: participant id, e3: unique trial id, e4: difficulty, e5: binary difficulty, e6: RC\n                trial_features = trial_level_features.loc[(e1, e2, e3, e6, e4, e5)]\n\n            trial_features = ETDataset.normalize_features(\n                trial_features,\n                normalize=self.normalize,\n                scaler=self.trial_features_scaler,\n            )\n            trial_level_features_list.append(trial_features)\n\n        return {\n            'trial_level_features': torch.tensor(\n                np.array(trial_level_features_list),\n                dtype=torch.float32,\n            )\n        }\n</code></pre>"},{"location":"reference/data/preprocessing/__init__/","title":"init","text":""},{"location":"reference/data/preprocessing/create_folds/","title":"create_folds","text":"<p>This script is responsible for splitting datasets into training, validation, and test sets.</p>"},{"location":"reference/data/preprocessing/create_folds/#data.preprocessing.create_folds.FoldSplitter","title":"<code>FoldSplitter</code>","text":"<p>A class used to split data into folds.</p> <p>Attributes:</p> Name Type Description <code>item_columns</code> <code>list[str]</code> <p>The columns that contain the item identifiers.</p> <code>subject_column</code> <code>str</code> <p>The column that contains the subject identifiers.</p> <code>groupby_columns</code> <code>list[str]</code> <p>The columns used to group the trials.</p> <code>num_folds</code> <code>int</code> <p>The number of folds to split the data into.</p> <code>stratify</code> <code>str | None</code> <p>The column that contains the target values.</p> <code>folds_path</code> <code>Path</code> <p>The path where the fold CSVs are stored.</p> Source code in <code>src/data/preprocessing/create_folds.py</code> <pre><code>class FoldSplitter:\n    \"\"\"\n    A class used to split data into folds.\n\n    Attributes:\n        item_columns (list[str]): The columns that contain the item identifiers.\n        subject_column (str): The column that contains the subject identifiers.\n        groupby_columns (list[str]): The columns used to group the trials.\n        num_folds (int): The number of folds to split the data into.\n        stratify (str | None): The column that contains the target values.\n        folds_path (Path): The path where the fold CSVs are stored.\n    \"\"\"\n\n    def __init__(\n        self,\n        item_columns: list[str],\n        subject_column: str,\n        groupby_columns: list[str],\n        num_folds: int,\n        stratify: str | None,\n        folds_path: Path = Path('data') / 'folds',\n        higher_level_split_column: str | None = None,\n    ) -&gt; None:\n        \"\"\"\n        Initialize the FoldSplitter.\n\n        Parameters:\n            item_columns (list[str]): The columns that contain the item identifiers.\n            subject_column (str): The column that contains the subject identifiers.\n            groupby_columns (list[str]): The columns used to group the trials.\n            higher_level_split_column (str): The column used for higher level splitting.\n            num_folds (int): The number of folds to split the data into.\n            stratify (str): The column that contains the target values.\n            folds_path (Path, optional): The path where the fold CSVs are stored.\n        \"\"\"\n        self.item_columns = item_columns\n        self.subject_column = subject_column\n        self.groupby_columns = groupby_columns\n        self.higher_level_split_column = higher_level_split_column\n        self.num_folds = num_folds\n        self.stratify = stratify\n        self.folds_path = folds_path\n        self.item_folds = {}\n        self.subject_folds = {}\n\n    def get_split_indices(\n        self, group_keys: pd.DataFrame, split_indices: pd.Series, is_item: bool\n    ) -&gt; pd.Index:\n        \"\"\"\n        Get the indices from group keys based on item or subject split.\n\n        Parameters:\n            group_keys (pd.DataFrame): DataFrame containing grouping keys.\n            split_indices (pd.Series): Series of split indices.\n            is_item (bool): Whether the split is for item identifiers.\n\n        Returns:\n            pd.Index: The indices from group_keys that match the split.\n        \"\"\"\n        if is_item:\n            column = group_keys[self.item_columns].astype(str).apply('_'.join, axis=1)\n        else:\n            column = group_keys[self.subject_column]\n\n        return group_keys.loc[column.isin(split_indices)].index\n\n    def load_folds(self) -&gt; tuple[list[pd.DataFrame], list[pd.DataFrame]]:\n        \"\"\"\n        Load folds from the CSV files for subjects and items.\n\n        Returns:\n            tuple[list[pd.DataFrame], list[pd.DataFrame]]: A tuple containing the subject folds\n                and item folds as DataFrames.\n        \"\"\"\n        try:\n            folds_path = Path(HydraConfig.get().runtime.output_dir) / 'folds'\n        except Exception:  # no hydra\n            logger.info(\n                f'HydraConfig not found. Using default path. Loading folds from {self.folds_path}'\n            )\n            folds_path = self.folds_path\n\n        subject_folds_path = folds_path / 'subjects'\n        item_folds_path = folds_path / 'items'\n        # load all folds\n\n        for i in range(self.num_folds):\n            subject_fold_path = subject_folds_path / f'fold_{i}.csv'\n            item_fold_path = item_folds_path / f'fold_{i}.csv'\n            self.subject_folds[i] = pd.read_csv(subject_fold_path, header=None)\n            self.item_folds[i] = pd.read_csv(item_fold_path, header=None)\n        return self.subject_folds, self.item_folds\n\n    def get_fold_indices(\n        self,\n        i: int,\n    ) -&gt; tuple[list[int], list[int], list[int]]:\n        \"\"\"\n        Get fold indices for test, validation, and training sets based on the given fold index.\n\n        Parameters:\n            i (int): The fold index (should be between 0 and num_folds - 1).\n\n        Returns:\n            tuple[list[int], list[int], list[int]]: A tuple containing the test indices,\n                validation indices, and training indices.\n        \"\"\"\n        if i &lt; 0 or i &gt; self.num_folds - 1:\n            raise ValueError('Fold index must be within the range [0, num_folds - 1].')\n\n        validation_indices = [i]\n        # modulo num_folds for the wraparound\n        test_indices = [(i + 1) % self.num_folds]\n\n        # The rest are training indices\n        train_indices = [\n            x\n            for x in range(self.num_folds)\n            if x not in test_indices and x not in validation_indices\n        ]\n        logger.info(\n            f'Test folds: {test_indices}, Validation fold: {validation_indices}, Train folds: {train_indices}'\n        )\n        return test_indices, validation_indices, train_indices\n\n    def create_default_folds(self, group_keys: pd.DataFrame) -&gt; None:\n        \"\"\"\n        Create default folds based on group keys and save them as CSV files.\n\n        Parameters:\n            group_keys (pd.DataFrame): DataFrame containing the group keys.\n        \"\"\"\n        all_folds_subjects = []\n        all_folds_items = []\n        n_splits = self.num_folds\n\n        def _split_and_collect(batch_data: pd.DataFrame, split_ind: int):\n            subjects = batch_data[self.subject_column]\n            items = batch_data[self.item_columns].astype(str).apply('_'.join, axis=1)\n            if self.stratify:\n                splitter = StratifiedGroupKFold(n_splits=n_splits)\n                y = batch_data[self.stratify]\n            else:\n                splitter = GroupKFold(n_splits=n_splits)\n                y = None\n\n            _, test_subjects_indx = list(\n                splitter.split(subjects, y=y, groups=subjects)\n            )[split_ind]\n            _, test_items_indx = list(splitter.split(items, y=y, groups=items))[\n                split_ind\n            ]\n\n            return subjects.iloc[test_subjects_indx].tolist(), items.iloc[\n                test_items_indx\n            ].tolist()\n\n        for split_ind in range(n_splits):\n            fold_subjects = []\n            fold_items = []\n\n            if self.higher_level_split_column:\n                for i_split in group_keys[self.higher_level_split_column].unique():\n                    batch_data = group_keys[\n                        group_keys[self.higher_level_split_column] == i_split\n                    ].reset_index(drop=True)\n                    batch_subjects, batch_items = _split_and_collect(\n                        batch_data, split_ind\n                    )\n                    fold_subjects.extend(batch_subjects)\n                    fold_items.extend(batch_items)\n            else:\n                batch_subjects, batch_items = _split_and_collect(\n                    group_keys.reset_index(drop=True), split_ind\n                )\n                fold_subjects.extend(batch_subjects)\n                fold_items.extend(batch_items)\n\n            all_folds_subjects.append(fold_subjects)\n            all_folds_items.append(fold_items)\n\n        try:\n            folds_path = Path(HydraConfig.get().runtime.output_dir) / 'folds'\n        except Exception:  # no hydra\n            logger.info(\n                f'HydraConfig not found. Using default path. Loading folds from {self.folds_path}'\n            )\n            folds_path = self.folds_path\n        subject_folds_path = folds_path / 'subjects'\n        item_folds_path = folds_path / 'items'\n\n        for i, (subject_fold, item_fold) in enumerate(\n            zip(all_folds_subjects, all_folds_items)\n        ):\n            item_folds_path.mkdir(parents=True, exist_ok=True)\n            subject_folds_path.mkdir(parents=True, exist_ok=True)\n            subject_df = pd.DataFrame(sorted(list(set(subject_fold))))\n            self.subject_folds[i] = subject_df\n            subject_df.to_csv(\n                subject_folds_path / f'fold_{i}.csv', header=False, index=False\n            )\n            item_df = pd.DataFrame(sorted(list(set(item_fold))))\n            self.item_folds[i] = item_df\n            item_df.to_csv(item_folds_path / f'fold_{i}.csv', header=False, index=False)\n\n    @staticmethod\n    def get_combined_indices(fold_dict: dict, folds_indices: list[int]) -&gt; pd.Series:\n        \"\"\"\n        Combine fold indices from a fold dictionary based on specified fold indices.\n\n        Parameters:\n            fold_dict (dict): Dictionary containing folds.\n            folds_indices (list[int]): List of fold indices to combine.\n\n        Returns:\n            pd.Series: Combined fold indices as a Series.\n        \"\"\"\n        return pd.concat(\n            [fold_dict[i] for i in folds_indices], ignore_index=True\n        ).squeeze('columns')\n\n    def get_train_val_test_splits(\n        self,\n        group_keys: pd.DataFrame,\n        fold_index: int,\n    ) -&gt; tuple[pd.DataFrame, list[pd.DataFrame], list[pd.DataFrame]]:\n        \"\"\"\n        Split the data into training, validation, and test sets.\n\n        Parameters:\n            group_keys (pd.DataFrame): DataFrame containing group keys.\n            fold_index (int): The fold index to use for splitting.\n\n        Returns:\n            tuple[pd.DataFrame, list[pd.DataFrame], list[pd.DataFrame]]:\n            A tuple containing the training keys, a list of validation keys, and a list of test keys.\n        \"\"\"\n        test_indices, val_indices, train_indices = self.get_fold_indices(fold_index)\n        subject_folds = self.subject_folds\n        item_folds = self.item_folds\n\n        # Get subject and item IDs for each split into train/val/test\n        subject_train_indices = FoldSplitter.get_combined_indices(\n            subject_folds, train_indices\n        )\n        subject_val_indices = FoldSplitter.get_combined_indices(\n            subject_folds, val_indices\n        )\n        subject_test_indices = FoldSplitter.get_combined_indices(\n            subject_folds, test_indices\n        )\n        item_train_indices = FoldSplitter.get_combined_indices(\n            item_folds, train_indices\n        )\n        item_val_indices = FoldSplitter.get_combined_indices(item_folds, val_indices)\n        item_test_indices = FoldSplitter.get_combined_indices(item_folds, test_indices)\n\n        # Get trial-level indices in group_keys per split\n        train_subjects_indx = self.get_split_indices(\n            group_keys, subject_train_indices, is_item=False\n        )\n        val_subjects_indx = self.get_split_indices(\n            group_keys, subject_val_indices, is_item=False\n        )\n        test_subjects_indx = self.get_split_indices(\n            group_keys, subject_test_indices, is_item=False\n        )\n        train_items_indx = self.get_split_indices(\n            group_keys, item_train_indices, is_item=True\n        )\n        val_items_indx = self.get_split_indices(\n            group_keys, item_val_indices, is_item=True\n        )\n        test_items_indx = self.get_split_indices(\n            group_keys, item_test_indices, is_item=True\n        )\n\n        train_indices = np.array(train_subjects_indx.intersection(train_items_indx))\n\n        seen_subject_unseen_item_test_indices = np.array(\n            test_items_indx.intersection(train_subjects_indx.union(val_subjects_indx))\n        )\n        unseen_subject_seen_item_test_indices = np.array(\n            train_items_indx.union(val_items_indx).intersection(test_subjects_indx)\n        )\n        unseen_subject_unseen_item_test_indices = np.array(\n            test_items_indx.intersection(test_subjects_indx)\n        )\n\n        unseen_subject_unseen_item_val_indices = np.array(\n            val_subjects_indx.intersection(val_items_indx)\n        )\n        unseen_subject_seen_item_val_indices = np.array(\n            val_subjects_indx.intersection(train_items_indx)\n        )\n        seen_subject_unseen_item_val_indices = np.array(\n            train_subjects_indx.intersection(val_items_indx)\n        )\n\n        assert len(group_keys) == len(train_indices) + len(\n            seen_subject_unseen_item_test_indices\n        ) + len(unseen_subject_seen_item_test_indices) + len(\n            unseen_subject_unseen_item_test_indices\n        ) + len(unseen_subject_unseen_item_val_indices) + len(\n            unseen_subject_seen_item_val_indices\n        ) + len(seen_subject_unseen_item_val_indices), (\n            'Data subsets do not sum to all the data'\n        )\n\n        self.assert_no_duplicates(train_indices, 'train_indices')\n        train_keys = group_keys.iloc[train_indices]\n        train_keys.attrs['name'] = SetNames.TRAIN\n        train_keys.attrs['set_name'] = SetNames.TRAIN\n\n        test_key_types = [\n            (SetNames.SEEN_SUBJECT_UNSEEN_ITEM, seen_subject_unseen_item_test_indices),\n            (SetNames.UNSEEN_SUBJECT_SEEN_ITEM, unseen_subject_seen_item_test_indices),\n            (\n                SetNames.UNSEEN_SUBJECT_UNSEEN_ITEM,\n                unseen_subject_unseen_item_test_indices,\n            ),\n        ]\n        test_keys_list = []\n        for key_name, indices in test_key_types:\n            self.assert_no_duplicates(indices, key_name)\n            keys = group_keys.iloc[indices]\n            keys.attrs['name'] = key_name\n            keys.attrs['set_name'] = SetNames.TEST\n            test_keys_list.append(keys)\n\n        val_keys_list = []\n        val_key_types = [\n            (SetNames.SEEN_SUBJECT_UNSEEN_ITEM, seen_subject_unseen_item_val_indices),\n            (SetNames.UNSEEN_SUBJECT_SEEN_ITEM, unseen_subject_seen_item_val_indices),\n            (\n                SetNames.UNSEEN_SUBJECT_UNSEEN_ITEM,\n                unseen_subject_unseen_item_val_indices,\n            ),\n        ]\n        for key_name, indices in val_key_types:\n            self.assert_no_duplicates(indices, key_name)\n            keys = group_keys.iloc[indices]\n            keys.attrs['name'] = key_name\n            keys.attrs['set_name'] = SetNames.VAL\n            val_keys_list.append(keys)\n\n        self.print_group_info('Train', train_keys)\n        for keys in val_keys_list:\n            self.print_group_info(f'Val {keys.attrs[\"name\"]}', keys)\n        for keys in test_keys_list:\n            self.print_group_info(f'Test {keys.attrs[\"name\"]}', keys)\n\n        all_keys = pd.concat([train_keys] + val_keys_list + test_keys_list).sort_index()\n        self.print_group_info('All', all_keys)\n\n        return train_keys, val_keys_list, test_keys_list\n\n    @staticmethod\n    def print_group_info(name: str, keys: pd.DataFrame) -&gt; None:\n        \"\"\"\n        Print group information.\n\n        Parameters:\n            name (str): The name of the group.\n            keys (pd.DataFrame): DataFrame containing the group keys.\n        \"\"\"\n        logger.info(\n            f'{name}: # Trials: {len(keys)}. '\n            f'# Items: {keys[Fields.UNIQUE_PARAGRAPH_ID].nunique()}; '\n            f'# Subjects: {keys[Fields.SUBJECT_ID].nunique()}'\n        )\n\n    @staticmethod\n    def assert_no_duplicates(indices: list, indices_name: str) -&gt; None:\n        \"\"\"\n        Assert that there are no duplicate indices.\n\n        Parameters:\n            indices: The indices to check.\n            indices_name: Name of the indices for error reporting.\n\n        Raises:\n            AssertionError: If duplicates are found in the indices.\n        \"\"\"\n        assert len(indices) == len(set(indices)), indices_name + ' contains duplicates'\n\n    def create_trial_folds(\n        self, group_keys: pd.DataFrame, eval_regime_names, trial_ids_folder: Path\n    ) -&gt; None:\n        # Create and save evaluation regimes for each fold\n        for fold_index in range(self.num_folds):\n            train_keys, val_keys_list, test_keys_list = self.get_train_val_test_splits(\n                group_keys=group_keys,\n                fold_index=fold_index,\n            )\n            eval_regimes = [train_keys] + val_keys_list + test_keys_list\n\n            # Save all regimes to a single CSV file for this fold\n            regimes_csv_path = (\n                trial_ids_folder / f'fold_{fold_index}_trial_ids_by_regime.csv'\n            )\n            save_eval_regimes_to_csv(eval_regimes, eval_regime_names, regimes_csv_path)\n            logger.info(f'Saved evaluation regimes to {regimes_csv_path}')\n</code></pre>"},{"location":"reference/data/preprocessing/create_folds/#data.preprocessing.create_folds.FoldSplitter.__init__","title":"<code>__init__(item_columns, subject_column, groupby_columns, num_folds, stratify, folds_path=Path('data') / 'folds', higher_level_split_column=None)</code>","text":"<p>Initialize the FoldSplitter.</p> <p>Parameters:</p> Name Type Description Default <code>item_columns</code> <code>list[str]</code> <p>The columns that contain the item identifiers.</p> required <code>subject_column</code> <code>str</code> <p>The column that contains the subject identifiers.</p> required <code>groupby_columns</code> <code>list[str]</code> <p>The columns used to group the trials.</p> required <code>higher_level_split_column</code> <code>str</code> <p>The column used for higher level splitting.</p> <code>None</code> <code>num_folds</code> <code>int</code> <p>The number of folds to split the data into.</p> required <code>stratify</code> <code>str</code> <p>The column that contains the target values.</p> required <code>folds_path</code> <code>Path</code> <p>The path where the fold CSVs are stored.</p> <code>Path('data') / 'folds'</code> Source code in <code>src/data/preprocessing/create_folds.py</code> <pre><code>def __init__(\n    self,\n    item_columns: list[str],\n    subject_column: str,\n    groupby_columns: list[str],\n    num_folds: int,\n    stratify: str | None,\n    folds_path: Path = Path('data') / 'folds',\n    higher_level_split_column: str | None = None,\n) -&gt; None:\n    \"\"\"\n    Initialize the FoldSplitter.\n\n    Parameters:\n        item_columns (list[str]): The columns that contain the item identifiers.\n        subject_column (str): The column that contains the subject identifiers.\n        groupby_columns (list[str]): The columns used to group the trials.\n        higher_level_split_column (str): The column used for higher level splitting.\n        num_folds (int): The number of folds to split the data into.\n        stratify (str): The column that contains the target values.\n        folds_path (Path, optional): The path where the fold CSVs are stored.\n    \"\"\"\n    self.item_columns = item_columns\n    self.subject_column = subject_column\n    self.groupby_columns = groupby_columns\n    self.higher_level_split_column = higher_level_split_column\n    self.num_folds = num_folds\n    self.stratify = stratify\n    self.folds_path = folds_path\n    self.item_folds = {}\n    self.subject_folds = {}\n</code></pre>"},{"location":"reference/data/preprocessing/create_folds/#data.preprocessing.create_folds.FoldSplitter.assert_no_duplicates","title":"<code>assert_no_duplicates(indices, indices_name)</code>  <code>staticmethod</code>","text":"<p>Assert that there are no duplicate indices.</p> <p>Parameters:</p> Name Type Description Default <code>indices</code> <code>list</code> <p>The indices to check.</p> required <code>indices_name</code> <code>str</code> <p>Name of the indices for error reporting.</p> required <p>Raises:</p> Type Description <code>AssertionError</code> <p>If duplicates are found in the indices.</p> Source code in <code>src/data/preprocessing/create_folds.py</code> <pre><code>@staticmethod\ndef assert_no_duplicates(indices: list, indices_name: str) -&gt; None:\n    \"\"\"\n    Assert that there are no duplicate indices.\n\n    Parameters:\n        indices: The indices to check.\n        indices_name: Name of the indices for error reporting.\n\n    Raises:\n        AssertionError: If duplicates are found in the indices.\n    \"\"\"\n    assert len(indices) == len(set(indices)), indices_name + ' contains duplicates'\n</code></pre>"},{"location":"reference/data/preprocessing/create_folds/#data.preprocessing.create_folds.FoldSplitter.create_default_folds","title":"<code>create_default_folds(group_keys)</code>","text":"<p>Create default folds based on group keys and save them as CSV files.</p> <p>Parameters:</p> Name Type Description Default <code>group_keys</code> <code>DataFrame</code> <p>DataFrame containing the group keys.</p> required Source code in <code>src/data/preprocessing/create_folds.py</code> <pre><code>def create_default_folds(self, group_keys: pd.DataFrame) -&gt; None:\n    \"\"\"\n    Create default folds based on group keys and save them as CSV files.\n\n    Parameters:\n        group_keys (pd.DataFrame): DataFrame containing the group keys.\n    \"\"\"\n    all_folds_subjects = []\n    all_folds_items = []\n    n_splits = self.num_folds\n\n    def _split_and_collect(batch_data: pd.DataFrame, split_ind: int):\n        subjects = batch_data[self.subject_column]\n        items = batch_data[self.item_columns].astype(str).apply('_'.join, axis=1)\n        if self.stratify:\n            splitter = StratifiedGroupKFold(n_splits=n_splits)\n            y = batch_data[self.stratify]\n        else:\n            splitter = GroupKFold(n_splits=n_splits)\n            y = None\n\n        _, test_subjects_indx = list(\n            splitter.split(subjects, y=y, groups=subjects)\n        )[split_ind]\n        _, test_items_indx = list(splitter.split(items, y=y, groups=items))[\n            split_ind\n        ]\n\n        return subjects.iloc[test_subjects_indx].tolist(), items.iloc[\n            test_items_indx\n        ].tolist()\n\n    for split_ind in range(n_splits):\n        fold_subjects = []\n        fold_items = []\n\n        if self.higher_level_split_column:\n            for i_split in group_keys[self.higher_level_split_column].unique():\n                batch_data = group_keys[\n                    group_keys[self.higher_level_split_column] == i_split\n                ].reset_index(drop=True)\n                batch_subjects, batch_items = _split_and_collect(\n                    batch_data, split_ind\n                )\n                fold_subjects.extend(batch_subjects)\n                fold_items.extend(batch_items)\n        else:\n            batch_subjects, batch_items = _split_and_collect(\n                group_keys.reset_index(drop=True), split_ind\n            )\n            fold_subjects.extend(batch_subjects)\n            fold_items.extend(batch_items)\n\n        all_folds_subjects.append(fold_subjects)\n        all_folds_items.append(fold_items)\n\n    try:\n        folds_path = Path(HydraConfig.get().runtime.output_dir) / 'folds'\n    except Exception:  # no hydra\n        logger.info(\n            f'HydraConfig not found. Using default path. Loading folds from {self.folds_path}'\n        )\n        folds_path = self.folds_path\n    subject_folds_path = folds_path / 'subjects'\n    item_folds_path = folds_path / 'items'\n\n    for i, (subject_fold, item_fold) in enumerate(\n        zip(all_folds_subjects, all_folds_items)\n    ):\n        item_folds_path.mkdir(parents=True, exist_ok=True)\n        subject_folds_path.mkdir(parents=True, exist_ok=True)\n        subject_df = pd.DataFrame(sorted(list(set(subject_fold))))\n        self.subject_folds[i] = subject_df\n        subject_df.to_csv(\n            subject_folds_path / f'fold_{i}.csv', header=False, index=False\n        )\n        item_df = pd.DataFrame(sorted(list(set(item_fold))))\n        self.item_folds[i] = item_df\n        item_df.to_csv(item_folds_path / f'fold_{i}.csv', header=False, index=False)\n</code></pre>"},{"location":"reference/data/preprocessing/create_folds/#data.preprocessing.create_folds.FoldSplitter.get_combined_indices","title":"<code>get_combined_indices(fold_dict, folds_indices)</code>  <code>staticmethod</code>","text":"<p>Combine fold indices from a fold dictionary based on specified fold indices.</p> <p>Parameters:</p> Name Type Description Default <code>fold_dict</code> <code>dict</code> <p>Dictionary containing folds.</p> required <code>folds_indices</code> <code>list[int]</code> <p>List of fold indices to combine.</p> required <p>Returns:</p> Type Description <code>Series</code> <p>pd.Series: Combined fold indices as a Series.</p> Source code in <code>src/data/preprocessing/create_folds.py</code> <pre><code>@staticmethod\ndef get_combined_indices(fold_dict: dict, folds_indices: list[int]) -&gt; pd.Series:\n    \"\"\"\n    Combine fold indices from a fold dictionary based on specified fold indices.\n\n    Parameters:\n        fold_dict (dict): Dictionary containing folds.\n        folds_indices (list[int]): List of fold indices to combine.\n\n    Returns:\n        pd.Series: Combined fold indices as a Series.\n    \"\"\"\n    return pd.concat(\n        [fold_dict[i] for i in folds_indices], ignore_index=True\n    ).squeeze('columns')\n</code></pre>"},{"location":"reference/data/preprocessing/create_folds/#data.preprocessing.create_folds.FoldSplitter.get_fold_indices","title":"<code>get_fold_indices(i)</code>","text":"<p>Get fold indices for test, validation, and training sets based on the given fold index.</p> <p>Parameters:</p> Name Type Description Default <code>i</code> <code>int</code> <p>The fold index (should be between 0 and num_folds - 1).</p> required <p>Returns:</p> Type Description <code>tuple[list[int], list[int], list[int]]</code> <p>tuple[list[int], list[int], list[int]]: A tuple containing the test indices, validation indices, and training indices.</p> Source code in <code>src/data/preprocessing/create_folds.py</code> <pre><code>def get_fold_indices(\n    self,\n    i: int,\n) -&gt; tuple[list[int], list[int], list[int]]:\n    \"\"\"\n    Get fold indices for test, validation, and training sets based on the given fold index.\n\n    Parameters:\n        i (int): The fold index (should be between 0 and num_folds - 1).\n\n    Returns:\n        tuple[list[int], list[int], list[int]]: A tuple containing the test indices,\n            validation indices, and training indices.\n    \"\"\"\n    if i &lt; 0 or i &gt; self.num_folds - 1:\n        raise ValueError('Fold index must be within the range [0, num_folds - 1].')\n\n    validation_indices = [i]\n    # modulo num_folds for the wraparound\n    test_indices = [(i + 1) % self.num_folds]\n\n    # The rest are training indices\n    train_indices = [\n        x\n        for x in range(self.num_folds)\n        if x not in test_indices and x not in validation_indices\n    ]\n    logger.info(\n        f'Test folds: {test_indices}, Validation fold: {validation_indices}, Train folds: {train_indices}'\n    )\n    return test_indices, validation_indices, train_indices\n</code></pre>"},{"location":"reference/data/preprocessing/create_folds/#data.preprocessing.create_folds.FoldSplitter.get_split_indices","title":"<code>get_split_indices(group_keys, split_indices, is_item)</code>","text":"<p>Get the indices from group keys based on item or subject split.</p> <p>Parameters:</p> Name Type Description Default <code>group_keys</code> <code>DataFrame</code> <p>DataFrame containing grouping keys.</p> required <code>split_indices</code> <code>Series</code> <p>Series of split indices.</p> required <code>is_item</code> <code>bool</code> <p>Whether the split is for item identifiers.</p> required <p>Returns:</p> Type Description <code>Index</code> <p>pd.Index: The indices from group_keys that match the split.</p> Source code in <code>src/data/preprocessing/create_folds.py</code> <pre><code>def get_split_indices(\n    self, group_keys: pd.DataFrame, split_indices: pd.Series, is_item: bool\n) -&gt; pd.Index:\n    \"\"\"\n    Get the indices from group keys based on item or subject split.\n\n    Parameters:\n        group_keys (pd.DataFrame): DataFrame containing grouping keys.\n        split_indices (pd.Series): Series of split indices.\n        is_item (bool): Whether the split is for item identifiers.\n\n    Returns:\n        pd.Index: The indices from group_keys that match the split.\n    \"\"\"\n    if is_item:\n        column = group_keys[self.item_columns].astype(str).apply('_'.join, axis=1)\n    else:\n        column = group_keys[self.subject_column]\n\n    return group_keys.loc[column.isin(split_indices)].index\n</code></pre>"},{"location":"reference/data/preprocessing/create_folds/#data.preprocessing.create_folds.FoldSplitter.get_train_val_test_splits","title":"<code>get_train_val_test_splits(group_keys, fold_index)</code>","text":"<p>Split the data into training, validation, and test sets.</p> <p>Parameters:</p> Name Type Description Default <code>group_keys</code> <code>DataFrame</code> <p>DataFrame containing group keys.</p> required <code>fold_index</code> <code>int</code> <p>The fold index to use for splitting.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>tuple[pd.DataFrame, list[pd.DataFrame], list[pd.DataFrame]]:</p> <code>list[DataFrame]</code> <p>A tuple containing the training keys, a list of validation keys, and a list of test keys.</p> Source code in <code>src/data/preprocessing/create_folds.py</code> <pre><code>def get_train_val_test_splits(\n    self,\n    group_keys: pd.DataFrame,\n    fold_index: int,\n) -&gt; tuple[pd.DataFrame, list[pd.DataFrame], list[pd.DataFrame]]:\n    \"\"\"\n    Split the data into training, validation, and test sets.\n\n    Parameters:\n        group_keys (pd.DataFrame): DataFrame containing group keys.\n        fold_index (int): The fold index to use for splitting.\n\n    Returns:\n        tuple[pd.DataFrame, list[pd.DataFrame], list[pd.DataFrame]]:\n        A tuple containing the training keys, a list of validation keys, and a list of test keys.\n    \"\"\"\n    test_indices, val_indices, train_indices = self.get_fold_indices(fold_index)\n    subject_folds = self.subject_folds\n    item_folds = self.item_folds\n\n    # Get subject and item IDs for each split into train/val/test\n    subject_train_indices = FoldSplitter.get_combined_indices(\n        subject_folds, train_indices\n    )\n    subject_val_indices = FoldSplitter.get_combined_indices(\n        subject_folds, val_indices\n    )\n    subject_test_indices = FoldSplitter.get_combined_indices(\n        subject_folds, test_indices\n    )\n    item_train_indices = FoldSplitter.get_combined_indices(\n        item_folds, train_indices\n    )\n    item_val_indices = FoldSplitter.get_combined_indices(item_folds, val_indices)\n    item_test_indices = FoldSplitter.get_combined_indices(item_folds, test_indices)\n\n    # Get trial-level indices in group_keys per split\n    train_subjects_indx = self.get_split_indices(\n        group_keys, subject_train_indices, is_item=False\n    )\n    val_subjects_indx = self.get_split_indices(\n        group_keys, subject_val_indices, is_item=False\n    )\n    test_subjects_indx = self.get_split_indices(\n        group_keys, subject_test_indices, is_item=False\n    )\n    train_items_indx = self.get_split_indices(\n        group_keys, item_train_indices, is_item=True\n    )\n    val_items_indx = self.get_split_indices(\n        group_keys, item_val_indices, is_item=True\n    )\n    test_items_indx = self.get_split_indices(\n        group_keys, item_test_indices, is_item=True\n    )\n\n    train_indices = np.array(train_subjects_indx.intersection(train_items_indx))\n\n    seen_subject_unseen_item_test_indices = np.array(\n        test_items_indx.intersection(train_subjects_indx.union(val_subjects_indx))\n    )\n    unseen_subject_seen_item_test_indices = np.array(\n        train_items_indx.union(val_items_indx).intersection(test_subjects_indx)\n    )\n    unseen_subject_unseen_item_test_indices = np.array(\n        test_items_indx.intersection(test_subjects_indx)\n    )\n\n    unseen_subject_unseen_item_val_indices = np.array(\n        val_subjects_indx.intersection(val_items_indx)\n    )\n    unseen_subject_seen_item_val_indices = np.array(\n        val_subjects_indx.intersection(train_items_indx)\n    )\n    seen_subject_unseen_item_val_indices = np.array(\n        train_subjects_indx.intersection(val_items_indx)\n    )\n\n    assert len(group_keys) == len(train_indices) + len(\n        seen_subject_unseen_item_test_indices\n    ) + len(unseen_subject_seen_item_test_indices) + len(\n        unseen_subject_unseen_item_test_indices\n    ) + len(unseen_subject_unseen_item_val_indices) + len(\n        unseen_subject_seen_item_val_indices\n    ) + len(seen_subject_unseen_item_val_indices), (\n        'Data subsets do not sum to all the data'\n    )\n\n    self.assert_no_duplicates(train_indices, 'train_indices')\n    train_keys = group_keys.iloc[train_indices]\n    train_keys.attrs['name'] = SetNames.TRAIN\n    train_keys.attrs['set_name'] = SetNames.TRAIN\n\n    test_key_types = [\n        (SetNames.SEEN_SUBJECT_UNSEEN_ITEM, seen_subject_unseen_item_test_indices),\n        (SetNames.UNSEEN_SUBJECT_SEEN_ITEM, unseen_subject_seen_item_test_indices),\n        (\n            SetNames.UNSEEN_SUBJECT_UNSEEN_ITEM,\n            unseen_subject_unseen_item_test_indices,\n        ),\n    ]\n    test_keys_list = []\n    for key_name, indices in test_key_types:\n        self.assert_no_duplicates(indices, key_name)\n        keys = group_keys.iloc[indices]\n        keys.attrs['name'] = key_name\n        keys.attrs['set_name'] = SetNames.TEST\n        test_keys_list.append(keys)\n\n    val_keys_list = []\n    val_key_types = [\n        (SetNames.SEEN_SUBJECT_UNSEEN_ITEM, seen_subject_unseen_item_val_indices),\n        (SetNames.UNSEEN_SUBJECT_SEEN_ITEM, unseen_subject_seen_item_val_indices),\n        (\n            SetNames.UNSEEN_SUBJECT_UNSEEN_ITEM,\n            unseen_subject_unseen_item_val_indices,\n        ),\n    ]\n    for key_name, indices in val_key_types:\n        self.assert_no_duplicates(indices, key_name)\n        keys = group_keys.iloc[indices]\n        keys.attrs['name'] = key_name\n        keys.attrs['set_name'] = SetNames.VAL\n        val_keys_list.append(keys)\n\n    self.print_group_info('Train', train_keys)\n    for keys in val_keys_list:\n        self.print_group_info(f'Val {keys.attrs[\"name\"]}', keys)\n    for keys in test_keys_list:\n        self.print_group_info(f'Test {keys.attrs[\"name\"]}', keys)\n\n    all_keys = pd.concat([train_keys] + val_keys_list + test_keys_list).sort_index()\n    self.print_group_info('All', all_keys)\n\n    return train_keys, val_keys_list, test_keys_list\n</code></pre>"},{"location":"reference/data/preprocessing/create_folds/#data.preprocessing.create_folds.FoldSplitter.load_folds","title":"<code>load_folds()</code>","text":"<p>Load folds from the CSV files for subjects and items.</p> <p>Returns:</p> Type Description <code>tuple[list[DataFrame], list[DataFrame]]</code> <p>tuple[list[pd.DataFrame], list[pd.DataFrame]]: A tuple containing the subject folds and item folds as DataFrames.</p> Source code in <code>src/data/preprocessing/create_folds.py</code> <pre><code>def load_folds(self) -&gt; tuple[list[pd.DataFrame], list[pd.DataFrame]]:\n    \"\"\"\n    Load folds from the CSV files for subjects and items.\n\n    Returns:\n        tuple[list[pd.DataFrame], list[pd.DataFrame]]: A tuple containing the subject folds\n            and item folds as DataFrames.\n    \"\"\"\n    try:\n        folds_path = Path(HydraConfig.get().runtime.output_dir) / 'folds'\n    except Exception:  # no hydra\n        logger.info(\n            f'HydraConfig not found. Using default path. Loading folds from {self.folds_path}'\n        )\n        folds_path = self.folds_path\n\n    subject_folds_path = folds_path / 'subjects'\n    item_folds_path = folds_path / 'items'\n    # load all folds\n\n    for i in range(self.num_folds):\n        subject_fold_path = subject_folds_path / f'fold_{i}.csv'\n        item_fold_path = item_folds_path / f'fold_{i}.csv'\n        self.subject_folds[i] = pd.read_csv(subject_fold_path, header=None)\n        self.item_folds[i] = pd.read_csv(item_fold_path, header=None)\n    return self.subject_folds, self.item_folds\n</code></pre>"},{"location":"reference/data/preprocessing/create_folds/#data.preprocessing.create_folds.FoldSplitter.print_group_info","title":"<code>print_group_info(name, keys)</code>  <code>staticmethod</code>","text":"<p>Print group information.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the group.</p> required <code>keys</code> <code>DataFrame</code> <p>DataFrame containing the group keys.</p> required Source code in <code>src/data/preprocessing/create_folds.py</code> <pre><code>@staticmethod\ndef print_group_info(name: str, keys: pd.DataFrame) -&gt; None:\n    \"\"\"\n    Print group information.\n\n    Parameters:\n        name (str): The name of the group.\n        keys (pd.DataFrame): DataFrame containing the group keys.\n    \"\"\"\n    logger.info(\n        f'{name}: # Trials: {len(keys)}. '\n        f'# Items: {keys[Fields.UNIQUE_PARAGRAPH_ID].nunique()}; '\n        f'# Subjects: {keys[Fields.SUBJECT_ID].nunique()}'\n    )\n</code></pre>"},{"location":"reference/data/preprocessing/create_folds/#data.preprocessing.create_folds.load_eval_regimes_from_csv","title":"<code>load_eval_regimes_from_csv(csv_path, groupby_columns)</code>","text":"<p>Loads evaluation regimes from a CSV file.</p> <p>Parameters:</p> Name Type Description Default <code>csv_path</code> <code>Path</code> <p>Path to the CSV file containing the regimes.</p> required <code>groupby_columns</code> <code>list</code> <p>Columns used for grouping.</p> required <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>List of dataframes, each containing the group keys for a regime.</p> Source code in <code>src/data/preprocessing/create_folds.py</code> <pre><code>def load_eval_regimes_from_csv(csv_path: Path, groupby_columns: list) -&gt; list:\n    \"\"\"\n    Loads evaluation regimes from a CSV file.\n\n    Args:\n        csv_path (Path): Path to the CSV file containing the regimes.\n        groupby_columns (list): Columns used for grouping.\n\n    Returns:\n        list: List of dataframes, each containing the group keys for a regime.\n    \"\"\"\n    # Load the combined dataframe\n    all_regimes_df = pd.read_csv(csv_path)\n\n    # Split back into separate dataframes based on regime column\n    regimes = []\n    for regime_name in all_regimes_df['regime'].unique():\n        regime_df = all_regimes_df[all_regimes_df['regime'] == regime_name].copy()\n        regime_df = regime_df[groupby_columns]  # Keep only the groupby columns\n        regimes.append(regime_df)\n\n    return regimes\n</code></pre>"},{"location":"reference/data/preprocessing/create_folds/#data.preprocessing.create_folds.main","title":"<code>main()</code>","text":"<p>Main function to execute dataset splitting.</p> <p>Configures the data settings and initiates the data splitting process.</p> Source code in <code>src/data/preprocessing/create_folds.py</code> <pre><code>def main() -&gt; None:\n    \"\"\"\n    Main function to execute dataset splitting.\n\n    Configures the data settings and initiates the data splitting process.\n    \"\"\"\n\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--dataset', type=str, default='')\n    parser.add_argument(\n        '--do_not_recreate_trial_folds',\n        action='store_true',\n        help='Recreate trial folds',\n    )\n    parser.add_argument(\n        '--do_not_recreate_item_subject_folds',\n        action='store_true',\n        help='Recreate item/subject folds',\n    )\n    args = parser.parse_args()\n    recreate_trial_folds = not args.do_not_recreate_trial_folds\n    recreate_item_subject_folds = not args.do_not_recreate_item_subject_folds\n    dataset = args.dataset\n    if dataset:\n        datasets = dataset.split(',')\n    else:\n        datasets = DataSets\n\n    for dataset_name in datasets:\n        data_args = get_data_args(dataset_name)\n        if not data_args:\n            logger.warning(f'No data args found for {dataset_name}. Skipping...')\n            continue\n        try:\n            logger.info(f'Splitting {dataset_name}...')\n            split_dataset(data_args, recreate_trial_folds, recreate_item_subject_folds)\n        # except ValueError as e:\n        #     logger.info(f'ValueError splitting {dataset_name}: {e}')\n        except FileNotFoundError as e:\n            logger.warning(f'FileNotFoundError splitting {dataset_name}: {e}')\n</code></pre>"},{"location":"reference/data/preprocessing/create_folds/#data.preprocessing.create_folds.save_eval_regimes_to_csv","title":"<code>save_eval_regimes_to_csv(eval_regimes, regime_names, save_path)</code>","text":"<p>Saves the evaluation regimes to a single CSV file.</p> <p>Parameters:</p> Name Type Description Default <code>eval_regimes</code> <code>list</code> <p>List of dataframes containing the group keys for each regime.</p> required <code>regime_names</code> <code>list</code> <p>Names of the evaluation regimes.</p> required <code>save_path</code> <code>Path</code> <p>Path where to save the CSV file.</p> required Source code in <code>src/data/preprocessing/create_folds.py</code> <pre><code>def save_eval_regimes_to_csv(\n    eval_regimes: list, regime_names: list, save_path: Path\n) -&gt; None:\n    \"\"\"\n    Saves the evaluation regimes to a single CSV file.\n\n    Args:\n        eval_regimes (list): List of dataframes containing the group keys for each regime.\n        regime_names (list): Names of the evaluation regimes.\n        save_path (Path): Path where to save the CSV file.\n    \"\"\"\n    # Create a combined dataframe with a column indicating the regime\n    all_regimes_df = pd.DataFrame()\n\n    for regime_df, regime_name in zip(eval_regimes, regime_names):\n        regime_df = regime_df.copy()\n        regime_df['regime'] = regime_name\n        all_regimes_df = pd.concat([all_regimes_df, regime_df])\n\n    # Save to CSV\n    all_regimes_df.to_csv(save_path, index=False)\n</code></pre>"},{"location":"reference/data/preprocessing/create_folds/#data.preprocessing.create_folds.save_fold_data","title":"<code>save_fold_data(subset_df, fold_dir, report_type, regime_name)</code>","text":"<p>Saves the subset of data for a specific fold and regime.</p> <p>Parameters:</p> Name Type Description Default <code>subset_df</code> <code>DataFrame</code> <p>The subset of data to save.</p> required <code>fold_dir</code> <code>Path</code> <p>Directory for the current fold.</p> required <code>report_type</code> <code>str</code> <p>The type of report, e.g., 'ia' or 'fixations'.</p> required <code>regime_name</code> <code>str</code> <p>The name of the evaluation regime.</p> required Source code in <code>src/data/preprocessing/create_folds.py</code> <pre><code>def save_fold_data(\n    subset_df: pd.DataFrame,\n    fold_dir: Path,\n    report_type: str,\n    regime_name: str,\n) -&gt; None:\n    \"\"\"\n    Saves the subset of data for a specific fold and regime.\n\n    Args:\n        subset_df (pd.DataFrame): The subset of data to save.\n        fold_dir (Path): Directory for the current fold.\n        report_type (str): The type of report, e.g., 'ia' or 'fixations'.\n        regime_name (str): The name of the evaluation regime.\n    \"\"\"\n    fold_dir.mkdir(parents=True, exist_ok=True)\n    save_path = fold_dir / f'{report_type}_{regime_name}.feather'\n    subset_df.to_feather(save_path)\n    logger.info(f'Saved {save_path}')\n</code></pre>"},{"location":"reference/data/preprocessing/create_folds/#data.preprocessing.create_folds.split_data_report","title":"<code>split_data_report(data_path, report_type, data_config, splitter, eval_regime_names, folds_folder_path, recreate_trial_folds, recreate_item_subject_folds)</code>","text":"<p>Generates and saves data reports for a specified report type.</p> <p>The function loads the raw data from a given path, converts grouping columns to strings, and organizes the data into folds using the provided splitter. It saves the evaluation regimes to a CSV file and uses them to create data subsets which are saved as Feather files.</p> <p>Parameters:</p> Name Type Description Default <code>data_path</code> <code>Path</code> <p>Path to the raw data file.</p> required <code>report_type</code> <code>DataType</code> <p>The type of report, e.g., 'ia' or 'fixations'.</p> required <code>data_config</code> <code>DataConfig</code> <p>Data configuration settings.</p> required <code>splitter</code> <code>FoldSplitter</code> <p>Instance responsible for splitting the dataset.</p> required <code>eval_regime_names</code> <code>List[str]</code> <p>Names for each evaluation regime.</p> required <code>folds_folder_path</code> <code>Path</code> <p>Directory where fold files will be stored.</p> required <code>recreate_trial_folds</code> <code>bool</code> <p>Flag to indicate if trial folds should be recreated.</p> required <code>recreate_item_subject_folds</code> <code>bool</code> <p>Flag to indicate if item/subject folds should be recreated.</p> required Source code in <code>src/data/preprocessing/create_folds.py</code> <pre><code>def split_data_report(\n    data_path: Path,\n    report_type: DataType,\n    data_config: DataArgs,\n    splitter: FoldSplitter,\n    eval_regime_names: list[str],\n    folds_folder_path: Path,\n    recreate_trial_folds: bool,\n    recreate_item_subject_folds: bool,\n) -&gt; None:\n    \"\"\"\n    Generates and saves data reports for a specified report type.\n\n    The function loads the raw data from a given path, converts grouping columns to strings,\n    and organizes the data into folds using the provided splitter. It saves the evaluation\n    regimes to a CSV file and uses them to create data subsets which are saved as Feather files.\n\n    Args:\n        data_path (Path): Path to the raw data file.\n        report_type (DataType): The type of report, e.g., 'ia' or 'fixations'.\n        data_config (DataConfig): Data configuration settings.\n        splitter (FoldSplitter): Instance responsible for splitting the dataset.\n        eval_regime_names (List[str]): Names for each evaluation regime.\n        folds_folder_path (Path): Directory where fold files will be stored.\n        recreate_trial_folds (bool): Flag to indicate if trial folds should be recreated.\n        recreate_item_subject_folds (bool): Flag to indicate if item/subject folds should be recreated.\n    \"\"\"\n    # Load raw data and ensure grouping columns are treated as strings.\n    df = load_raw_data(data_path)\n    if 'RCS_score' in df.columns:\n        df['RCS_score'] = df['RCS_score'].fillna(-1)\n    grouped_data = df.groupby(data_config.groupby_columns)\n    group_keys = pd.DataFrame(\n        data=list(grouped_data.groups), columns=data_config.groupby_columns\n    )\n\n    # Check if folds should be created or already exist\n    trial_ids_folder = folds_folder_path / 'trial_ids'\n    trial_ids_folder.mkdir(parents=True, exist_ok=True)\n\n    def check_or_create_folds(\n        folder_path: Path, recreate_flag: bool, action: callable\n    ) -&gt; None:\n        \"\"\"\n        Check if folds exist or need to be recreated, and perform the specified action.\n\n        Args:\n            folder_path (Path): Path to the folder containing fold files.\n            recreate_flag (bool): Flag indicating whether to recreate the folds.\n            action (callable): Function to execute if folds need to be created.\n        \"\"\"\n        if recreate_flag or not all(\n            (folder_path / f'fold_{fold_index}_trial_ids_by_regime.csv').exists()\n            for fold_index in range(data_config.n_folds)\n        ):\n            action()\n\n    # Step 1: Create or load item/subject folds\n    if recreate_item_subject_folds:\n        check_or_create_folds(\n            trial_ids_folder,\n            recreate_item_subject_folds,\n            lambda: splitter.create_default_folds(group_keys),\n        )\n    else:\n        splitter.load_folds()\n\n    # Step 2: Create or load trial folds\n    check_or_create_folds(\n        trial_ids_folder,\n        recreate_trial_folds,\n        lambda: splitter.create_trial_folds(\n            group_keys, eval_regime_names, trial_ids_folder\n        ),\n    )\n\n    # Step 3: Use the saved evaluation regimes to create data subsets\n    for fold_index in range(data_config.n_folds):\n        # Define the path for the trial IDs CSV for the current fold\n        trial_ids_csv_path = (\n            trial_ids_folder / f'fold_{fold_index}_trial_ids_by_regime.csv'\n        )\n\n        # Load the evaluation regimes from CSV\n        eval_regimes = load_eval_regimes_from_csv(\n            trial_ids_csv_path, data_config.groupby_columns\n        )\n\n        # Create the data subsets using the loaded regimes\n        for regime_data, regime_name in zip(eval_regimes, eval_regime_names):\n            subset_df = _get_data(\n                raw_data=df,\n                groups=grouped_data.groups,\n                group_keys=regime_data,\n            )\n\n            save_fold_data(\n                subset_df=subset_df,\n                fold_dir=folds_folder_path.parent / 'folds' / f'fold_{fold_index}',\n                report_type=report_type,\n                regime_name=regime_name,\n            )\n</code></pre>"},{"location":"reference/data/preprocessing/create_folds/#data.preprocessing.create_folds.split_dataset","title":"<code>split_dataset(data_args, recreate_trial_folds, recreate_item_subject_folds)</code>","text":"<p>Splits the dataset into folds and generates reports for different data types.</p> <p>This function initializes a FoldSplitter and then produces training, validation, and test splits for multiple report types ('ia' and 'fixations').</p> <p>Parameters:</p> Name Type Description Default <code>data_args</code> <code>DataArgs</code> <p>Configuration parameters for data splitting.</p> required <code>recreate_trial_folds</code> <code>bool</code> <p>Flag to indicate if trial folds should be recreated.</p> required <code>recreate_item_subject_folds</code> <code>bool</code> <p>Flag to indicate if item/subject folds should be</p> required Source code in <code>src/data/preprocessing/create_folds.py</code> <pre><code>def split_dataset(\n    data_args: DataArgs, recreate_trial_folds: bool, recreate_item_subject_folds: bool\n) -&gt; None:\n    \"\"\"\n    Splits the dataset into folds and generates reports for different data types.\n\n    This function initializes a FoldSplitter and then produces training, validation,\n    and test splits for multiple report types ('ia' and 'fixations').\n\n    Args:\n        data_args (DataArgs): Configuration parameters for data splitting.\n        recreate_trial_folds (bool): Flag to indicate if trial folds should be recreated.\n        recreate_item_subject_folds (bool): Flag to indicate if item/subject folds should be\n    \"\"\"\n    folds_folder_path = data_args.base_path / 'folds_metadata'\n\n    splitter = FoldSplitter(\n        item_columns=data_args.split_item_columns,\n        subject_column=data_args.subject_column,\n        groupby_columns=data_args.groupby_columns,\n        num_folds=data_args.n_folds,\n        stratify=data_args.stratify,\n        folds_path=folds_folder_path,\n        higher_level_split_column=data_args.higher_level_split,\n    )\n\n    eval_regime_names = (\n        [f'{SetNames.TRAIN}_{SetNames.TRAIN}']\n        + [f'{SetNames.VAL}_{fold_name}' for fold_name in REGIMES]\n        + [f'{SetNames.TEST}_{fold_name}' for fold_name in REGIMES]\n    )\n\n    # Retrieve data paths from configuration.\n    data_args = get_data_args(data_args.dataset_name)\n    report_details = [\n        (DataType.IA, data_args.ia_path),\n        (DataType.FIXATIONS, data_args.fixations_path),\n        (DataType.TRIAL_LEVEL, data_args.trial_level_path),\n    ]\n\n    for report_type, data_path in report_details:\n        split_data_report(\n            data_path=data_path,\n            report_type=report_type,\n            data_config=data_args,\n            splitter=splitter,\n            eval_regime_names=eval_regime_names,\n            folds_folder_path=folds_folder_path,\n            recreate_trial_folds=recreate_trial_folds,\n            recreate_item_subject_folds=recreate_item_subject_folds,\n        )\n        # makes sure that we don't recreate the folds again\n        if recreate_trial_folds:\n            recreate_trial_folds = False\n        if recreate_item_subject_folds:\n            recreate_item_subject_folds = False\n</code></pre>"},{"location":"reference/data/preprocessing/download_data/","title":"download_data","text":""},{"location":"reference/data/preprocessing/download_data/#data.preprocessing.download_data.convert_rda_to_csv","title":"<code>convert_rda_to_csv(root, dataset_name)</code>","text":"<p>Convert RDA files to CSV for specific datasets.</p> Source code in <code>src/data/preprocessing/download_data.py</code> <pre><code>def convert_rda_to_csv(root: Path, dataset_name: str) -&gt; None:\n    \"\"\"Convert RDA files to CSV for specific datasets.\"\"\"\n    if dataset_name != DataSets.MECO_L2:\n        return\n    rda_path = root / 'MECOL2/stimuli/texts.meco.l2.rda'\n    csv_path = root / 'MECOL2/stimuli/stimuli.csv'\n\n    if csv_path.exists():\n        logger.info(f'{csv_path} already exists. Skipping conversion...')\n        return\n\n    if not rda_path.exists():\n        logger.warning(f'{rda_path} not found. Skipping conversion...')\n        return\n\n    logger.info(f'Converting {rda_path} to {csv_path}')\n    rda_data = rdata.read_rda(rda_path)\n    df = rda_data['d']\n    csv_path.parent.mkdir(parents=True, exist_ok=True)\n    df.to_csv(csv_path, index=False)\n    logger.info(f'Saved stimuli CSV to {csv_path}')\n</code></pre>"},{"location":"reference/data/preprocessing/download_data/#data.preprocessing.download_data.download_auxiliary_files","title":"<code>download_auxiliary_files(root, dataset_name)</code>","text":"<p>Download auxiliary resources not covered by DatasetLibrary for a specific dataset.</p> Source code in <code>src/data/preprocessing/download_data.py</code> <pre><code>def download_auxiliary_files(root: Path, dataset_name: str) -&gt; None:\n    \"\"\"Download auxiliary resources not covered by DatasetLibrary for a specific dataset.\"\"\"\n    if dataset_name not in AUXILIARY_FILES:\n        return\n\n    for relative_path, resource_id in AUXILIARY_FILES[dataset_name].items():\n        destination = root / relative_path\n        if destination.exists():\n            logger.info(\n                f'{relative_path} already present at {destination}. Continuing...'\n            )\n            continue\n\n        url = f'{BASE_OSF_URL}{resource_id}'\n        logger.info(f'Downloading {relative_path} from {url}')\n        response = requests.get(url, stream=True, timeout=60)\n        response.raise_for_status()\n\n        destination.parent.mkdir(parents=True, exist_ok=True)\n        with open(destination, 'wb') as fp:\n            for chunk in response.iter_content(chunk_size=8192):\n                if chunk:\n                    fp.write(chunk)\n</code></pre>"},{"location":"reference/data/preprocessing/download_data/#data.preprocessing.download_data.load_or_download_dataset","title":"<code>load_or_download_dataset(dataset_name, data_path, download=False)</code>","text":"<p>Load or download a dataset based on the flag.</p> Source code in <code>src/data/preprocessing/download_data.py</code> <pre><code>def load_or_download_dataset(\n    dataset_name: str, data_path: Path, download: bool = False\n) -&gt; None:\n    \"\"\"Load or download a dataset based on the flag.\"\"\"\n    if dataset_name == DataSets.MECO_L2:\n        dataset_def_w1 = prepare_dataset_definition(f'{dataset_name}W1')\n        dataset_def_w2 = prepare_dataset_definition(f'{dataset_name}W2')\n        dataset_w1 = pm.Dataset(dataset_def_w1, data_path / DataSets.MECO_L2W1)\n        dataset_w2 = pm.Dataset(dataset_def_w2, data_path / DataSets.MECO_L2W2)\n        if download:\n            dataset_w1.download()\n            dataset_w2.download()\n        else:\n            dataset_w1.load()\n            dataset_w2.load()\n    else:\n        dataset_def = prepare_dataset_definition(dataset_name)\n        dataset = pm.Dataset(dataset_def, data_path / dataset_name)\n        if download:\n            dataset.download()\n        else:\n            dataset.load()\n</code></pre>"},{"location":"reference/data/preprocessing/download_data/#data.preprocessing.download_data.prepare_dataset_definition","title":"<code>prepare_dataset_definition(dataset_name)</code>","text":"<p>Prepare dataset definition with gaze files disabled.</p> Source code in <code>src/data/preprocessing/download_data.py</code> <pre><code>def prepare_dataset_definition(dataset_name: str):\n    \"\"\"Prepare dataset definition with gaze files disabled.\"\"\"\n    dataset_def = pm.DatasetLibrary.get(dataset_name)\n    dataset_def.resources = ResourceDefinitions(\n        [resource for resource in dataset_def.resources if resource.content != 'gaze']\n    )\n\n    return dataset_def\n</code></pre>"},{"location":"reference/data/preprocessing/preprocess_data/","title":"preprocess_data","text":"<p>Preprocess data for different datasets. This script standardizes column names, filters data, and calculates text metrics. It is designed to be modular, allowing for easy addition of new datasets and processing steps.</p>"},{"location":"reference/data/preprocessing/preprocess_data/#data.preprocessing.preprocess_data.main","title":"<code>main()</code>","text":"<p>Main function to execute dataset processing. Configures the data settings and initiates the data processing.</p> Source code in <code>src/data/preprocessing/preprocess_data.py</code> <pre><code>def main() -&gt; int:\n    \"\"\"\n    Main function to execute dataset processing.\n    Configures the data settings and initiates the data processing.\n    \"\"\"\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--dataset', type=str, default='')\n    args = parser.parse_args()\n    dataset = args.dataset\n\n    datasets = dataset.split(',') if dataset else list(DataSets)\n\n    for dataset_name in datasets:\n        logger.info(f'Processing {dataset_name}...')\n        data_args = get_data_args(dataset_name)\n        if not data_args:\n            logger.warning(\n                f'No data args found for {dataset_name}. Skipping...',\n            )\n            continue\n        try:\n            processor = get_processor(data_args)\n            processed_data = processor.process()\n            processor.save_processed_data(processed_data=processed_data)\n            logger.info(f'Finished processing {dataset_name}')\n        except FileNotFoundError as e:\n            logger.warning(f'FileNotFoundError processing {dataset_name}: {e}')\n\n    return 0\n</code></pre>"},{"location":"reference/data/preprocessing/stats/","title":"stats","text":""},{"location":"reference/data/preprocessing/stats/#data.preprocessing.stats.compute_dataset_profile","title":"<code>compute_dataset_profile(dataset_name, mode, curr_df, output_path, data_args)</code>","text":"<p>Computes and updates per-dataset profile stats in a shared CSV.</p> Source code in <code>src/data/preprocessing/stats.py</code> <pre><code>def compute_dataset_profile(\n    dataset_name: str,\n    mode: DataType,\n    curr_df: pd.DataFrame,\n    output_path: Path,\n    data_args: DataArgs,\n):\n    \"\"\"\n    Computes and updates per-dataset profile stats in a shared CSV.\n    \"\"\"\n    logger.info(f'---- Stats for: {dataset_name} ({mode}) ----')\n    stats = {}\n    item_col = data_args.unique_item_column\n    participant_col = data_args.subject_column\n    task_and_labels = data_args.tasks\n    additional_info = {\n        key: getattr(data_args, key, None)\n        for key in ['text_source', 'text_language', 'text_domain', 'text_type']\n        if hasattr(data_args, key)\n    }\n    if mode == DataType.IA:\n        stats = {\n            'n_participants': curr_df[participant_col].nunique(),\n            'n_items': curr_df[item_col].nunique(),\n            'n_words': curr_df.shape[0],\n            'n_trials': curr_df['unique_trial_id'].nunique(),\n            'n_words_per_participant': format_stats(\n                curr_df.groupby(participant_col).size()\n            ),\n            'n_words_per_item': format_stats(curr_df.groupby(item_col).size()),\n        }\n\n        trial_level = curr_df.drop_duplicates(subset=['unique_trial_id'])\n        # label stats\n\n        if dataset_name == 'OneStop':\n            stats['n_words_corpus'] = 19428  # hardcoded for OneStop (based on Adv)\n\n        elif dataset_name == 'CopCo':\n            data_parags = curr_df[\n                ['part', 'unique_paragraph_id', 'paragraph']\n            ].drop_duplicates()\n            data_parags['n_words'] = data_parags['paragraph'].apply(\n                lambda x: len((x).split())\n            )\n            stats['n_words_corpus'] = int(data_parags['n_words'].sum())\n            df_to_save = data_parags[\n                ['n_words', 'part', 'unique_paragraph_id', 'paragraph']\n            ].sort_values(by=['part', 'unique_paragraph_id'])\n            df_to_save.to_csv(STATS_FOLDER / 'CopCo_paragraphs.csv', index=False)\n            logger.info(f'n_words calculated from {data_parags.shape[0]} paragraphs.')\n\n        elif dataset_name == 'PoTeC':\n            data_parags = (\n                curr_df[['unique_paragraph_id', 'paragraph']]\n                .drop_duplicates()\n                .sort_values(by='unique_paragraph_id')\n            )\n            data_parags['n_words'] = data_parags['paragraph'].apply(\n                lambda x: len((x).split())\n            )\n            stats['n_words_corpus'] = int(data_parags['n_words'].sum())\n            logger.info(f'Total n_words: {stats[\"n_words_corpus\"]}')\n            df_to_save = data_parags[\n                ['n_words', 'unique_paragraph_id', 'paragraph']\n            ].sort_values(by='unique_paragraph_id')\n            df_to_save.to_csv(\n                STATS_FOLDER / f'{dataset_name}_paragraphs.csv', index=False\n            )\n            logger.info(f'n_words calculated from {data_parags.shape[0]} paragraphs.')\n\n        elif dataset_name == 'SBSAT':\n            data_parags = (\n                curr_df[['unique_paragraph_id', 'paragraph']]\n                .sort_values(by='unique_paragraph_id')\n                .drop_duplicates()\n                .sort_values(by='unique_paragraph_id')\n            )\n            data_parags['n_words'] = data_parags['paragraph'].apply(\n                lambda x: len((x).split())\n            )\n            # TODO: fix bug in SBSAT data\n            logger.warning(\n                'using different caclulation because of BUG in SBSAT paragraphs'\n            )\n            grouped = (\n                data_parags.groupby('unique_paragraph_id')[\n                    ['unique_paragraph_id', 'n_words']\n                ]\n                .max()\n                .reset_index(drop=True)\n            )\n            stats['n_words_corpus'] = int(grouped['n_words'].sum())\n            logger.info(f'Total n_words: {stats[\"n_words_corpus\"]}')\n            df_to_save = data_parags[\n                ['n_words', 'unique_paragraph_id', 'paragraph']\n            ].sort_values(by='unique_paragraph_id')\n            df_to_save.to_csv(\n                STATS_FOLDER / f'{dataset_name}_paragraphs.csv', index=False\n            )\n            logger.info(f'n_words calculated from {data_parags.shape[0]} paragraphs.')\n\n        elif dataset_name == 'MECOL2':\n            data_parags = (\n                curr_df[['paragraph', 'itemid', 'unique_paragraph_id']]\n                .drop_duplicates()\n                .sort_values(by='itemid')\n                .reset_index(drop=True)\n            )\n            data_parags['n_words'] = data_parags['paragraph'].apply(\n                lambda x: len((x).split())\n            )\n            # TODO: fix bug in MECO data\n            logger.warning(\n                'using different caclulation because of BUG in MECO paragraphs'\n            )\n            grouped = (\n                data_parags.groupby('itemid')[['itemid', 'n_words']]\n                .min()\n                .reset_index(drop=True)\n            )\n            stats['n_words_corpus'] = int(grouped['n_words'].sum())\n            logger.info(f'Total n_words: {stats[\"n_words_corpus\"]}')\n            df_to_save = data_parags[\n                ['n_words', 'itemid', 'unique_paragraph_id', 'paragraph']\n            ].sort_values(by='itemid')\n            df_to_save.to_csv(\n                STATS_FOLDER / f'{dataset_name}_paragraphs.csv', index=False\n            )\n            logger.info(f'n_words calculated from {data_parags.shape[0]} paragraphs.')\n\n        elif dataset_name == 'IITBHGC':\n            data_parags = (\n                curr_df[['unique_paragraph_id', 'paragraph']]\n                .drop_duplicates()\n                .sort_values(by='unique_paragraph_id')\n            )\n            data_parags['n_words'] = data_parags['paragraph'].apply(\n                lambda x: len((x).split())\n            )\n            stats['n_words_corpus'] = int(data_parags['n_words'].sum())\n            logger.info(f'Total n_words: {stats[\"n_words_corpus\"]}')\n            df_to_save = data_parags[\n                ['n_words', 'unique_paragraph_id', 'paragraph']\n            ].sort_values(by='unique_paragraph_id')\n            df_to_save.to_csv(\n                STATS_FOLDER / f'{dataset_name}_paragraphs.csv', index=False\n            )\n            logger.info(f'n_words calculated from {data_parags.shape[0]} paragraphs.')\n\n        else:\n            logger.info(f'Unknown dataset: {dataset_name}')\n            stats['n_words_corpus'] = '???'\n\n        for task, label_col in task_and_labels.items():\n            stats[f'{task}_col'] = label_col\n\n            # if label_col is boolean, convert to int\n            if pd.api.types.is_bool_dtype(curr_df[label_col]):\n                curr_df[label_col] = curr_df[label_col].astype(int)\n\n            # calc mean if not categorical\n            if pd.api.types.is_numeric_dtype(curr_df[label_col]):\n                stats[f'{task}_overall'] = format_stats(\n                    trial_level[label_col], rounding=2\n                )\n                stats[f'{task}_per_participant'] = format_stats(\n                    trial_level.groupby(participant_col)[label_col].mean(), rounding=2\n                )\n                stats[f'{task}_per_item'] = format_stats(\n                    trial_level.groupby(item_col)[label_col].mean(), rounding=2\n                )\n\n            # label distribution if n unique values is small\n            if trial_level[label_col].nunique() &lt;= 15:\n                label_distribution = (\n                    trial_level[label_col].value_counts(normalize=True) * 100\n                ).round(1)\n                stats[f'{task}_distribution'] = label_distribution.to_dict()\n\n    elif mode == DataType.FIXATIONS:\n        stats = {\n            'n_fix': curr_df.shape[0],\n            'n_fix_per_trial': format_stats(curr_df.groupby('unique_trial_id').size()),\n            'n_fix_per_participant': format_stats(\n                curr_df.groupby(participant_col).size()\n            ),\n            'n_fix_per_item': format_stats(curr_df.groupby(item_col).size()),\n        }\n\n    elif mode == DataType.TRIAL_LEVEL:\n        pass  # TODO: add?\n\n    # save to stats all fields in additional_info_dataset except task_and_labels\n    for key, val in additional_info.items():\n        if key != 'task_and_labels':\n            stats[key] = val\n\n    # Load or create the profile DataFrame\n    if output_path.exists():\n        profile_df = pd.read_csv(output_path, index_col=0)\n    else:\n        profile_df = pd.DataFrame()\n\n    # Initialize profile_df if empty\n    if profile_df.empty:\n        profile_df = pd.DataFrame(index=stats.keys(), columns=[data_args.dataset_name])\n    else:\n        for key in stats.keys():\n            if key not in profile_df.index:\n                profile_df.loc[key] = pd.Series()\n\n    # Assign each stat\n    for key, val in stats.items():\n        profile_df.at[key, data_args.dataset_name] = val\n\n    profile_df.sort_index(inplace=True)\n    output_path.parent.mkdir(parents=True, exist_ok=True)\n    profile_df.to_csv(output_path)\n\n    logger.info(\n        f'Updated profile for {data_args.dataset_name} ({mode}) \u2192 {output_path}'\n    )\n</code></pre>"},{"location":"reference/data/preprocessing/union_raw_files/","title":"union_raw_files","text":""},{"location":"reference/data/preprocessing/union_raw_files/#data.preprocessing.union_raw_files.combine_stimulus_files","title":"<code>combine_stimulus_files(data_path, matching_pattern, dataset_name)</code>","text":"<p>Combine stimulus files from a given data path matching a specific pattern.</p> <p>Parameters:</p> Name Type Description Default <code>data_path</code> <code>str</code> <p>Path to the directory containing stimulus files.</p> required <code>matching_pattern</code> <code>str</code> <p>Pattern to match files for combining.</p> required Source code in <code>src/data/preprocessing/union_raw_files.py</code> <pre><code>def combine_stimulus_files(\n    data_path: Path,\n    matching_pattern: str,\n    dataset_name: str,\n) -&gt; None:\n    \"\"\"\n    Combine stimulus files from a given data path matching a specific pattern.\n\n    Args:\n        data_path (str): Path to the directory containing stimulus files.\n        matching_pattern (str): Pattern to match files for combining.\n    \"\"\"\n    stimulus_files = list(data_path.rglob(matching_pattern))\n\n    if not stimulus_files:\n        logger.warning(f'No files found matching {matching_pattern} in {data_path}.')\n        return\n\n    combined_df = pd.DataFrame()\n    for file_ in stimulus_files:\n        if 'reading' in str(file_):\n            read_df = pd.read_csv(file_, sep='\\t')\n            read_df['filename'] = f'{file_.name.split(\"_\")[0]}.png'\n            read_df['sequence_num'] = -1\n            combined_df = pd.concat([combined_df, read_df])\n        else:\n            quest_df = pd.read_csv(file_)\n            quest_df['sequence_num'] = file_.name.split('_')[0].split('-')[-1]\n            combined_df = pd.concat([combined_df, quest_df])\n\n    if 'SBSAT' in str(data_path):\n        logger.info('Filling in missing values for SBSAT dataset...')\n        combined_df['stimulus_type'] = combined_df['filename'].apply(\n            lambda x: x.split('-')[1]\n        )\n        combined_df['is_question'].fillna(False, inplace=True)\n\n        # Create 'question' column by concatenating all 'word' where 'is_question' is True for each filename\n        question_map = (\n            combined_df[combined_df['is_question']]\n            .groupby(['stimulus_type', 'sequence_num'])['word']\n            .apply(' '.join)\n        )\n        combined_df['question'] = combined_df.apply(\n            lambda row: question_map.get(\n                (row['stimulus_type'], row['sequence_num']), None\n            ),  # type: ignore\n            axis=1,\n        )\n\n    combined_df.to_csv(Path(data_path) / 'combined_stimulus.csv', index=False)\n    logger.info(f'Combined stimulus CSV saved: {data_path / \"combined_stimulus.csv\"}')\n</code></pre>"},{"location":"reference/data/preprocessing/dataset_preprocessing/__init__/","title":"init","text":""},{"location":"reference/data/preprocessing/dataset_preprocessing/base/","title":"base","text":""},{"location":"reference/data/preprocessing/dataset_preprocessing/base/#data.preprocessing.dataset_preprocessing.base.DatasetProcessor","title":"<code>DatasetProcessor</code>","text":"<p>Base class for dataset processors</p> Source code in <code>src/data/preprocessing/dataset_preprocessing/base.py</code> <pre><code>class DatasetProcessor:\n    \"\"\"Base class for dataset processors\"\"\"\n\n    def __init__(self, data_args: DataArgs):\n        self.data_args = data_args\n\n    def process(self) -&gt; dict[str, pd.DataFrame]:\n        \"\"\"Process the dataset\"\"\"\n        # Load raw data\n        raw_data = {}\n        if self.data_args.raw_ia_path:\n            raw_data[DataType.IA] = self.load_raw_data(self.data_args.raw_ia_path)\n        else:\n            raw_data[DataType.IA] = None\n\n        if self.data_args.raw_fixations_path:\n            raw_data[DataType.FIXATIONS] = self.load_raw_data(\n                self.data_args.raw_fixations_path\n            )\n        else:\n            raw_data[DataType.FIXATIONS] = None\n\n        # Standardize column names\n        processed_data = {}\n        for data_type, df in raw_data.items():\n            if df is not None:\n                processed_data[data_type] = self.standardize_column_names(\n                    df, data_type=data_type\n                )\n\n        # Dataset-specific processing\n        processed_data = self.dataset_specific_processing(processed_data)\n\n        # Filter data\n        for data_type in processed_data:\n            # Trial level are computed, so we want to keep them as is\n            if data_type != DataType.TRIAL_LEVEL:\n                processed_data[data_type] = self.filter_data(processed_data[data_type])\n\n        return processed_data\n\n    def load_raw_data(self, data_path: Path) -&gt; pd.DataFrame:\n        return load_raw_data(data_path)\n\n    def standardize_column_names(\n        self, df: pd.DataFrame, data_type: DataType\n    ) -&gt; pd.DataFrame:\n        \"\"\"Standardize column names for the dataset\"\"\"\n        # Map of original column names to standardized names\n        column_map = self.get_column_map(data_type)\n\n        if column_map:\n            # Create a dictionary of only the columns that exist in the dataframe\n            valid_columns = {k: v for k, v in column_map.items() if k in df.columns}\n            logger.info(\n                f'Standardizing column names for {self.data_args.dataset_name} {data_type}: {valid_columns}'\n            )\n            return df.rename(columns=valid_columns)\n\n        logger.info(\n            f'{self.data_args.dataset_name} not found in column maps. No changes made.'\n        )\n        return df\n\n    def filter_data(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"Filter data based on dataset-specific criteria\"\"\"\n        columns_to_keep = self.get_columns_to_keep()\n\n        if columns_to_keep:\n            missing_columns = [col for col in columns_to_keep if col not in df.columns]\n            if missing_columns:\n                logger.warning(\n                    f'Missing columns not found in the dataframe: {missing_columns}'\n                )\n                logger.warning(f'All possible columns: {list(df.columns)}')\n            existing_columns = [col for col in columns_to_keep if col in df.columns]\n            df = df[existing_columns]\n            logger.info(\n                f'Filtering columns for {self.data_args.dataset_name}: {existing_columns}'\n            )\n\n        return df\n\n    def save_processed_data(self, processed_data: dict[str, pd.DataFrame]) -&gt; None:\n        \"\"\"\n        Save processed data to processed data folder.\n\n        Args:\n            processed_data: Dictionary containing processed dataframes\n        \"\"\"\n        self.data_args.processed_data_path.mkdir(parents=True, exist_ok=True)\n\n        # Save each dataframe to the processed folder\n        for data_type, df in processed_data.items():\n            if df is not None:\n                output_path = (\n                    self.data_args.processed_data_path / f'{data_type}.feather'\n                )\n                df.to_feather(output_path)\n                logger.info(f'Saved {data_type} to {output_path}')\n\n    @abstractmethod\n    def get_column_map(self, data_type: DataType) -&gt; dict:\n        \"\"\"Get column mapping for the dataset\"\"\"\n        return {}\n\n    @abstractmethod\n    def get_columns_to_keep(self) -&gt; list:\n        \"\"\"Get list of columns to keep after filtering\"\"\"\n        return []\n\n    @abstractmethod\n    def dataset_specific_processing(\n        self, data_dict: dict[str, pd.DataFrame]\n    ) -&gt; dict[str, pd.DataFrame]:\n        \"\"\"Dataset-specific processing steps\"\"\"\n        # Can use the following for surprisal and other metric calculations\n        # from text_metrics.merge_metrics_with_eye_movements import (\n        #     add_metrics_to_word_level_eye_tracking_report,\n        # )\n        # from text_metrics.surprisal_extractors import extractor_switch\n        return data_dict\n</code></pre>"},{"location":"reference/data/preprocessing/dataset_preprocessing/base/#data.preprocessing.dataset_preprocessing.base.DatasetProcessor.dataset_specific_processing","title":"<code>dataset_specific_processing(data_dict)</code>  <code>abstractmethod</code>","text":"<p>Dataset-specific processing steps</p> Source code in <code>src/data/preprocessing/dataset_preprocessing/base.py</code> <pre><code>@abstractmethod\ndef dataset_specific_processing(\n    self, data_dict: dict[str, pd.DataFrame]\n) -&gt; dict[str, pd.DataFrame]:\n    \"\"\"Dataset-specific processing steps\"\"\"\n    # Can use the following for surprisal and other metric calculations\n    # from text_metrics.merge_metrics_with_eye_movements import (\n    #     add_metrics_to_word_level_eye_tracking_report,\n    # )\n    # from text_metrics.surprisal_extractors import extractor_switch\n    return data_dict\n</code></pre>"},{"location":"reference/data/preprocessing/dataset_preprocessing/base/#data.preprocessing.dataset_preprocessing.base.DatasetProcessor.filter_data","title":"<code>filter_data(df)</code>","text":"<p>Filter data based on dataset-specific criteria</p> Source code in <code>src/data/preprocessing/dataset_preprocessing/base.py</code> <pre><code>def filter_data(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"Filter data based on dataset-specific criteria\"\"\"\n    columns_to_keep = self.get_columns_to_keep()\n\n    if columns_to_keep:\n        missing_columns = [col for col in columns_to_keep if col not in df.columns]\n        if missing_columns:\n            logger.warning(\n                f'Missing columns not found in the dataframe: {missing_columns}'\n            )\n            logger.warning(f'All possible columns: {list(df.columns)}')\n        existing_columns = [col for col in columns_to_keep if col in df.columns]\n        df = df[existing_columns]\n        logger.info(\n            f'Filtering columns for {self.data_args.dataset_name}: {existing_columns}'\n        )\n\n    return df\n</code></pre>"},{"location":"reference/data/preprocessing/dataset_preprocessing/base/#data.preprocessing.dataset_preprocessing.base.DatasetProcessor.get_column_map","title":"<code>get_column_map(data_type)</code>  <code>abstractmethod</code>","text":"<p>Get column mapping for the dataset</p> Source code in <code>src/data/preprocessing/dataset_preprocessing/base.py</code> <pre><code>@abstractmethod\ndef get_column_map(self, data_type: DataType) -&gt; dict:\n    \"\"\"Get column mapping for the dataset\"\"\"\n    return {}\n</code></pre>"},{"location":"reference/data/preprocessing/dataset_preprocessing/base/#data.preprocessing.dataset_preprocessing.base.DatasetProcessor.get_columns_to_keep","title":"<code>get_columns_to_keep()</code>  <code>abstractmethod</code>","text":"<p>Get list of columns to keep after filtering</p> Source code in <code>src/data/preprocessing/dataset_preprocessing/base.py</code> <pre><code>@abstractmethod\ndef get_columns_to_keep(self) -&gt; list:\n    \"\"\"Get list of columns to keep after filtering\"\"\"\n    return []\n</code></pre>"},{"location":"reference/data/preprocessing/dataset_preprocessing/base/#data.preprocessing.dataset_preprocessing.base.DatasetProcessor.process","title":"<code>process()</code>","text":"<p>Process the dataset</p> Source code in <code>src/data/preprocessing/dataset_preprocessing/base.py</code> <pre><code>def process(self) -&gt; dict[str, pd.DataFrame]:\n    \"\"\"Process the dataset\"\"\"\n    # Load raw data\n    raw_data = {}\n    if self.data_args.raw_ia_path:\n        raw_data[DataType.IA] = self.load_raw_data(self.data_args.raw_ia_path)\n    else:\n        raw_data[DataType.IA] = None\n\n    if self.data_args.raw_fixations_path:\n        raw_data[DataType.FIXATIONS] = self.load_raw_data(\n            self.data_args.raw_fixations_path\n        )\n    else:\n        raw_data[DataType.FIXATIONS] = None\n\n    # Standardize column names\n    processed_data = {}\n    for data_type, df in raw_data.items():\n        if df is not None:\n            processed_data[data_type] = self.standardize_column_names(\n                df, data_type=data_type\n            )\n\n    # Dataset-specific processing\n    processed_data = self.dataset_specific_processing(processed_data)\n\n    # Filter data\n    for data_type in processed_data:\n        # Trial level are computed, so we want to keep them as is\n        if data_type != DataType.TRIAL_LEVEL:\n            processed_data[data_type] = self.filter_data(processed_data[data_type])\n\n    return processed_data\n</code></pre>"},{"location":"reference/data/preprocessing/dataset_preprocessing/base/#data.preprocessing.dataset_preprocessing.base.DatasetProcessor.save_processed_data","title":"<code>save_processed_data(processed_data)</code>","text":"<p>Save processed data to processed data folder.</p> <p>Parameters:</p> Name Type Description Default <code>processed_data</code> <code>dict[str, DataFrame]</code> <p>Dictionary containing processed dataframes</p> required Source code in <code>src/data/preprocessing/dataset_preprocessing/base.py</code> <pre><code>def save_processed_data(self, processed_data: dict[str, pd.DataFrame]) -&gt; None:\n    \"\"\"\n    Save processed data to processed data folder.\n\n    Args:\n        processed_data: Dictionary containing processed dataframes\n    \"\"\"\n    self.data_args.processed_data_path.mkdir(parents=True, exist_ok=True)\n\n    # Save each dataframe to the processed folder\n    for data_type, df in processed_data.items():\n        if df is not None:\n            output_path = (\n                self.data_args.processed_data_path / f'{data_type}.feather'\n            )\n            df.to_feather(output_path)\n            logger.info(f'Saved {data_type} to {output_path}')\n</code></pre>"},{"location":"reference/data/preprocessing/dataset_preprocessing/base/#data.preprocessing.dataset_preprocessing.base.DatasetProcessor.standardize_column_names","title":"<code>standardize_column_names(df, data_type)</code>","text":"<p>Standardize column names for the dataset</p> Source code in <code>src/data/preprocessing/dataset_preprocessing/base.py</code> <pre><code>def standardize_column_names(\n    self, df: pd.DataFrame, data_type: DataType\n) -&gt; pd.DataFrame:\n    \"\"\"Standardize column names for the dataset\"\"\"\n    # Map of original column names to standardized names\n    column_map = self.get_column_map(data_type)\n\n    if column_map:\n        # Create a dictionary of only the columns that exist in the dataframe\n        valid_columns = {k: v for k, v in column_map.items() if k in df.columns}\n        logger.info(\n            f'Standardizing column names for {self.data_args.dataset_name} {data_type}: {valid_columns}'\n        )\n        return df.rename(columns=valid_columns)\n\n    logger.info(\n        f'{self.data_args.dataset_name} not found in column maps. No changes made.'\n    )\n    return df\n</code></pre>"},{"location":"reference/data/preprocessing/dataset_preprocessing/copco/","title":"copco","text":""},{"location":"reference/data/preprocessing/dataset_preprocessing/copco/#data.preprocessing.dataset_preprocessing.copco.CopCoProcessor","title":"<code>CopCoProcessor</code>","text":"<p>               Bases: <code>DatasetProcessor</code></p> <p>Processor for CopCo dataset</p> Source code in <code>src/data/preprocessing/dataset_preprocessing/copco.py</code> <pre><code>class CopCoProcessor(DatasetProcessor):\n    \"\"\"Processor for CopCo dataset\"\"\"\n\n    def dataset_specific_processing(\n        self, data_dict: dict[str, pd.DataFrame]\n    ) -&gt; dict[str, pd.DataFrame]:\n        \"\"\"CopCo-specific processing steps\"\"\"\n        participants_df = self._load_participant_labels()\n\n        for data_type in [DataType.IA, DataType.FIXATIONS]:\n            if data_type not in data_dict or data_dict[data_type] is None:\n                continue\n\n            df = self._process_data_type(\n                data_dict[data_type], data_type, participants_df\n            )\n            data_dict[data_type] = df\n\n        data_dict['fixations'], data_dict['ia'] = (\n            self.add_ia_report_features_to_fixation_data(\n                data_dict['ia'], data_dict['fixations']\n            )\n        )\n\n        for data_type in [DataType.IA, DataType.FIXATIONS]:\n            data_dict[data_type] = add_missing_features(\n                et_data=data_dict[data_type],\n                trial_groupby_columns=self.data_args.groupby_columns,\n                mode=data_type,\n            )\n\n        data_dict[DataType.TRIAL_LEVEL] = compute_trial_level_features(\n            raw_fixation_data=data_dict[DataType.FIXATIONS],\n            raw_ia_data=data_dict[DataType.IA],\n            trial_groupby_columns=self.data_args.groupby_columns,\n            processed_data_path=self.data_args.processed_data_path,\n        )\n\n        return data_dict\n\n    def get_column_map(self, data_type: DataType) -&gt; dict:\n        \"\"\"Get column mapping for CopCo dataset\"\"\"\n        if data_type == DataType.IA:\n            return {\n                'char_IA_ids': 'char_IA_ids',\n                'landing_position': 'landing_position',\n                'number_of_fixations': 'IA_FIXATION_COUNT',\n                'paragraphId': 'paragraph_id',\n                'paragraphid': 'paragraph_id',\n                'part': 'part',\n                'sentenceId': 'sentence_id',\n                'speechId': 'speech_id',\n                'speechid': 'speech_id',\n                'trialId': 'trial_id',\n                'word': 'word',\n                'wordId': 'word_id',\n                'word_first_fix_dur': 'IA_FIRST_FIX_DURATION',\n                'word_first_pass_dur': 'IA_FIRST_FIX_DWELL_TIME',\n                'word_go_past_time': 'IA_REGRESSION_OUT_TIME',\n                'word_mean_fix_dur': 'IA_DWELL_TIME',\n                'word_mean_sacc_dur': 'mean_sacc_dur',\n                'word_peak_sacc_velocity': 'peak_sacc_velocity',\n                'word_total_fix_dur': 'IA_TOTAL_FIXATION_DURATION',\n            }\n        elif data_type == DataType.FIXATIONS:\n            return {\n                'Trial_Index_': 'trial_id',\n                'paragraphid': 'paragraph_id',\n                'speechid': 'speech_id',\n            }\n\n    def get_columns_to_keep(self) -&gt; list:\n        \"\"\"Get list of columns to keep after filtering\"\"\"\n        return []\n\n    def _load_participant_labels(self) -&gt; pd.DataFrame:\n        \"\"\"Load and prepare participant labels\"\"\"\n        participants_df = pd.read_csv(self.data_args.participant_stats_path)\n        participants_df = participants_df.rename(\n            columns={\n                'subj': 'participant_id',\n                'score_reading_comprehension_test': 'RCS_score',\n            }\n        )\n        participants_df['dyslexia'] = (\n            participants_df['dyslexia'].map({'yes': 1, 'no': 0}).astype(int)\n        )\n        return participants_df\n\n    def _process_data_type(\n        self,\n        df: pd.DataFrame,\n        data_type: DataType,\n        participants_df: pd.DataFrame,\n    ) -&gt; pd.DataFrame:\n        \"\"\"Process a single data type (IA or FIXATIONS)\"\"\"\n        # drop instances:\n        # - paragraph_id == -1 are test trials\n        # - missing values in speech_id\n        # - speech_id == 1327 is a practice speech\n        # https://github.com/norahollenstein/copco-processing/blob/0b3bd294a3c09e186c551c085b709423181947f9/extract_features.py#L30\n        df = df[\n            (df.paragraph_id != -1) &amp; (~df['speech_id'].isna() &amp; (df.speech_id != 1327))\n        ].reset_index(drop=True)\n\n        # Add participant IDs\n        if data_type == DataType.IA:\n            df['participant_id'] = (\n                df['source_file'].astype(str).str.zfill(2).apply(lambda x: f'P{x}')\n            )\n        elif data_type == DataType.FIXATIONS:\n            df['participant_id'] = df['RECORDING_SESSION_LABEL'].apply(literal_eval)\n            df = self._map_character_fixations_to_words(df)\n\n        df = self._add_unique_ids(df)\n        df = self._filter_and_log_nulls(df)\n        df['speech_id'] = df['speech_id'].astype('Float64').astype('Int64')\n        df['paragraph_id'] = df['paragraph_id'].astype('Float64').astype('Int64')\n        df['trial_id'] = df['trial_id'].astype('Int64')\n\n        df = self._merge_participant_labels(df, participants_df)\n\n        return df\n\n    def _add_unique_ids(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n        speech_id_str = df['speech_id'].astype(int).astype(str)\n        paragraph_id_str = df['paragraph_id'].astype(int).astype(str)\n        participant_id_str = df['participant_id'].astype(str)\n\n        df['unique_paragraph_id'] = speech_id_str + '_' + paragraph_id_str\n        df['unique_trial_id'] = (\n            participant_id_str + '_' + speech_id_str + '_' + paragraph_id_str\n        )\n        return df\n\n    def _filter_and_log_nulls(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"Filter null values and log warnings\"\"\"\n        if df['speech_id'].isna().any():\n            null_speech_df = df[df['speech_id'].isna()]\n            n_trials = null_speech_df['unique_trial_id'].nunique()\n            n_subjects = null_speech_df['participant_id'].nunique()\n            logger.warning(\n                f'WARNING: {n_trials} trials of {n_subjects} subjects have null values in speech_id'\n            )\n            df = df.dropna(subset=['speech_id'])\n\n        if df['paragraph_id'].isna().any():\n            null_paragraph_df = df[df['paragraph_id'].isna()]\n            n_trials = null_paragraph_df['unique_trial_id'].nunique()\n            logger.warning(\n                f'WARNING: {n_trials} trials have null values in paragraph_id'\n            )\n            df = df.dropna(subset=['paragraph_id'])\n\n        df = df[df['speech_id'] != 'UNDEFINEDnull'].reset_index(drop=True)\n\n        return df\n\n    def _merge_participant_labels(\n        self, df: pd.DataFrame, participants_df: pd.DataFrame\n    ) -&gt; pd.DataFrame:\n        \"\"\"Merge participant labels and validate\"\"\"\n        df = df.merge(\n            participants_df[['participant_id', 'dyslexia', 'RCS_score']],\n            on='participant_id',\n            how='left',\n        )\n\n        if df['dyslexia'].isna().any():\n            n_participants = df[df['dyslexia'].isna()]['participant_id'].nunique()\n            logger.warning(\n                f'WARNING: {n_participants} participants have null values in dyslexia'\n            )\n\n        if df['RCS_score'].isna().any():\n            n_participants = df[df['RCS_score'].isna()]['participant_id'].nunique()\n            logger.warning(\n                f'WARNING: {n_participants} participants have null values in RCS_score'\n            )\n            df['RCS_score'] = df['RCS_score'].fillna(-1)\n\n        return df\n\n    def _map_character_fixations_to_words(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"Map character-level fixations to word-level data\"\"\"\n        # fixations were recorded on character-level interest areas\n        # we need to map them to word-level interest areas\n        # using the provided word2char_IA_mapping.csv file\n        # https://github.com/norahollenstein/copco-processing/blob/main/char2word_mapping.py\n        w2c_df = pd.read_csv(\n            'data/CopCo/labels/word2char_IA_mapping.csv',\n            converters={'characters': literal_eval, 'char_IA_ids': literal_eval},\n        )\n        w2c_df['word'] = w2c_df['word'].str.replace('\\xa0', ' __NBSP__ ', regex=False)\n        w2c_df['word'] = w2c_df['word'].str.split()\n        w2c_df = w2c_df.explode('word', ignore_index=True)\n        w2c_df['word_id'] = w2c_df.groupby(['speechId', 'paragraphId']).cumcount()\n\n        # recalculate character IA IDs\n        w2c_df = w2c_df.groupby(['speechId', 'paragraphId'], group_keys=False).apply(\n            self._add_char_IA_ids\n        )\n\n        logger.info('Mapping character fixations to words...')\n\n        # create lookup tables\n        lookup_id = self._create_lookup(w2c_df, 'wordId')\n        lookup_word = self._create_lookup(w2c_df, 'word')\n\n        df['word_id'] = df.apply(self._find_mapping, axis=1, lookup=lookup_id)\n        df['word'] = df.apply(self._find_mapping, axis=1, lookup=lookup_word)\n\n        logger.info('Finished mapping character fixations to words.')\n        return df\n\n    @staticmethod\n    def _add_char_IA_ids(group: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"Calculate character IA IDs for a group\"\"\"\n        lengths = group['word'].str.len()\n        end_ids = lengths.cumsum()\n        start_ids = end_ids.shift(1, fill_value=0) + 1\n        group['char_IA_ids'] = [\n            list(range(int(start), int(end) + 1))\n            for start, end in zip(start_ids, end_ids)\n        ]\n        return group\n\n    @staticmethod\n    def _create_lookup(w2c: pd.DataFrame, lookup_key: str) -&gt; dict:\n        \"\"\"Create lookup dictionary for efficient mapping\"\"\"\n        lookup = {}\n        for _, row in w2c.iterrows():\n            key = (row['paragraphId'], row['speechId'])\n            if key not in lookup:\n                lookup[key] = []\n            lookup[key].append((row['char_IA_ids'], row[lookup_key]))\n        return lookup\n\n    @staticmethod\n    def _find_mapping(\n        row: pd.Series,\n        lookup: dict[tuple[int, int], list[tuple[list[int], int | str]]],\n    ) -&gt; str | int | None:\n        \"\"\"Find word mapping for a fixation\"\"\"\n        if pd.isna(row['CURRENT_FIX_INTEREST_AREA_LABEL']):\n            return None\n\n        key = (int(row['paragraph_id']), int(row['speech_id']))\n        candidates = lookup.get(key, [])\n\n        idx = int(row['CURRENT_FIX_INTEREST_AREA_INDEX'])\n        for char_ids, word_id in candidates:\n            if idx in char_ids:\n                return word_id\n\n        return None\n\n    def add_ia_report_features_to_fixation_data(\n        self, ia_df: pd.DataFrame, fix_df: pd.DataFrame\n    ) -&gt; tuple[pd.DataFrame, pd.DataFrame]:\n        \"\"\"Merge per-IA (interest-area) features into fixation-level data\"\"\"\n        ia_df = ia_df.rename(\n            columns={\n                Fields.IA_DATA_IA_ID_COL_NAME: Fields.FIXATION_REPORT_IA_ID_COL_NAME\n            }\n        )\n\n        # matching column names with fix_df\n        ia_df = self._prepare_ia_data(ia_df)\n\n        ia_df = self._add_ia_placeholder_columns(ia_df)\n        ia_df = self._compute_text_metrics(ia_df)\n        enriched_fix_df = self._merge_ia_with_fixations(ia_df, fix_df)\n\n        return enriched_fix_df, ia_df\n\n    def _prepare_ia_data(self, ia_df: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"Prepare IA data by processing words and creating derived features\"\"\"\n        # Process words\n        ia_df['word'] = ia_df['word'].str.replace('\\xa0', ' __NBSP__ ', regex=False)\n        ia_df['word'] = ia_df['word'].str.split()\n        ia_df = ia_df.explode('word', ignore_index=True)\n\n        # Create word-level features\n        ia_df['word_id'] = ia_df.groupby('unique_trial_id').cumcount()\n        ia_df['word_length'] = ia_df['word'].str.len()\n        ia_df['total_skip'] = ia_df['IA_FIRST_FIX_DURATION'].isna()\n        ia_df['TRIAL_IA_COUNT'] = ia_df.groupby('unique_trial_id')[\n            'unique_trial_id'\n        ].transform('count')\n        ia_df['TRIAL_IA_COUNT'] = ia_df['TRIAL_IA_COUNT'].fillna(0)\n\n        # Create paragraph text\n        paragraph = ia_df.groupby('unique_trial_id')['word'].apply(' '.join)\n        ia_df['paragraph'] = ia_df['unique_trial_id'].map(paragraph)\n\n        # Add paragraph-level reading time\n        ia_df['PARAGRAPH_RT'] = ia_df.groupby(Fields.UNIQUE_PARAGRAPH_ID)[\n            'IA_DWELL_TIME'\n        ].transform('sum')\n        ia_df['IA_DWELL_TIME_%'] = ia_df.groupby('unique_trial_id')[\n            'IA_DWELL_TIME'\n        ].transform(lambda x: x / np.sum(x))\n\n        return ia_df\n\n    def _add_ia_placeholder_columns(self, ia_df: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"Add placeholder columns for IA features\"\"\"\n        # Define all placeholder columns with their default values\n        placeholder_cols = {\n            'question': '',\n            'IA_SKIP': ia_df['total_skip'].astype(int),\n            'IA_TOP': 0,\n            'IA_LEFT': 0,\n            'IA_RIGHT': 0,\n            'IA_BOTTOM': 0,\n            'IA_REGRESSION_PATH_DURATION': 0,\n            'IA_REGRESSION_IN_COUNT': 0,\n            'IA_REGRESSION_OUT_FULL_COUNT': 0,\n            'IA_REGRESSION_OUT_COUNT': 0,\n            'IA_LAST_RUN_DWELL_TIME': 0,\n            'IA_LAST_RUN_LANDING_POSITION': 0,\n            'IA_SELECTIVE_REGRESSION_PATH_DURATION': 0,\n            'IA_FIRST_FIXATION_VISITED_IA_COUNT': 0,\n            'IA_FIRST_RUN_FIXATION_COUNT': 0,\n            'IA_FIRST_RUN_LANDING_POSITION': 0,\n            'IA_FIRST_FIXATION_DURATION': ia_df['IA_FIRST_FIX_DURATION'],\n            'IA_FIRST_FIX_PROGRESSIVE': 0,\n            'IA_LAST_FIXATION_DURATION': 0,\n            'IA_FIRST_RUN_DWELL_TIME': 0,\n            'IA_LAST_RUN_FIXATION_COUNT': 0,\n            'start_of_line': 0,\n            'end_of_line': 0,\n            'PREVIOUS_FIX_DISTANCE': 0,\n            'IA_FIXATION_%': 0,\n            'IA_RUN_COUNT': 0,\n            'NEXT_SAC_START_X': 0,\n            'NEXT_SAC_START_Y': 0,\n            'NEXT_SAC_END_X': 0,\n            'NEXT_SAC_END_Y': 0,\n            'normalized_ID': 0,\n        }\n\n        return ia_df.assign(**placeholder_cols)\n\n    def _compute_text_metrics(self, ia_df: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"Compute linguistic metrics using surprisal and parsing\"\"\"\n        # Initialize models\n        model_name = 'KennethTM/gpt2-medium-danish'\n        surp_extractor = get_surp_extractor(\n            extractor_type=SurpExtractorType.CAT_CTX_LEFT, model_name=model_name\n        )\n        nlp = spacy.load('da_core_news_sm')\n\n        def process_group(group):\n            \"\"\"Process a single trial group\"\"\"\n            sentence = group.paragraph.iloc[0]\n            metrics = get_metrics(\n                target_text=sentence,\n                surp_extractor=surp_extractor,\n                parsing_model=nlp,\n                parsing_mode='re-tokenize',\n                add_parsing_features=True,\n                language='da',\n            )\n            metrics['unique_trial_id'] = group['unique_trial_id'].iloc[0]\n            metrics['word_id'] = list(group['word_id'])\n            return metrics\n\n        # Process all groups\n        metrics_list = [\n            process_group(group)\n            for _, group in tqdm(\n                ia_df.groupby('unique_trial_id'),\n                desc='Computing text metrics',\n                total=ia_df['unique_trial_id'].nunique(),\n            )\n        ]\n\n        # Combine and merge\n        metrics_df = pd.concat(metrics_list, ignore_index=True)\n        merge_keys = ['unique_trial_id', 'word_id']\n        drop_keys = set(metrics_df.columns) &amp; set(ia_df.columns) - set(merge_keys)\n        ia_df = ia_df.merge(metrics_df.drop(columns=list(drop_keys)), on=merge_keys)\n\n        # Rename columns for consistency\n        column_mapping = {\n            'POS': 'universal_pos',\n            'Length': 'word_length_no_punctuation',\n            'Wordfreq_Frequency': 'wordfreq_frequency',\n            'subtlex_Frequency': 'subtlex_frequency',\n            'Reduced_POS': 'ptb_pos',\n            'Head_word_idx': 'head_word_index',\n            'Dependency_Relation': 'dependency_relation',\n            'Entity': 'entity_type',\n            'Head_word_index': 'head_word_index',\n            'KennethTM/gpt2-medium-danish_Surprisal': 'gpt2_surprisal',\n            'Head_Direction': 'head_direction',\n            'Is_Content_Word': 'is_content_word',\n            'n_Lefts': 'left_dependents_count',\n            'n_Rights': 'right_dependents_count',\n            'Distance2Head': 'distance_to_head',\n        }\n        ia_df = ia_df.rename(columns=column_mapping)\n\n        return ia_df\n\n    def _merge_ia_with_fixations(\n        self, ia_df: pd.DataFrame, fix_df: pd.DataFrame\n    ) -&gt; pd.DataFrame:\n        \"\"\"Merge IA features into fixation data\"\"\"\n        # Add placeholder columns to fixation data\n        fix_placeholder_cols = {\n            'NEXT_SAC_ANGLE': 0,\n            'NEXT_FIX_ANGLE': 0,\n            'NEXT_FIX_DISTANCE': 0,\n            'PREVIOUS_FIX_ANGLE': 0,\n            'CURRENT_FIX_PUPIL': 0,\n            'NEXT_SAC_AVG_VELOCITY': 0,\n        }\n        fix_df = fix_df.assign(**fix_placeholder_cols)\n\n        # Prepare merge keys\n        merge_keys = self.data_args.groupby_columns + [\n            Fields.FIXATION_REPORT_IA_ID_COL_NAME\n        ]\n        dup_cols = set(fix_df.columns) &amp; set(ia_df.columns) - set(merge_keys)\n\n        # Set word_id as IA ID and prepare for merge\n        ia_df[Fields.FIXATION_REPORT_IA_ID_COL_NAME] = ia_df['word_id']\n        _ia_df = ia_df.drop(columns=list(dup_cols))\n\n        # Clean up fixation data\n        if 'normalized_part_ID' in fix_df.columns:\n            if fix_df['normalized_part_ID'].isna().any():\n                logger.info('normalized_part_ID contains NaNs; dropping it.')\n            fix_df = fix_df.drop(columns='normalized_part_ID')\n\n        # Merge\n        enriched_fix_df = fix_df.merge(\n            _ia_df,\n            on=merge_keys,\n            how='left',\n            validate='many_to_one',\n        )\n        enriched_fix_df['CURRENT_FIX_INTEREST_AREA_INDEX'] = enriched_fix_df[\n            'word_id'\n        ].fillna(-1)\n        enriched_fix_df['IA_ID'] = enriched_fix_df['word_id'].fillna(-1)\n        enriched_fix_df['TRIAL_IA_COUNT'] = enriched_fix_df['TRIAL_IA_COUNT'].fillna(0)\n\n        # Add word count per trial\n        num_of_words_in_trials = (\n            _ia_df.groupby(self.data_args.groupby_columns)\n            .size()\n            .rename('num_of_words_in_trial')\n        )\n        enriched_fix_df = enriched_fix_df.merge(\n            num_of_words_in_trials,\n            left_on=self.data_args.groupby_columns,\n            right_index=True,\n            how='left',\n        )\n\n        return enriched_fix_df\n</code></pre>"},{"location":"reference/data/preprocessing/dataset_preprocessing/copco/#data.preprocessing.dataset_preprocessing.copco.CopCoProcessor.add_ia_report_features_to_fixation_data","title":"<code>add_ia_report_features_to_fixation_data(ia_df, fix_df)</code>","text":"<p>Merge per-IA (interest-area) features into fixation-level data</p> Source code in <code>src/data/preprocessing/dataset_preprocessing/copco.py</code> <pre><code>def add_ia_report_features_to_fixation_data(\n    self, ia_df: pd.DataFrame, fix_df: pd.DataFrame\n) -&gt; tuple[pd.DataFrame, pd.DataFrame]:\n    \"\"\"Merge per-IA (interest-area) features into fixation-level data\"\"\"\n    ia_df = ia_df.rename(\n        columns={\n            Fields.IA_DATA_IA_ID_COL_NAME: Fields.FIXATION_REPORT_IA_ID_COL_NAME\n        }\n    )\n\n    # matching column names with fix_df\n    ia_df = self._prepare_ia_data(ia_df)\n\n    ia_df = self._add_ia_placeholder_columns(ia_df)\n    ia_df = self._compute_text_metrics(ia_df)\n    enriched_fix_df = self._merge_ia_with_fixations(ia_df, fix_df)\n\n    return enriched_fix_df, ia_df\n</code></pre>"},{"location":"reference/data/preprocessing/dataset_preprocessing/copco/#data.preprocessing.dataset_preprocessing.copco.CopCoProcessor.dataset_specific_processing","title":"<code>dataset_specific_processing(data_dict)</code>","text":"<p>CopCo-specific processing steps</p> Source code in <code>src/data/preprocessing/dataset_preprocessing/copco.py</code> <pre><code>def dataset_specific_processing(\n    self, data_dict: dict[str, pd.DataFrame]\n) -&gt; dict[str, pd.DataFrame]:\n    \"\"\"CopCo-specific processing steps\"\"\"\n    participants_df = self._load_participant_labels()\n\n    for data_type in [DataType.IA, DataType.FIXATIONS]:\n        if data_type not in data_dict or data_dict[data_type] is None:\n            continue\n\n        df = self._process_data_type(\n            data_dict[data_type], data_type, participants_df\n        )\n        data_dict[data_type] = df\n\n    data_dict['fixations'], data_dict['ia'] = (\n        self.add_ia_report_features_to_fixation_data(\n            data_dict['ia'], data_dict['fixations']\n        )\n    )\n\n    for data_type in [DataType.IA, DataType.FIXATIONS]:\n        data_dict[data_type] = add_missing_features(\n            et_data=data_dict[data_type],\n            trial_groupby_columns=self.data_args.groupby_columns,\n            mode=data_type,\n        )\n\n    data_dict[DataType.TRIAL_LEVEL] = compute_trial_level_features(\n        raw_fixation_data=data_dict[DataType.FIXATIONS],\n        raw_ia_data=data_dict[DataType.IA],\n        trial_groupby_columns=self.data_args.groupby_columns,\n        processed_data_path=self.data_args.processed_data_path,\n    )\n\n    return data_dict\n</code></pre>"},{"location":"reference/data/preprocessing/dataset_preprocessing/copco/#data.preprocessing.dataset_preprocessing.copco.CopCoProcessor.get_column_map","title":"<code>get_column_map(data_type)</code>","text":"<p>Get column mapping for CopCo dataset</p> Source code in <code>src/data/preprocessing/dataset_preprocessing/copco.py</code> <pre><code>def get_column_map(self, data_type: DataType) -&gt; dict:\n    \"\"\"Get column mapping for CopCo dataset\"\"\"\n    if data_type == DataType.IA:\n        return {\n            'char_IA_ids': 'char_IA_ids',\n            'landing_position': 'landing_position',\n            'number_of_fixations': 'IA_FIXATION_COUNT',\n            'paragraphId': 'paragraph_id',\n            'paragraphid': 'paragraph_id',\n            'part': 'part',\n            'sentenceId': 'sentence_id',\n            'speechId': 'speech_id',\n            'speechid': 'speech_id',\n            'trialId': 'trial_id',\n            'word': 'word',\n            'wordId': 'word_id',\n            'word_first_fix_dur': 'IA_FIRST_FIX_DURATION',\n            'word_first_pass_dur': 'IA_FIRST_FIX_DWELL_TIME',\n            'word_go_past_time': 'IA_REGRESSION_OUT_TIME',\n            'word_mean_fix_dur': 'IA_DWELL_TIME',\n            'word_mean_sacc_dur': 'mean_sacc_dur',\n            'word_peak_sacc_velocity': 'peak_sacc_velocity',\n            'word_total_fix_dur': 'IA_TOTAL_FIXATION_DURATION',\n        }\n    elif data_type == DataType.FIXATIONS:\n        return {\n            'Trial_Index_': 'trial_id',\n            'paragraphid': 'paragraph_id',\n            'speechid': 'speech_id',\n        }\n</code></pre>"},{"location":"reference/data/preprocessing/dataset_preprocessing/copco/#data.preprocessing.dataset_preprocessing.copco.CopCoProcessor.get_columns_to_keep","title":"<code>get_columns_to_keep()</code>","text":"<p>Get list of columns to keep after filtering</p> Source code in <code>src/data/preprocessing/dataset_preprocessing/copco.py</code> <pre><code>def get_columns_to_keep(self) -&gt; list:\n    \"\"\"Get list of columns to keep after filtering\"\"\"\n    return []\n</code></pre>"},{"location":"reference/data/preprocessing/dataset_preprocessing/iitbhgc/","title":"iitbhgc","text":""},{"location":"reference/data/preprocessing/dataset_preprocessing/iitbhgc/#data.preprocessing.dataset_preprocessing.iitbhgc.IITBHGCProcessor","title":"<code>IITBHGCProcessor</code>","text":"<p>               Bases: <code>DatasetProcessor</code></p> <p>Processor for IITBHGC dataset</p> Source code in <code>src/data/preprocessing/dataset_preprocessing/iitbhgc.py</code> <pre><code>class IITBHGCProcessor(DatasetProcessor):\n    \"\"\"Processor for IITBHGC dataset\"\"\"\n\n    # Text fixes mapping as class constant for better performance\n    TEXT_FIXES = {\n        3: [('with Andy', 'with__NBWS__Andy')],\n        5: [('watch Jose', 'watch__NBWS__Jose')],\n        9: [('$1,750', '$__NBWS__1,750.00')],\n        25: [('\u00a33', '\u00a3__NBWS__3.00')],\n        26: [('for Virgil', 'for__NBWS__Virgil')],\n        33: [('at FC', 'at__NBWS__FC')],\n        53: [('$5,000', '$__NBWS__5,000.00')],\n        74: [('$20,000', '$__NBWS__20,000.00')],\n        82: [('\u00a3750,000', '\u00a3__NBWS__7,50,000.00')],\n        99: [('$5.3', '$__NBWS__5.30')],\n        102: [('$10', '$__NBWS__10.00'), ('$9', '$__NBWS__9.00')],\n        130: [('$50,000', '$__NBWS__50,000.00')],\n        257: [('$10', '$__NBWS__10.00')],\n        280: [('Claim: Mile Jedinak twisted', 'Claim: Mile Jedinak__NBWS__twisted')],\n        288: [('$2', '$__NBWS__2.00')],\n        298: [('$200', '$__NBWS__200.00')],\n        325: [('$9', '$__NBWS__9.00')],\n        357: [('$1.8', '$__NBWS__1.80'), ('$2.6', '$__NBWS__2.60')],\n        365: [('$25,000', '$__NBWS__25,000.00')],\n        373: [\n            ('$260', '$__NBWS__260.00'),\n            ('$1.7', '$__NBWS__1.70'),\n            ('$1.37', '$__NBWS__1.37'),\n        ],\n        403: [('hour-long', 'hour-long hour-long')],\n        404: [('$2.9', '$__NBWS__2.90')],\n        425: [\n            ('- many', '-__NBWS__many'),\n            ('against Leicester City', 'against__NBWS__Leicester__NBWS__City'),\n            (\"mock Louis van Gaal's\", \"mock Louis van__NBWS__Gaal's\"),\n        ],\n        441: [('$10,000', '$__NBWS__10,000.00')],\n        460: [('$1.6', '$__NBWS__1.60')],\n        468: [('$105', '$__NBWS__105.00')],\n        483: [('(\u00a34,943)', '(\u00a3__NBWS__4,943.00)')],\n        485: [('long-running', 'long-running long-running')],\n    }\n\n    @staticmethod\n    def fix_texts(text: str, text_name: np.int64) -&gt; str:\n        \"\"\"Apply text-specific fixes based on text_name.\"\"\"\n        if text_name in IITBHGCProcessor.TEXT_FIXES:\n            for old, new in IITBHGCProcessor.TEXT_FIXES[text_name]:\n                text = text.replace(old, new)\n        return text\n\n    @staticmethod\n    def init_word_dict(text_strs: list[str], text_aois: list[int]) -&gt; dict:\n        \"\"\"Initialize word dictionary with default values.\"\"\"\n        return {\n            int(word_index): {\n                'IA_LABEL': word.replace('__NBWS__', '\\xa0'),\n                'IA_ID': word_index,\n                **{\n                    key: 0\n                    for key in [\n                        'FFD',\n                        'SFD',\n                        'FD',\n                        'FPRT',\n                        'FRT',\n                        'TFT',\n                        'RRT',\n                        'RPD_inc',\n                        'RPD_exc',\n                        'RBRT',\n                        'Fix',\n                        'FPF',\n                        'RR',\n                        'FPReg',\n                        'TRC_out',\n                        'TRC_in',\n                        'SL_in',\n                        'SL_out',\n                        'TFC',\n                    ]\n                },\n            }\n            for word_index, word in zip(text_aois, text_strs)\n        }\n\n    def compute_word_level_reading_measures(\n        self,\n        fix_df: pd.DataFrame,\n        stim_df: pd.DataFrame,\n    ) -&gt; pd.DataFrame:\n        \"\"\"Compute word-level reading measures from fixation data.\"\"\"\n\n        def process_participant(\n            text_name: np.int64, participant_id: str\n        ) -&gt; pd.DataFrame:\n            try:\n                # Get relevant data slices\n                aoi_df = stim_df[stim_df[Fields.UNIQUE_PARAGRAPH_ID] == text_name]\n                tmp_df = fix_df[fix_df[Fields.UNIQUE_PARAGRAPH_ID] == text_name]\n                fixations_df = tmp_df[\n                    tmp_df[Fields.SUBJECT_ID] == participant_id\n                ].copy()\n\n                if fixations_df.empty:\n                    return pd.DataFrame()\n\n                assert len(fixations_df.label.unique()) == 1\n                label = fixations_df['label'].iloc[0]\n\n                # Add dummy row\n                fixations_df = pd.concat(\n                    [\n                        fixations_df,\n                        pd.DataFrame(\n                            [[0] * len(fixations_df.columns)],\n                            columns=fixations_df.columns,\n                        ),\n                    ],\n                    ignore_index=True,\n                )\n\n                # Process text\n                text = self.fix_texts(aoi_df.paragraph.iloc[0], text_name)\n                text_strs = text.split()\n                text_aois = list(range(len(text_strs)))\n\n                # Initialize word dictionary\n                word_dict = self.init_word_dict(text_strs, text_aois)\n\n                # Process fixations\n                right_most_word = cur_fix_word_idx = next_fix_word_idx = (\n                    next_fix_dur\n                ) = -1\n\n                for _, fixation in fixations_df.iterrows():\n                    # Skip invalid fixations\n                    try:\n                        aoi = int(fixation['CURRENT_FIX_X'])\n                    except (ValueError, TypeError):\n                        continue\n\n                    if (\n                        fixation['IA_LABEL'] == '.'\n                        or fixation['CURRENT_FIX_DURATION'] == 0\n                    ):\n                        continue\n\n                    # Update fixation indices\n                    last_fix_word_idx = cur_fix_word_idx\n                    cur_fix_word_idx = next_fix_word_idx\n                    cur_fix_dur = next_fix_dur\n                    next_fix_word_idx = aoi\n                    next_fix_dur = fixation['CURRENT_FIX_DURATION']\n\n                    # Validate word match\n                    if aoi in word_dict:\n                        self._validate_word_match(\n                            word_dict[aoi]['IA_LABEL'],\n                            fixation['IA_LABEL'],\n                            text_name,\n                            participant_id,\n                            aoi,\n                        )\n                    else:\n                        continue\n\n                    if next_fix_dur == 0:\n                        next_fix_word_idx = cur_fix_word_idx\n\n                    if cur_fix_word_idx == -1:\n                        continue\n\n                    right_most_word = max(right_most_word, cur_fix_word_idx)\n\n                    # Update word statistics\n                    cur_word = word_dict[cur_fix_word_idx]\n                    cur_word['TFT'] += int(cur_fix_dur)\n                    cur_word['TFC'] += 1\n\n                    if cur_word['FD'] == 0:\n                        cur_word['FD'] = int(cur_fix_dur)\n\n                    if right_most_word == cur_fix_word_idx:\n                        if cur_word['TRC_out'] == 0:\n                            cur_word['FPRT'] += int(cur_fix_dur)\n                            if last_fix_word_idx &lt; cur_fix_word_idx:\n                                cur_word['FFD'] += int(cur_fix_dur)\n                    else:\n                        word_dict[right_most_word]['RPD_exc'] += int(cur_fix_dur)\n\n                    if cur_fix_word_idx &lt; last_fix_word_idx:\n                        cur_word['TRC_in'] += 1\n                    if cur_fix_word_idx &gt; next_fix_word_idx:\n                        cur_word['TRC_out'] += 1\n                    if cur_fix_word_idx == right_most_word:\n                        cur_word['RBRT'] += int(cur_fix_dur)\n\n                    if cur_word['FRT'] == 0 and (\n                        cur_fix_word_idx != next_fix_word_idx or next_fix_dur == 0\n                    ):\n                        cur_word['FRT'] = cur_word['TFT']\n\n                    if cur_word['SL_in'] == 0:\n                        cur_word['SL_in'] = cur_fix_word_idx - last_fix_word_idx\n                    if cur_word['SL_out'] == 0:\n                        cur_word['SL_out'] = next_fix_word_idx - cur_fix_word_idx\n\n                # Finalize word measures\n                rows = []\n                for word_index, word_rm in word_dict.items():\n                    if word_rm['FFD'] == word_rm['FPRT']:\n                        word_rm['SFD'] = word_rm['FFD']\n                    word_rm['RRT'] = word_rm['TFT'] - word_rm['FPRT']\n                    word_rm['FPF'] = int(word_rm['FFD'] &gt; 0)\n                    word_rm['RR'] = int(word_rm['RRT'] &gt; 0)\n                    word_rm['FPReg'] = int(word_rm['RPD_exc'] &gt; 0)\n                    word_rm['Fix'] = int(word_rm['TFT'] &gt; 0)\n                    word_rm['RPD_inc'] = word_rm['RPD_exc'] + word_rm['RBRT']\n                    word_rm[Fields.SUBJECT_ID] = participant_id\n                    word_rm[Fields.UNIQUE_PARAGRAPH_ID] = text_name\n                    word_rm['paragraph'] = text\n                    word_rm['word_index'] = word_index\n                    word_rm['label'] = label\n                    rows.append(word_rm)\n\n                return pd.DataFrame(rows)\n\n            except Exception as e:\n                logger.exception(\n                    f'Error processing {text_name} - {participant_id}: {e}'\n                )\n                return pd.DataFrame()\n\n        # Process all participants in parallel-ready structure\n        rm_df_parts = [\n            process_participant(text_name, participant_id)\n            for text_name in fix_df[Fields.UNIQUE_PARAGRAPH_ID].unique()\n            for participant_id in fix_df[\n                fix_df[Fields.UNIQUE_PARAGRAPH_ID] == text_name\n            ][Fields.SUBJECT_ID].unique()\n        ]\n\n        return pd.concat(rm_df_parts, ignore_index=True)\n\n    def _validate_word_match(\n        self,\n        expected: str,\n        actual: str,\n        text_name: np.int64,\n        participant_id: str,\n        aoi: int,\n    ) -&gt; None:\n        \"\"\"Validate word match between expected and actual labels.\"\"\"\n        if expected == actual:\n            return\n\n        # Check for known acceptable mismatches\n        if (\n            actual in expected\n            or expected.strip('(').strip(')')\n            in actual  # problems in their aois vs. paragraph\n            or expected.lower() == actual.lower()  # case differences\n            or text_name\n            in [122, 305]  # known issues in these texts [wrong kommata in numbers]\n        ):\n            logger.info(f'Acceptable mismatch: \"{expected}\" vs \"{actual}\"')\n            return\n\n        if '\\xa0' in actual:\n            logger.warning(\n                f'Non-breaking space in fixation: text={text_name}, participant={participant_id}, aoi={aoi}'\n            )\n\n        logger.warning(\n            f'Mismatch in text {text_name} for participant {participant_id} at AOI {aoi}: '\n            f'expected \"{expected}\", got \"{actual}\"'\n        )\n\n    def get_column_map(self, data_type: DataType) -&gt; dict:\n        \"\"\"Get column mapping for IITBHGC dataset.\"\"\"\n        base_map = {\n            'trial_id': Fields.UNIQUE_PARAGRAPH_ID,\n            'participant_id': Fields.SUBJECT_ID,\n            'fixation_word_ids': 'CURRENT_FIX_X',\n            'fixation_durations': 'CURRENT_FIX_DURATION',\n            'text': 'paragraph',\n            'fixation_seqs': 'CURRENT_FIX_INDEX',\n            'fixation_word_texts': 'IA_LABEL',\n        }\n        return base_map if data_type in [DataType.IA, DataType.FIXATIONS] else {}\n\n    def get_columns_to_keep(self) -&gt; list:\n        \"\"\"Get list of columns to keep after filtering.\"\"\"\n        return []\n\n    def dataset_specific_processing(\n        self, data_dict: dict[str, pd.DataFrame]\n    ) -&gt; dict[str, pd.DataFrame]:\n        \"\"\"IITBHGC-specific processing steps.\"\"\"\n\n        # Process IA and FIXATIONS data\n        for data_type in [DataType.IA, DataType.FIXATIONS]:\n            if data_type not in data_dict or data_dict[data_type] is None:\n                continue\n\n            df = data_dict[data_type]\n\n            # Add unique trial ID\n            df[Fields.UNIQUE_TRIAL_ID] = (\n                df[Fields.SUBJECT_ID].astype(str)\n                + '_'\n                + df[Fields.UNIQUE_PARAGRAPH_ID].astype(str)\n            )\n\n            # Parse list columns\n            list_columns = [\n                'CURRENT_FIX_X',\n                'CURRENT_FIX_DURATION',\n                'CURRENT_FIX_INDEX',\n                'IA_LABEL',\n            ]\n            df[list_columns] = df[list_columns].map(literal_eval)\n            df = df.explode(list_columns).reset_index(drop=True)\n\n            # Clean text\n            df['paragraph'] = (\n                df['paragraph']\n                .str.replace('\u21b5', ' ', regex=False)\n                .str.split()\n                .str.join(' ')\n            )\n            df['label'] = (df['true_labels'] == df['annotator_labels']).astype(int)\n            df['CURRENT_FIX_X'] = df['CURRENT_FIX_X'].apply(\n                lambda x: int(x) - 1 if x != '.' else -1\n            )\n\n            if data_type == DataType.FIXATIONS:\n                df['CURRENT_FIX_Y'] = 0\n                df['CURRENT_FIX_INDEX'] = df['CURRENT_FIX_INDEX'].astype(int)\n\n            data_dict[data_type] = df\n\n        # Compute reading measures\n        rm_df = self.compute_word_level_reading_measures(\n            data_dict['fixations'], data_dict['ia']\n        )\n        data_dict['ia'] = rm_df\n\n        # Add IA features to fixation data\n        logger.info('Adding IA report features to fixation data...')\n        data_dict[DataType.FIXATIONS], data_dict[DataType.IA] = (\n            self.add_ia_report_features_to_fixation_data(\n                data_dict[DataType.IA],\n                data_dict[DataType.FIXATIONS],\n            )\n        )\n\n        # Add missing features\n        for data_type in [DataType.IA, DataType.FIXATIONS]:\n            if data_type == DataType.IA:\n                data_dict['ia']['NEXT_FIX_INTEREST_AREA_INDEX'] = data_dict['ia'][\n                    'word_index'\n                ].shift(-1)\n                data_dict['ia']['PREVIOUS_FIX_INTEREST_AREA_INDEX'] = data_dict['ia'][\n                    'word_index'\n                ].shift(1)\n            else:\n                data_dict['fixations']['NEXT_FIX_INTEREST_AREA_INDEX'] = 0\n                data_dict['fixations']['NEXT_SAC_PEAK_VELOCITY'] = 0\n\n            data_dict[data_type] = add_missing_features(\n                et_data=data_dict[data_type],\n                trial_groupby_columns=self.data_args.groupby_columns,\n                mode=data_type,\n            )\n\n        # Compute trial-level features\n        trial_level_features = compute_trial_level_features(\n            raw_fixation_data=data_dict[DataType.FIXATIONS],\n            raw_ia_data=data_dict[DataType.IA],\n            trial_groupby_columns=self.data_args.groupby_columns,\n            processed_data_path=self.data_args.processed_data_path,\n        )\n        data_dict[DataType.TRIAL_LEVEL] = trial_level_features\n\n        return data_dict\n\n    def add_ia_report_features_to_fixation_data(\n        self, ia_df: pd.DataFrame, fix_df: pd.DataFrame\n    ) -&gt; tuple[pd.DataFrame, pd.DataFrame]:\n        \"\"\"Merge per-IA features into fixation-level data.\"\"\"\n\n        # Remove duplicates from groupby columns\n        self.data_args.groupby_columns = list(\n            dict.fromkeys(self.data_args.groupby_columns)\n        )\n\n        # Rename IA ID column\n        ia_df = ia_df.rename(\n            columns={\n                Fields.IA_DATA_IA_ID_COL_NAME: Fields.FIXATION_REPORT_IA_ID_COL_NAME\n            }\n        )\n\n        # Add computed columns\n        ia_df['unique_trial_id'] = (\n            ia_df['participant_id'].astype(str)\n            + '_'\n            + ia_df['unique_paragraph_id'].astype(str)\n        )\n        ia_df['word_length'] = ia_df['IA_LABEL'].str.len()\n        ia_df['TRIAL_IA_COUNT'] = ia_df.groupby('unique_trial_id')[\n            'unique_trial_id'\n        ].transform('count')\n\n        surp_extractor = get_surp_extractor(\n            extractor_type=SurpExtractorType.CAT_CTX_LEFT, model_name='gpt2'\n        )\n        nlp = spacy.load('en_core_web_sm')\n\n        # Process metrics\n        def process_group(group):\n            sentence = group.iloc[0].paragraph\n            metrics = get_metrics(\n                target_text=sentence,\n                surp_extractor=surp_extractor,\n                parsing_model=nlp,\n                parsing_mode='re-tokenize',\n                add_parsing_features=True,\n                language='en',\n            )\n            metrics['unique_paragraph_id'] = group['unique_paragraph_id'].iloc[0]\n            metrics[Fields.FIXATION_REPORT_IA_ID_COL_NAME] = metrics['Token_idx']\n            return metrics\n\n        metrics_list = [\n            process_group(group)\n            for _, group in tqdm(\n                ia_df.groupby(Fields.UNIQUE_PARAGRAPH_ID), desc='Processing metrics'\n            )\n        ]\n        metrics_df = pd.concat(metrics_list, ignore_index=True)\n\n        # Merge metrics\n        ia_df[Fields.UNIQUE_TRIAL_ID] = (\n            ia_df[Fields.SUBJECT_ID].astype(str)\n            + '_'\n            + ia_df[Fields.UNIQUE_PARAGRAPH_ID].astype(str)\n        )\n        merge_keys = {'unique_paragraph_id', Fields.FIXATION_REPORT_IA_ID_COL_NAME}\n        drop_keys = (set(metrics_df.columns) &amp; set(ia_df.columns)) - merge_keys\n\n        ia_df['CURRENT_FIX_INTEREST_AREA_INDEX'] = ia_df['word_index']\n        ia_df = ia_df.merge(\n            metrics_df.drop(columns=list(drop_keys)), on=list(merge_keys), how='left'\n        )\n\n        # Rename columns\n        column_renames = {\n            'POS': 'universal_pos',\n            'Length': 'word_length_no_punctuation',\n            'Wordfreq_Frequency': 'wordfreq_frequency',\n            'subtlex_Frequency': 'subtlex_frequency',\n            'Reduced_POS': 'ptb_pos',\n            'Head_word_idx': 'head_word_index',\n            'Dependency_Relation': 'dependency_relation',\n            'Entity': 'entity_type',\n            'gpt2_Surprisal': 'gpt2_surprisal',\n            'gpt2': 'gpt2_surprisal',\n            'Head_Direction': 'head_direction',\n            'Is_Content_Word': 'is_content_word',\n            'n_Lefts': 'left_dependents_count',\n            'n_Rights': 'right_dependents_count',\n            'Distance2Head': 'distance_to_head',\n        }\n        ia_df = ia_df.rename(columns=column_renames)\n\n        # Add default columns efficiently\n        zero_columns = [\n            'start_of_line',\n            'end_of_line',\n            'IA_LAST_FIXATION_DURATION',\n            'IA_LAST_RUN_DWELL_TIME',\n            'IA_SELECTIVE_REGRESSION_PATH_DURATION',\n            'IA_FIRST_FIXATION_VISITED_IA_COUNT',\n            'IA_LEFT',\n            'IA_RIGHT',\n            'IA_TOP',\n            'IA_BOTTOM',\n            'IA_REGRESSION_PATH_DURATION',\n            'IA_REGRESSION_OUT_COUNT',\n            'IA_REGRESSION_IN_COUNT',\n            'IA_FIRST_FIX_PROGRESSIVE',\n            'normalized_ID',\n            'IA_FIRST_RUN_FIXATION_COUNT',\n            'IA_LAST_RUN_FIXATION_COUNT',\n        ]\n        ia_df[zero_columns] = 0\n\n        # Computed columns\n        ia_df['IA_FIRST_RUN_DWELL_TIME'] = ia_df['FPRT']\n        ia_df['IA_FIRST_RUN_FIXATION_DURATION'] = ia_df['FPRT']\n        ia_df['IA_DWELL_TIME'] = ia_df['FD']\n        ia_df['IA_DWELL_TIME_%'] = ia_df.groupby('unique_trial_id')[\n            'IA_DWELL_TIME'\n        ].transform(lambda x: x / x.sum() if x.sum() &gt; 0 else 0)\n        ia_df['PARAGRAPH_RT'] = ia_df.groupby(Fields.UNIQUE_PARAGRAPH_ID)[\n            'IA_DWELL_TIME'\n        ].transform('sum')\n        ia_df['IA_SKIP'] = (ia_df['Fix'] &gt; 0).astype(int)\n        ia_df['total_skip'] = (ia_df['Fix'] &gt; 0).astype(int)\n        ia_df['IA_FIXATION_COUNT'] = ia_df['TFC']\n        ia_df['IA_FIXATION_%'] = ia_df.groupby('unique_trial_id')[\n            'IA_FIXATION_COUNT'\n        ].transform(lambda x: x / np.sum(x))\n        ia_df['IA_FIRST_FIXATION_DURATION'] = ia_df['FFD']\n        ia_df['IA_SINGLE_FIXATION_DURATION'] = ia_df['SFD']\n        ia_df['IA_RUN_COUNT'] = ia_df['TFC']\n        ia_df['IA_REGRESSION_OUT_FULL_COUNT'] = ia_df['TRC_out']\n\n        # Fixation defaults\n        fix_df['CURRENT_FIX_PUPIL'] = 0\n        fix_df['CURRENT_FIX_NEAREST_INTEREST_AREA_DISTANCE'] = (\n            fix_df['IA_LABEL'] == '.'\n        ).astype(int)\n        fix_df[['NEXT_SAC_DURATION', 'NEXT_SAC_AVG_VELOCITY', 'NEXT_SAC_AMPLITUDE']] = 0\n        fix_df['CURRENT_FIX_INTEREST_AREA_INDEX'] = fix_df['CURRENT_FIX_X'].fillna(-1)\n\n        # IA defaults for spatial/angular features\n        spatial_columns = [\n            'NEXT_SAC_START_X',\n            'NEXT_SAC_END_X',\n            'NEXT_SAC_END_Y',\n            'NEXT_SAC_START_Y',\n            'PREVIOUS_FIX_DISTANCE',\n            'NEXT_SAC_ANGLE',\n            'NEXT_FIX_ANGLE',\n            'NEXT_FIX_DISTANCE',\n            'PREVIOUS_FIX_ANGLE',\n        ]\n        ia_df[spatial_columns] = 0\n\n        # Merge fixations with IA features\n        merge_keys = set(\n            self.data_args.groupby_columns + [Fields.FIXATION_REPORT_IA_ID_COL_NAME]\n        )\n        dup_cols = (set(fix_df.columns) &amp; set(ia_df.columns)) - merge_keys\n        _ia_df = ia_df.drop(columns=list(dup_cols))\n\n        if 'normalized_part_ID' in fix_df.columns:\n            fix_df = fix_df.drop(columns='normalized_part_ID')\n\n        enriched_fix_df = fix_df.merge(\n            _ia_df,\n            on=list(merge_keys),\n            how='left',\n            validate='many_to_one',\n        )\n\n        # Add word count\n        num_of_words_in_trials = ia_df.groupby(self.data_args.groupby_columns).size()\n        num_of_words_in_trials.name = 'num_of_words_in_trial'\n        enriched_fix_df = enriched_fix_df.merge(\n            num_of_words_in_trials,\n            on=self.data_args.groupby_columns,\n            how='left',\n        )\n\n        return enriched_fix_df, ia_df\n</code></pre>"},{"location":"reference/data/preprocessing/dataset_preprocessing/iitbhgc/#data.preprocessing.dataset_preprocessing.iitbhgc.IITBHGCProcessor.add_ia_report_features_to_fixation_data","title":"<code>add_ia_report_features_to_fixation_data(ia_df, fix_df)</code>","text":"<p>Merge per-IA features into fixation-level data.</p> Source code in <code>src/data/preprocessing/dataset_preprocessing/iitbhgc.py</code> <pre><code>def add_ia_report_features_to_fixation_data(\n    self, ia_df: pd.DataFrame, fix_df: pd.DataFrame\n) -&gt; tuple[pd.DataFrame, pd.DataFrame]:\n    \"\"\"Merge per-IA features into fixation-level data.\"\"\"\n\n    # Remove duplicates from groupby columns\n    self.data_args.groupby_columns = list(\n        dict.fromkeys(self.data_args.groupby_columns)\n    )\n\n    # Rename IA ID column\n    ia_df = ia_df.rename(\n        columns={\n            Fields.IA_DATA_IA_ID_COL_NAME: Fields.FIXATION_REPORT_IA_ID_COL_NAME\n        }\n    )\n\n    # Add computed columns\n    ia_df['unique_trial_id'] = (\n        ia_df['participant_id'].astype(str)\n        + '_'\n        + ia_df['unique_paragraph_id'].astype(str)\n    )\n    ia_df['word_length'] = ia_df['IA_LABEL'].str.len()\n    ia_df['TRIAL_IA_COUNT'] = ia_df.groupby('unique_trial_id')[\n        'unique_trial_id'\n    ].transform('count')\n\n    surp_extractor = get_surp_extractor(\n        extractor_type=SurpExtractorType.CAT_CTX_LEFT, model_name='gpt2'\n    )\n    nlp = spacy.load('en_core_web_sm')\n\n    # Process metrics\n    def process_group(group):\n        sentence = group.iloc[0].paragraph\n        metrics = get_metrics(\n            target_text=sentence,\n            surp_extractor=surp_extractor,\n            parsing_model=nlp,\n            parsing_mode='re-tokenize',\n            add_parsing_features=True,\n            language='en',\n        )\n        metrics['unique_paragraph_id'] = group['unique_paragraph_id'].iloc[0]\n        metrics[Fields.FIXATION_REPORT_IA_ID_COL_NAME] = metrics['Token_idx']\n        return metrics\n\n    metrics_list = [\n        process_group(group)\n        for _, group in tqdm(\n            ia_df.groupby(Fields.UNIQUE_PARAGRAPH_ID), desc='Processing metrics'\n        )\n    ]\n    metrics_df = pd.concat(metrics_list, ignore_index=True)\n\n    # Merge metrics\n    ia_df[Fields.UNIQUE_TRIAL_ID] = (\n        ia_df[Fields.SUBJECT_ID].astype(str)\n        + '_'\n        + ia_df[Fields.UNIQUE_PARAGRAPH_ID].astype(str)\n    )\n    merge_keys = {'unique_paragraph_id', Fields.FIXATION_REPORT_IA_ID_COL_NAME}\n    drop_keys = (set(metrics_df.columns) &amp; set(ia_df.columns)) - merge_keys\n\n    ia_df['CURRENT_FIX_INTEREST_AREA_INDEX'] = ia_df['word_index']\n    ia_df = ia_df.merge(\n        metrics_df.drop(columns=list(drop_keys)), on=list(merge_keys), how='left'\n    )\n\n    # Rename columns\n    column_renames = {\n        'POS': 'universal_pos',\n        'Length': 'word_length_no_punctuation',\n        'Wordfreq_Frequency': 'wordfreq_frequency',\n        'subtlex_Frequency': 'subtlex_frequency',\n        'Reduced_POS': 'ptb_pos',\n        'Head_word_idx': 'head_word_index',\n        'Dependency_Relation': 'dependency_relation',\n        'Entity': 'entity_type',\n        'gpt2_Surprisal': 'gpt2_surprisal',\n        'gpt2': 'gpt2_surprisal',\n        'Head_Direction': 'head_direction',\n        'Is_Content_Word': 'is_content_word',\n        'n_Lefts': 'left_dependents_count',\n        'n_Rights': 'right_dependents_count',\n        'Distance2Head': 'distance_to_head',\n    }\n    ia_df = ia_df.rename(columns=column_renames)\n\n    # Add default columns efficiently\n    zero_columns = [\n        'start_of_line',\n        'end_of_line',\n        'IA_LAST_FIXATION_DURATION',\n        'IA_LAST_RUN_DWELL_TIME',\n        'IA_SELECTIVE_REGRESSION_PATH_DURATION',\n        'IA_FIRST_FIXATION_VISITED_IA_COUNT',\n        'IA_LEFT',\n        'IA_RIGHT',\n        'IA_TOP',\n        'IA_BOTTOM',\n        'IA_REGRESSION_PATH_DURATION',\n        'IA_REGRESSION_OUT_COUNT',\n        'IA_REGRESSION_IN_COUNT',\n        'IA_FIRST_FIX_PROGRESSIVE',\n        'normalized_ID',\n        'IA_FIRST_RUN_FIXATION_COUNT',\n        'IA_LAST_RUN_FIXATION_COUNT',\n    ]\n    ia_df[zero_columns] = 0\n\n    # Computed columns\n    ia_df['IA_FIRST_RUN_DWELL_TIME'] = ia_df['FPRT']\n    ia_df['IA_FIRST_RUN_FIXATION_DURATION'] = ia_df['FPRT']\n    ia_df['IA_DWELL_TIME'] = ia_df['FD']\n    ia_df['IA_DWELL_TIME_%'] = ia_df.groupby('unique_trial_id')[\n        'IA_DWELL_TIME'\n    ].transform(lambda x: x / x.sum() if x.sum() &gt; 0 else 0)\n    ia_df['PARAGRAPH_RT'] = ia_df.groupby(Fields.UNIQUE_PARAGRAPH_ID)[\n        'IA_DWELL_TIME'\n    ].transform('sum')\n    ia_df['IA_SKIP'] = (ia_df['Fix'] &gt; 0).astype(int)\n    ia_df['total_skip'] = (ia_df['Fix'] &gt; 0).astype(int)\n    ia_df['IA_FIXATION_COUNT'] = ia_df['TFC']\n    ia_df['IA_FIXATION_%'] = ia_df.groupby('unique_trial_id')[\n        'IA_FIXATION_COUNT'\n    ].transform(lambda x: x / np.sum(x))\n    ia_df['IA_FIRST_FIXATION_DURATION'] = ia_df['FFD']\n    ia_df['IA_SINGLE_FIXATION_DURATION'] = ia_df['SFD']\n    ia_df['IA_RUN_COUNT'] = ia_df['TFC']\n    ia_df['IA_REGRESSION_OUT_FULL_COUNT'] = ia_df['TRC_out']\n\n    # Fixation defaults\n    fix_df['CURRENT_FIX_PUPIL'] = 0\n    fix_df['CURRENT_FIX_NEAREST_INTEREST_AREA_DISTANCE'] = (\n        fix_df['IA_LABEL'] == '.'\n    ).astype(int)\n    fix_df[['NEXT_SAC_DURATION', 'NEXT_SAC_AVG_VELOCITY', 'NEXT_SAC_AMPLITUDE']] = 0\n    fix_df['CURRENT_FIX_INTEREST_AREA_INDEX'] = fix_df['CURRENT_FIX_X'].fillna(-1)\n\n    # IA defaults for spatial/angular features\n    spatial_columns = [\n        'NEXT_SAC_START_X',\n        'NEXT_SAC_END_X',\n        'NEXT_SAC_END_Y',\n        'NEXT_SAC_START_Y',\n        'PREVIOUS_FIX_DISTANCE',\n        'NEXT_SAC_ANGLE',\n        'NEXT_FIX_ANGLE',\n        'NEXT_FIX_DISTANCE',\n        'PREVIOUS_FIX_ANGLE',\n    ]\n    ia_df[spatial_columns] = 0\n\n    # Merge fixations with IA features\n    merge_keys = set(\n        self.data_args.groupby_columns + [Fields.FIXATION_REPORT_IA_ID_COL_NAME]\n    )\n    dup_cols = (set(fix_df.columns) &amp; set(ia_df.columns)) - merge_keys\n    _ia_df = ia_df.drop(columns=list(dup_cols))\n\n    if 'normalized_part_ID' in fix_df.columns:\n        fix_df = fix_df.drop(columns='normalized_part_ID')\n\n    enriched_fix_df = fix_df.merge(\n        _ia_df,\n        on=list(merge_keys),\n        how='left',\n        validate='many_to_one',\n    )\n\n    # Add word count\n    num_of_words_in_trials = ia_df.groupby(self.data_args.groupby_columns).size()\n    num_of_words_in_trials.name = 'num_of_words_in_trial'\n    enriched_fix_df = enriched_fix_df.merge(\n        num_of_words_in_trials,\n        on=self.data_args.groupby_columns,\n        how='left',\n    )\n\n    return enriched_fix_df, ia_df\n</code></pre>"},{"location":"reference/data/preprocessing/dataset_preprocessing/iitbhgc/#data.preprocessing.dataset_preprocessing.iitbhgc.IITBHGCProcessor.compute_word_level_reading_measures","title":"<code>compute_word_level_reading_measures(fix_df, stim_df)</code>","text":"<p>Compute word-level reading measures from fixation data.</p> Source code in <code>src/data/preprocessing/dataset_preprocessing/iitbhgc.py</code> <pre><code>def compute_word_level_reading_measures(\n    self,\n    fix_df: pd.DataFrame,\n    stim_df: pd.DataFrame,\n) -&gt; pd.DataFrame:\n    \"\"\"Compute word-level reading measures from fixation data.\"\"\"\n\n    def process_participant(\n        text_name: np.int64, participant_id: str\n    ) -&gt; pd.DataFrame:\n        try:\n            # Get relevant data slices\n            aoi_df = stim_df[stim_df[Fields.UNIQUE_PARAGRAPH_ID] == text_name]\n            tmp_df = fix_df[fix_df[Fields.UNIQUE_PARAGRAPH_ID] == text_name]\n            fixations_df = tmp_df[\n                tmp_df[Fields.SUBJECT_ID] == participant_id\n            ].copy()\n\n            if fixations_df.empty:\n                return pd.DataFrame()\n\n            assert len(fixations_df.label.unique()) == 1\n            label = fixations_df['label'].iloc[0]\n\n            # Add dummy row\n            fixations_df = pd.concat(\n                [\n                    fixations_df,\n                    pd.DataFrame(\n                        [[0] * len(fixations_df.columns)],\n                        columns=fixations_df.columns,\n                    ),\n                ],\n                ignore_index=True,\n            )\n\n            # Process text\n            text = self.fix_texts(aoi_df.paragraph.iloc[0], text_name)\n            text_strs = text.split()\n            text_aois = list(range(len(text_strs)))\n\n            # Initialize word dictionary\n            word_dict = self.init_word_dict(text_strs, text_aois)\n\n            # Process fixations\n            right_most_word = cur_fix_word_idx = next_fix_word_idx = (\n                next_fix_dur\n            ) = -1\n\n            for _, fixation in fixations_df.iterrows():\n                # Skip invalid fixations\n                try:\n                    aoi = int(fixation['CURRENT_FIX_X'])\n                except (ValueError, TypeError):\n                    continue\n\n                if (\n                    fixation['IA_LABEL'] == '.'\n                    or fixation['CURRENT_FIX_DURATION'] == 0\n                ):\n                    continue\n\n                # Update fixation indices\n                last_fix_word_idx = cur_fix_word_idx\n                cur_fix_word_idx = next_fix_word_idx\n                cur_fix_dur = next_fix_dur\n                next_fix_word_idx = aoi\n                next_fix_dur = fixation['CURRENT_FIX_DURATION']\n\n                # Validate word match\n                if aoi in word_dict:\n                    self._validate_word_match(\n                        word_dict[aoi]['IA_LABEL'],\n                        fixation['IA_LABEL'],\n                        text_name,\n                        participant_id,\n                        aoi,\n                    )\n                else:\n                    continue\n\n                if next_fix_dur == 0:\n                    next_fix_word_idx = cur_fix_word_idx\n\n                if cur_fix_word_idx == -1:\n                    continue\n\n                right_most_word = max(right_most_word, cur_fix_word_idx)\n\n                # Update word statistics\n                cur_word = word_dict[cur_fix_word_idx]\n                cur_word['TFT'] += int(cur_fix_dur)\n                cur_word['TFC'] += 1\n\n                if cur_word['FD'] == 0:\n                    cur_word['FD'] = int(cur_fix_dur)\n\n                if right_most_word == cur_fix_word_idx:\n                    if cur_word['TRC_out'] == 0:\n                        cur_word['FPRT'] += int(cur_fix_dur)\n                        if last_fix_word_idx &lt; cur_fix_word_idx:\n                            cur_word['FFD'] += int(cur_fix_dur)\n                else:\n                    word_dict[right_most_word]['RPD_exc'] += int(cur_fix_dur)\n\n                if cur_fix_word_idx &lt; last_fix_word_idx:\n                    cur_word['TRC_in'] += 1\n                if cur_fix_word_idx &gt; next_fix_word_idx:\n                    cur_word['TRC_out'] += 1\n                if cur_fix_word_idx == right_most_word:\n                    cur_word['RBRT'] += int(cur_fix_dur)\n\n                if cur_word['FRT'] == 0 and (\n                    cur_fix_word_idx != next_fix_word_idx or next_fix_dur == 0\n                ):\n                    cur_word['FRT'] = cur_word['TFT']\n\n                if cur_word['SL_in'] == 0:\n                    cur_word['SL_in'] = cur_fix_word_idx - last_fix_word_idx\n                if cur_word['SL_out'] == 0:\n                    cur_word['SL_out'] = next_fix_word_idx - cur_fix_word_idx\n\n            # Finalize word measures\n            rows = []\n            for word_index, word_rm in word_dict.items():\n                if word_rm['FFD'] == word_rm['FPRT']:\n                    word_rm['SFD'] = word_rm['FFD']\n                word_rm['RRT'] = word_rm['TFT'] - word_rm['FPRT']\n                word_rm['FPF'] = int(word_rm['FFD'] &gt; 0)\n                word_rm['RR'] = int(word_rm['RRT'] &gt; 0)\n                word_rm['FPReg'] = int(word_rm['RPD_exc'] &gt; 0)\n                word_rm['Fix'] = int(word_rm['TFT'] &gt; 0)\n                word_rm['RPD_inc'] = word_rm['RPD_exc'] + word_rm['RBRT']\n                word_rm[Fields.SUBJECT_ID] = participant_id\n                word_rm[Fields.UNIQUE_PARAGRAPH_ID] = text_name\n                word_rm['paragraph'] = text\n                word_rm['word_index'] = word_index\n                word_rm['label'] = label\n                rows.append(word_rm)\n\n            return pd.DataFrame(rows)\n\n        except Exception as e:\n            logger.exception(\n                f'Error processing {text_name} - {participant_id}: {e}'\n            )\n            return pd.DataFrame()\n\n    # Process all participants in parallel-ready structure\n    rm_df_parts = [\n        process_participant(text_name, participant_id)\n        for text_name in fix_df[Fields.UNIQUE_PARAGRAPH_ID].unique()\n        for participant_id in fix_df[\n            fix_df[Fields.UNIQUE_PARAGRAPH_ID] == text_name\n        ][Fields.SUBJECT_ID].unique()\n    ]\n\n    return pd.concat(rm_df_parts, ignore_index=True)\n</code></pre>"},{"location":"reference/data/preprocessing/dataset_preprocessing/iitbhgc/#data.preprocessing.dataset_preprocessing.iitbhgc.IITBHGCProcessor.dataset_specific_processing","title":"<code>dataset_specific_processing(data_dict)</code>","text":"<p>IITBHGC-specific processing steps.</p> Source code in <code>src/data/preprocessing/dataset_preprocessing/iitbhgc.py</code> <pre><code>def dataset_specific_processing(\n    self, data_dict: dict[str, pd.DataFrame]\n) -&gt; dict[str, pd.DataFrame]:\n    \"\"\"IITBHGC-specific processing steps.\"\"\"\n\n    # Process IA and FIXATIONS data\n    for data_type in [DataType.IA, DataType.FIXATIONS]:\n        if data_type not in data_dict or data_dict[data_type] is None:\n            continue\n\n        df = data_dict[data_type]\n\n        # Add unique trial ID\n        df[Fields.UNIQUE_TRIAL_ID] = (\n            df[Fields.SUBJECT_ID].astype(str)\n            + '_'\n            + df[Fields.UNIQUE_PARAGRAPH_ID].astype(str)\n        )\n\n        # Parse list columns\n        list_columns = [\n            'CURRENT_FIX_X',\n            'CURRENT_FIX_DURATION',\n            'CURRENT_FIX_INDEX',\n            'IA_LABEL',\n        ]\n        df[list_columns] = df[list_columns].map(literal_eval)\n        df = df.explode(list_columns).reset_index(drop=True)\n\n        # Clean text\n        df['paragraph'] = (\n            df['paragraph']\n            .str.replace('\u21b5', ' ', regex=False)\n            .str.split()\n            .str.join(' ')\n        )\n        df['label'] = (df['true_labels'] == df['annotator_labels']).astype(int)\n        df['CURRENT_FIX_X'] = df['CURRENT_FIX_X'].apply(\n            lambda x: int(x) - 1 if x != '.' else -1\n        )\n\n        if data_type == DataType.FIXATIONS:\n            df['CURRENT_FIX_Y'] = 0\n            df['CURRENT_FIX_INDEX'] = df['CURRENT_FIX_INDEX'].astype(int)\n\n        data_dict[data_type] = df\n\n    # Compute reading measures\n    rm_df = self.compute_word_level_reading_measures(\n        data_dict['fixations'], data_dict['ia']\n    )\n    data_dict['ia'] = rm_df\n\n    # Add IA features to fixation data\n    logger.info('Adding IA report features to fixation data...')\n    data_dict[DataType.FIXATIONS], data_dict[DataType.IA] = (\n        self.add_ia_report_features_to_fixation_data(\n            data_dict[DataType.IA],\n            data_dict[DataType.FIXATIONS],\n        )\n    )\n\n    # Add missing features\n    for data_type in [DataType.IA, DataType.FIXATIONS]:\n        if data_type == DataType.IA:\n            data_dict['ia']['NEXT_FIX_INTEREST_AREA_INDEX'] = data_dict['ia'][\n                'word_index'\n            ].shift(-1)\n            data_dict['ia']['PREVIOUS_FIX_INTEREST_AREA_INDEX'] = data_dict['ia'][\n                'word_index'\n            ].shift(1)\n        else:\n            data_dict['fixations']['NEXT_FIX_INTEREST_AREA_INDEX'] = 0\n            data_dict['fixations']['NEXT_SAC_PEAK_VELOCITY'] = 0\n\n        data_dict[data_type] = add_missing_features(\n            et_data=data_dict[data_type],\n            trial_groupby_columns=self.data_args.groupby_columns,\n            mode=data_type,\n        )\n\n    # Compute trial-level features\n    trial_level_features = compute_trial_level_features(\n        raw_fixation_data=data_dict[DataType.FIXATIONS],\n        raw_ia_data=data_dict[DataType.IA],\n        trial_groupby_columns=self.data_args.groupby_columns,\n        processed_data_path=self.data_args.processed_data_path,\n    )\n    data_dict[DataType.TRIAL_LEVEL] = trial_level_features\n\n    return data_dict\n</code></pre>"},{"location":"reference/data/preprocessing/dataset_preprocessing/iitbhgc/#data.preprocessing.dataset_preprocessing.iitbhgc.IITBHGCProcessor.fix_texts","title":"<code>fix_texts(text, text_name)</code>  <code>staticmethod</code>","text":"<p>Apply text-specific fixes based on text_name.</p> Source code in <code>src/data/preprocessing/dataset_preprocessing/iitbhgc.py</code> <pre><code>@staticmethod\ndef fix_texts(text: str, text_name: np.int64) -&gt; str:\n    \"\"\"Apply text-specific fixes based on text_name.\"\"\"\n    if text_name in IITBHGCProcessor.TEXT_FIXES:\n        for old, new in IITBHGCProcessor.TEXT_FIXES[text_name]:\n            text = text.replace(old, new)\n    return text\n</code></pre>"},{"location":"reference/data/preprocessing/dataset_preprocessing/iitbhgc/#data.preprocessing.dataset_preprocessing.iitbhgc.IITBHGCProcessor.get_column_map","title":"<code>get_column_map(data_type)</code>","text":"<p>Get column mapping for IITBHGC dataset.</p> Source code in <code>src/data/preprocessing/dataset_preprocessing/iitbhgc.py</code> <pre><code>def get_column_map(self, data_type: DataType) -&gt; dict:\n    \"\"\"Get column mapping for IITBHGC dataset.\"\"\"\n    base_map = {\n        'trial_id': Fields.UNIQUE_PARAGRAPH_ID,\n        'participant_id': Fields.SUBJECT_ID,\n        'fixation_word_ids': 'CURRENT_FIX_X',\n        'fixation_durations': 'CURRENT_FIX_DURATION',\n        'text': 'paragraph',\n        'fixation_seqs': 'CURRENT_FIX_INDEX',\n        'fixation_word_texts': 'IA_LABEL',\n    }\n    return base_map if data_type in [DataType.IA, DataType.FIXATIONS] else {}\n</code></pre>"},{"location":"reference/data/preprocessing/dataset_preprocessing/iitbhgc/#data.preprocessing.dataset_preprocessing.iitbhgc.IITBHGCProcessor.get_columns_to_keep","title":"<code>get_columns_to_keep()</code>","text":"<p>Get list of columns to keep after filtering.</p> Source code in <code>src/data/preprocessing/dataset_preprocessing/iitbhgc.py</code> <pre><code>def get_columns_to_keep(self) -&gt; list:\n    \"\"\"Get list of columns to keep after filtering.\"\"\"\n    return []\n</code></pre>"},{"location":"reference/data/preprocessing/dataset_preprocessing/iitbhgc/#data.preprocessing.dataset_preprocessing.iitbhgc.IITBHGCProcessor.init_word_dict","title":"<code>init_word_dict(text_strs, text_aois)</code>  <code>staticmethod</code>","text":"<p>Initialize word dictionary with default values.</p> Source code in <code>src/data/preprocessing/dataset_preprocessing/iitbhgc.py</code> <pre><code>@staticmethod\ndef init_word_dict(text_strs: list[str], text_aois: list[int]) -&gt; dict:\n    \"\"\"Initialize word dictionary with default values.\"\"\"\n    return {\n        int(word_index): {\n            'IA_LABEL': word.replace('__NBWS__', '\\xa0'),\n            'IA_ID': word_index,\n            **{\n                key: 0\n                for key in [\n                    'FFD',\n                    'SFD',\n                    'FD',\n                    'FPRT',\n                    'FRT',\n                    'TFT',\n                    'RRT',\n                    'RPD_inc',\n                    'RPD_exc',\n                    'RBRT',\n                    'Fix',\n                    'FPF',\n                    'RR',\n                    'FPReg',\n                    'TRC_out',\n                    'TRC_in',\n                    'SL_in',\n                    'SL_out',\n                    'TFC',\n                ]\n            },\n        }\n        for word_index, word in zip(text_aois, text_strs)\n    }\n</code></pre>"},{"location":"reference/data/preprocessing/dataset_preprocessing/meco/","title":"meco","text":""},{"location":"reference/data/preprocessing/dataset_preprocessing/meco/#data.preprocessing.dataset_preprocessing.meco.MECOProcessor","title":"<code>MECOProcessor</code>","text":"<p>               Bases: <code>DatasetProcessor</code></p> <p>Processor MECO dataset</p> Source code in <code>src/data/preprocessing/dataset_preprocessing/meco.py</code> <pre><code>class MECOProcessor(DatasetProcessor):\n    \"\"\"Processor MECO dataset\"\"\"\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self._nlp = None\n        self._surp_extractor = None\n\n    def dataset_specific_processing(\n        self, data_dict: dict[str, pd.DataFrame]\n    ) -&gt; dict[str, pd.DataFrame]:\n        \"\"\"MECO-specific processing steps\"\"\"\n\n        # add unique trial IDs and merge labels\n        labels = self._load_labels().rename(columns={'uniform_id': Fields.SUBJECT_ID})\n\n        for data_type in [DataType.IA, DataType.FIXATIONS]:\n            df = data_dict[data_type]\n\n            df[Fields.UNIQUE_TRIAL_ID] = (\n                df[Fields.SUBJECT_ID].astype(str)\n                + '_'\n                + df[Fields.UNIQUE_PARAGRAPH_ID].astype(str)\n            )\n\n            df = df.merge(labels, on='participant_id', validate='many_to_one')\n            df = df[~df['lextale'].isna()]\n\n            data_dict[data_type] = df\n\n        # add IA features to fixations\n        data_dict['fixations'], data_dict['ia'] = (\n            self.add_ia_report_features_to_fixation_data(\n                data_dict['ia'],\n                data_dict['fixations'],\n            )\n        )\n\n        # add missing features\n        for data_type in [DataType.IA, DataType.FIXATIONS]:\n            data_dict[data_type] = add_missing_features(\n                et_data=data_dict[data_type],\n                trial_groupby_columns=self.data_args.groupby_columns,\n                mode=data_type,\n            )\n            data_dict[data_type] = data_dict[data_type].assign(\n                normalized_ID=(\n                    data_dict[data_type]['IA_ID'] - data_dict[data_type]['IA_ID'].min()\n                )\n                / (\n                    data_dict[data_type]['IA_ID'].max()\n                    - data_dict[data_type]['IA_ID'].min()\n                ),\n            )\n\n        # compute trial-level features\n        trial_level_features = compute_trial_level_features(\n            raw_fixation_data=data_dict[DataType.FIXATIONS],\n            raw_ia_data=data_dict[DataType.IA],\n            trial_groupby_columns=self.data_args.groupby_columns,\n            processed_data_path=self.data_args.processed_data_path,\n        )\n        data_dict[DataType.TRIAL_LEVEL] = trial_level_features\n\n        # replace missing values\n        data_dict = replace_missing_values(data_dict)\n\n        return data_dict\n\n    @property\n    def nlp(self):\n        if self._nlp is None:\n            self._nlp = spacy.load('en_core_web_sm')\n        return self._nlp\n\n    @property\n    def surp_extractor(self):\n        if self._surp_extractor is None:\n            self._surp_extractor = get_surp_extractor(\n                extractor_type=SurpExtractorType.CAT_CTX_LEFT, model_name='gpt2'\n            )\n        return self._surp_extractor\n\n    def _prepare_ia_dataframe(self, ia_df: pd.DataFrame) -&gt; pd.DataFrame:\n        ia_df = ia_df.rename(\n            columns={\n                Fields.IA_DATA_IA_ID_COL_NAME: Fields.FIXATION_REPORT_IA_ID_COL_NAME\n            }\n        )\n\n        # make sure order preserved\n        ia_df = ia_df.sort_values(\n            ['unique_trial_id', 'CURRENT_FIX_INTEREST_AREA_INDEX']\n        )\n\n        # group operations in batch\n        grouped = ia_df.groupby('unique_trial_id')\n        ia_df['TRIAL_IA_COUNT'] = grouped['unique_trial_id'].transform('count')\n        ia_df['IA_DWELL_TIME_%'] = grouped['IA_DWELL_TIME'].transform(\n            lambda x: x / np.sum(x)\n        )\n        ia_df['PARAGRAPH_RT'] = ia_df.groupby(Fields.UNIQUE_PARAGRAPH_ID)[\n            'IA_DWELL_TIME'\n        ].transform('sum')\n\n        ia_df['IA_FIXATION_%'] = ia_df.groupby('unique_trial_id')[\n            'IA_FIXATION_COUNT'\n        ].transform(lambda x: x / x.sum())\n\n        # feature calculations\n        ia_df['IA_FIRST_FIX_PROGRESSIVE'] = (ia_df['firstfix.sac.in'] &gt; 0).astype(int)\n        ia_df['IA_RUN_COUNT'] = ia_df['nrun']\n        ia_df['IA_SELECTIVE_REGRESSION_PATH_DURATION'] = ia_df['firstrun.gopast.sel']\n        ia_df['IA_SKIP'] = ia_df['total_skip']\n        ia_df['IA_FIRST_FIX_PROGRESSIVE'] = (ia_df['firstfix.sac.in'] &gt; 0).astype(int)\n        ia_df['word_length'] = ia_df['IA_LABEL'].str.len()\n\n        # not really the feature but better than 0, it is a binary indicator only\n        ia_df['IA_REGRESSION_IN_COUNT'] = ia_df['IA_REGRESSION_IN']\n        ia_df['IA_REGRESSION_OUT_FULL_COUNT'] = ia_df['IA_REGRESSION_OUT']\n        ia_df['IA_REGRESSION_OUT_COUNT'] = ia_df['IA_REGRESSION_OUT']\n        ia_df['IA_ID'] = ia_df[Fields.FIXATION_REPORT_IA_ID_COL_NAME]\n\n        # missing columns\n        zero_cols = [\n            'NEXT_FIX_INTEREST_AREA_INDEX',\n            'CURRENT_FIX_NEAREST_INTEREST_AREA_DISTANCE',\n            'start_of_line',\n            'end_of_line',\n            'IA_LAST_FIXATION_DURATION',\n            'IA_LAST_RUN_DWELL_TIME',\n            'IA_REGRESSION_PATH_DURATION',\n            'IA_LAST_RUN_FIXATION_COUNT',\n            'IA_FIRST_FIXATION_VISITED_IA_COUNT',\n            'IA_LEFT',\n            'IA_RIGHT',\n            'IA_TOP',\n            'IA_BOTTOM',\n            'NEXT_SAC_DURATION',\n            'NEXT_SAC_AVG_VELOCITY',\n            'NEXT_SAC_AMPLITUDE',\n            'NEXT_SAC_END_X',\n            'NEXT_SAC_START_X',\n            'NEXT_SAC_START_Y',\n            'NEXT_SAC_END_Y',\n        ]\n        ia_df[zero_cols] = 0\n\n        return ia_df\n\n    def _process_metrics_batch(self, groups: list[pd.DataFrame]) -&gt; pd.DataFrame:\n        metrics_list = []\n\n        for group in tqdm(groups, desc='Sequential metric extraction'):\n            try:\n                sentence = group['paragraph'].iloc[0]\n                metrics = get_metrics(\n                    target_text=sentence,\n                    surp_extractor=self.surp_extractor,\n                    parsing_model=self.nlp,\n                    parsing_mode='re-tokenize',\n                    add_parsing_features=True,\n                    language='en',\n                )\n                metrics['unique_paragraph_id'] = group['unique_paragraph_id'].iloc[0]\n                metrics['participant_id'] = group['participant_id'].iloc[0]\n                metrics[Fields.FIXATION_REPORT_IA_ID_COL_NAME] = (\n                    metrics['Token_idx'] + 1\n                )\n                metrics_list.append(metrics)\n            except Exception as e:\n                logger.warning(\n                    f'Error processing group {group[\"unique_paragraph_id\"].iloc[0]}: {e}'\n                )\n                continue\n\n        return (\n            pd.concat(metrics_list, ignore_index=True)\n            if metrics_list\n            else pd.DataFrame()\n        )\n\n    def _merge_metrics_to_ia(\n        self, ia_df: pd.DataFrame, metrics_df: pd.DataFrame\n    ) -&gt; pd.DataFrame:\n        merge_keys = ['unique_trial_id', Fields.FIXATION_REPORT_IA_ID_COL_NAME]\n\n        metrics_df[Fields.UNIQUE_TRIAL_ID] = (\n            metrics_df[Fields.SUBJECT_ID].astype(str)\n            + '_'\n            + metrics_df[Fields.UNIQUE_PARAGRAPH_ID].astype(str)\n        )\n\n        drop_keys = (set(metrics_df.columns) &amp; set(ia_df.columns)) - set(merge_keys)\n        cols_to_drop = list(drop_keys) + ['Morph']\n        cols_to_drop = [c for c in cols_to_drop if c in metrics_df.columns]\n\n        metrics_clean = metrics_df.drop(columns=cols_to_drop).drop_duplicates()\n\n        ia_df = ia_df.merge(\n            metrics_clean,\n            on=merge_keys,\n            how='left',\n            validate='many_to_one',\n        )\n\n        rename_map = {\n            'POS': 'universal_pos',\n            'Length': 'word_length_no_punctuation',\n            'Wordfreq_Frequency': 'wordfreq_frequency',\n            'subtlex_Frequency': 'subtlex_frequency',\n            'Reduced_POS': 'ptb_pos',\n            'Head_word_idx': 'head_word_index',\n            'Dependency_Relation': 'dependency_relation',\n            'Entity': 'entity_type',\n            'gpt2_Surprisal': 'gpt2_surprisal',\n            'gpt2': 'gpt2_surprisal',\n            'Head_Direction': 'head_direction',\n            'Is_Content_Word': 'is_content_word',\n            'n_Lefts': 'left_dependents_count',\n            'n_Rights': 'right_dependents_count',\n            'Distance2Head': 'distance_to_head',\n        }\n        ia_df = ia_df.rename(columns=rename_map)\n\n        return ia_df\n\n    def add_ia_report_features_to_fixation_data(\n        self,\n        ia_df: pd.DataFrame,\n        fix_df: pd.DataFrame,\n    ) -&gt; tuple[pd.DataFrame, pd.DataFrame]:\n        # Deduplicate groupby columns\n        self.data_args.groupby_columns = list(\n            dict.fromkeys(self.data_args.groupby_columns)\n        )\n\n        ia_df = self._prepare_ia_dataframe(ia_df)\n\n        # fix misalignments in meco\n        # wrong paragraph ids for participant sp_36 from 5 to 12 (off by one)\n        key_value_map = {\n            'sp_36': {11: 12, 10: 11, 9: 10, 8: 9, 7: 8, 6: 7, 5: 6},\n            'gr_45': {11: 12, 10: 11, 9: 10, 8: 9, 7: 8},\n            'it_25': {10: 11, 9: 10, 7: 8, 6: 7},\n            'se_38': {11: 12, 10: 11, 9: 10, 8: 9, 7: 8, 6: 7, 5: 6, 4: 5},\n        }\n        for participant_id, mapping in key_value_map.items():\n            mask = ia_df.participant_id == participant_id\n            fix_mask = fix_df.participant_id == participant_id\n            for old_id, new_id in sorted(mapping.items(), reverse=True):\n                ia_df.loc[\n                    mask &amp; (ia_df.unique_paragraph_id == old_id), 'unique_paragraph_id'\n                ] = new_id\n                fix_df.loc[\n                    fix_mask &amp; (fix_df.unique_paragraph_id == old_id),\n                    'unique_paragraph_id',\n                ] = new_id\n\n        ia_df['unique_trial_id'] = (\n            ia_df[Fields.SUBJECT_ID].astype(str)\n            + '_'\n            + ia_df[Fields.UNIQUE_PARAGRAPH_ID].astype(str)\n        )\n        fix_df['unique_trial_id'] = (\n            fix_df[Fields.SUBJECT_ID].astype(str)\n            + '_'\n            + fix_df[Fields.UNIQUE_PARAGRAPH_ID].astype(str)\n        )\n\n        # load stimuli because ia_df seems to contain different number of aois\n        # in this way we can be sure to that all paragraphs are what MECO authors provide\n        stimuli_df = pd.read_csv(\n            'data/MECOL2/stimuli/stimuli.csv', engine='python', encoding='latin1'\n        )\n        stimuli_df = stimuli_df.rename(\n            columns={'trialid': 'unique_paragraph_id', 'text': 'paragraph'},\n        )\n        stimuli_df = stimuli_df.drop(columns=['question1', 'question2'])\n        ia_df = ia_df.merge(\n            stimuli_df,\n            on='unique_paragraph_id',\n            validate='many_to_one',\n        )\n        fix_df = fix_df.merge(\n            stimuli_df,\n            on='unique_paragraph_id',\n            validate='many_to_one',\n        )\n\n        groups = [group for _, group in ia_df.groupby('unique_paragraph_id')]\n        metrics_df = self._process_metrics_batch(groups)\n\n        ia_df = self._merge_metrics_to_ia(ia_df, metrics_df)\n\n        fix_df['NEXT_FIX_INTEREST_AREA_INDEX'] = fix_df[\n            'CURRENT_FIX_INTEREST_AREA_INDEX'\n        ].shift(-1)\n        fix_df['CURRENT_FIX_INTEREST_AREA_INDEX'] = fix_df[\n            'CURRENT_FIX_INTEREST_AREA_INDEX'\n        ].fillna(-1)\n\n        if 'normalized_part_ID' in fix_df.columns:\n            if fix_df['normalized_part_ID'].isna().any():\n                logger.info('normalized_part_ID contains NaNs; dropping it.')\n                fix_df = fix_df.drop(columns='normalized_part_ID')\n\n        merge_keys = self.data_args.groupby_columns + [\n            Fields.FIXATION_REPORT_IA_ID_COL_NAME\n        ]\n        dup_cols = (set(fix_df.columns) &amp; set(ia_df.columns)) - set(merge_keys)\n        _ia_df = ia_df.drop(columns=list(dup_cols))\n        enriched_fix_df = fix_df.merge(\n            _ia_df.drop_duplicates(subset=merge_keys, keep='first'),\n            on=merge_keys,\n            how='left',\n            validate='many_to_one',\n        )\n\n        num_of_words = ia_df.groupby(self.data_args.groupby_columns).size()\n        num_of_words.name = 'num_of_words_in_trial'\n        enriched_fix_df = enriched_fix_df.merge(\n            num_of_words,\n            on=self.data_args.groupby_columns,\n            how='left',\n            validate='many_to_one',\n        )\n\n        # convert types\n        float_cols = ['TOWRE_word', 'TOWRE_nonword']\n        for col in float_cols:\n            if col in enriched_fix_df.columns:\n                enriched_fix_df[col] = enriched_fix_df[col].astype(float)\n            if col in ia_df.columns:\n                ia_df[col] = ia_df[col].astype(float)\n\n        enriched_fix_df['TRIAL_IA_COUNT'] = enriched_fix_df['TRIAL_IA_COUNT'].fillna(0)\n\n        return enriched_fix_df, ia_df\n\n    def get_column_map(self, data_type: DataType) -&gt; dict:\n        column_maps = {\n            DataType.IA: {\n                'uniform_id': Fields.SUBJECT_ID,\n                'trialid': Fields.UNIQUE_PARAGRAPH_ID,\n                'skip': 'total_skip',\n                'wordnum': 'IA_ID',\n                'word': 'IA_LABEL',\n                'nfix': 'IA_FIXATION_COUNT',\n                'reg.in': 'IA_REGRESSION_IN',\n                'reg.out': 'IA_REGRESSION_OUT',\n                'dur': 'IA_DWELL_TIME',\n                'firstrun.nfix': 'IA_FIRST_RUN_FIXATION_COUNT',\n                'firstrun.dur': 'IA_FIRST_RUN_DWELL_TIME',\n                'firstfix.launch': 'IA_FIRST_RUN_LAUNCH_SITE',\n                'firstfix.land': 'IA_FIRST_RUN_LANDING_POSITION',\n                'firstfix.dur': 'IA_FIRST_FIXATION_DURATION',\n            },\n            DataType.FIXATIONS: {\n                'xn': 'CURRENT_FIX_X',\n                'yn': 'CURRENT_FIX_Y',\n                'dur': 'CURRENT_FIX_DURATION',\n                'uniform_id': Fields.SUBJECT_ID,\n                'trialid': Fields.UNIQUE_PARAGRAPH_ID,\n                'fixid': 'CURRENT_FIX_INDEX',\n                'start': 'CURENT_FIX_START',\n                'stop': 'CURRENT_FIX_END',\n                'ps': 'CURRENT_FIX_PUPIL_SIZE',\n                'blink': 'CURRENT_FIX_BLINK_AROUND',\n                'word': 'CURRENT_FIX_INTEREST_AREA_LABEL',\n                'ianum': 'CURRENT_FIX_INTEREST_AREA_INDEX',\n                'ia': 'CURRENT_FIX_LABEL',\n                'ia.fix': 'CURRENT_FIX_INTEREST_AREA_FIX_COUNT',\n                'ia.runid': 'CURRENT_FIX_INTEREST_AREA_ID',\n            },\n        }\n\n        return column_maps.get(data_type, {})\n\n    def get_columns_to_keep(self) -&gt; list:\n        \"\"\"Get list of columns to keep after filtering\"\"\"\n        return []\n\n    @staticmethod\n    @lru_cache(maxsize=2)\n    def _load_labels():\n        labels_w1 = pyreadr.read_r(\n            'data/MECOL2W1/demographics/joint.ind.diff.l2.rda',\n        )['joint_id']\n        labels_w2 = pyreadr.read_r(\n            'data/MECOL2W2/demographics/joint.ind.diff.l2.w2.rda',\n        )['joint_id_w2']\n\n        return pd.concat([labels_w1, labels_w2], axis=0).reset_index(drop=True)\n</code></pre>"},{"location":"reference/data/preprocessing/dataset_preprocessing/meco/#data.preprocessing.dataset_preprocessing.meco.MECOProcessor.dataset_specific_processing","title":"<code>dataset_specific_processing(data_dict)</code>","text":"<p>MECO-specific processing steps</p> Source code in <code>src/data/preprocessing/dataset_preprocessing/meco.py</code> <pre><code>def dataset_specific_processing(\n    self, data_dict: dict[str, pd.DataFrame]\n) -&gt; dict[str, pd.DataFrame]:\n    \"\"\"MECO-specific processing steps\"\"\"\n\n    # add unique trial IDs and merge labels\n    labels = self._load_labels().rename(columns={'uniform_id': Fields.SUBJECT_ID})\n\n    for data_type in [DataType.IA, DataType.FIXATIONS]:\n        df = data_dict[data_type]\n\n        df[Fields.UNIQUE_TRIAL_ID] = (\n            df[Fields.SUBJECT_ID].astype(str)\n            + '_'\n            + df[Fields.UNIQUE_PARAGRAPH_ID].astype(str)\n        )\n\n        df = df.merge(labels, on='participant_id', validate='many_to_one')\n        df = df[~df['lextale'].isna()]\n\n        data_dict[data_type] = df\n\n    # add IA features to fixations\n    data_dict['fixations'], data_dict['ia'] = (\n        self.add_ia_report_features_to_fixation_data(\n            data_dict['ia'],\n            data_dict['fixations'],\n        )\n    )\n\n    # add missing features\n    for data_type in [DataType.IA, DataType.FIXATIONS]:\n        data_dict[data_type] = add_missing_features(\n            et_data=data_dict[data_type],\n            trial_groupby_columns=self.data_args.groupby_columns,\n            mode=data_type,\n        )\n        data_dict[data_type] = data_dict[data_type].assign(\n            normalized_ID=(\n                data_dict[data_type]['IA_ID'] - data_dict[data_type]['IA_ID'].min()\n            )\n            / (\n                data_dict[data_type]['IA_ID'].max()\n                - data_dict[data_type]['IA_ID'].min()\n            ),\n        )\n\n    # compute trial-level features\n    trial_level_features = compute_trial_level_features(\n        raw_fixation_data=data_dict[DataType.FIXATIONS],\n        raw_ia_data=data_dict[DataType.IA],\n        trial_groupby_columns=self.data_args.groupby_columns,\n        processed_data_path=self.data_args.processed_data_path,\n    )\n    data_dict[DataType.TRIAL_LEVEL] = trial_level_features\n\n    # replace missing values\n    data_dict = replace_missing_values(data_dict)\n\n    return data_dict\n</code></pre>"},{"location":"reference/data/preprocessing/dataset_preprocessing/meco/#data.preprocessing.dataset_preprocessing.meco.MECOProcessor.get_columns_to_keep","title":"<code>get_columns_to_keep()</code>","text":"<p>Get list of columns to keep after filtering</p> Source code in <code>src/data/preprocessing/dataset_preprocessing/meco.py</code> <pre><code>def get_columns_to_keep(self) -&gt; list:\n    \"\"\"Get list of columns to keep after filtering\"\"\"\n    return []\n</code></pre>"},{"location":"reference/data/preprocessing/dataset_preprocessing/onestop/","title":"onestop","text":""},{"location":"reference/data/preprocessing/dataset_preprocessing/onestop/#data.preprocessing.dataset_preprocessing.onestop.ArgsParser","title":"<code>ArgsParser</code>","text":"<p>               Bases: <code>Tap</code></p> <p>Args parser for preprocessing.py</p> <pre><code>Note, for fixation data, the X_IA_DWELL_TIME, for X in\n[total, min, max, part_total, part_min, part_max]\ncolumns are computed based on the CURRENT_FIX_DURATION column.\n\nNote, documentation was generated automatically. Please check the source code for more info.\n</code></pre> <p>Args:     SURPRISAL_MODELS (list[str]): Models to extract surprisal from     unique_item_columns (list[str]): columns that make up a unique item      (Path | None): Path to question difficulty data from prolific     mode (Mode): whether to use interest area or fixation data</p> Source code in <code>src/data/preprocessing/dataset_preprocessing/onestop.py</code> <pre><code>class ArgsParser(Tap):\n    \"\"\"Args parser for preprocessing.py\n\n        Note, for fixation data, the X_IA_DWELL_TIME, for X in\n        [total, min, max, part_total, part_min, part_max]\n        columns are computed based on the CURRENT_FIX_DURATION column.\n\n        Note, documentation was generated automatically. Please check the source code for more info.\n    Args:\n        SURPRISAL_MODELS (list[str]): Models to extract surprisal from\n        unique_item_columns (list[str]): columns that make up a unique item\n         (Path | None): Path to question difficulty data from prolific\n        mode (Mode): whether to use interest area or fixation data\n    \"\"\"\n\n    SURPRISAL_MODELS: list[str] = [\n        'gpt2',\n    ]  # Models to shift surprisal for\n\n    onestopqa_path: Path = Path('metadata/onestop_qa.json')\n    mode: Mode = Mode.IA  # whether to use interest area or fixation data\n</code></pre>"},{"location":"reference/data/preprocessing/dataset_preprocessing/onestop/#data.preprocessing.dataset_preprocessing.onestop.Mode","title":"<code>Mode</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Enum for processing mode. Defines whether to process interest area (IA) or fixation data.</p> Source code in <code>src/data/preprocessing/dataset_preprocessing/onestop.py</code> <pre><code>class Mode(Enum):\n    \"\"\"\n    Enum for processing mode.\n    Defines whether to process interest area (IA) or fixation data.\n    \"\"\"\n\n    IA = 'ia'\n    FIXATION = 'fixations'\n</code></pre>"},{"location":"reference/data/preprocessing/dataset_preprocessing/onestop/#data.preprocessing.dataset_preprocessing.onestop.OneStopProcessor","title":"<code>OneStopProcessor</code>","text":"<p>               Bases: <code>DatasetProcessor</code></p> <p>Processor for OneStop dataset</p> Source code in <code>src/data/preprocessing/dataset_preprocessing/onestop.py</code> <pre><code>class OneStopProcessor(DatasetProcessor):\n    \"\"\"Processor for OneStop dataset\"\"\"\n\n    def get_column_map(self, data_type: DataType) -&gt; dict:\n        \"\"\"Get column mapping for OneStop dataset\"\"\"\n        return {\n            # Empty for now as it's handled in onestop processing\n        }\n\n    def get_columns_to_keep(self) -&gt; list:\n        \"\"\"Get list of columns to keep after filtering\"\"\"\n        return list(\n            set(\n                list(Fields)\n                + BaseModelArgs().word_features\n                + BaseModelArgs().eye_features\n                + BaseModelArgs().fixation_features\n                + BEyeLSTMArgs().fixation_features\n                + BEyeLSTMArgs().eye_features\n                + BEyeLSTMArgs().word_features\n                + ['unique_trial_id']\n                + self.data_args.groupby_columns\n                + [\n                    'CURRENT_FIX_NEAREST_INTEREST_AREA_DISTANCE',\n                    'IA_FIRST_RUN_LANDING_POSITION',\n                    'IA_LAST_RUN_LANDING_POSITION',\n                    'NEXT_SAC_START_Y',\n                    'NEXT_SAC_END_X',\n                    'NEXT_SAC_END_Y',\n                    'NEXT_SAC_START_X',\n                    'ptb_pos',\n                    'is_content_word',\n                    'LengthCategory',\n                    'is_reg_sum',\n                    'is_progressive_sum',\n                    'IA_REGRESSION_IN_COUNT_sum',\n                    'normalized_outgoing_regression_count',\n                    'normalized_outgoing_progressive_count',\n                    'normalized_incoming_regression_count',\n                    'LengthCategory_normalized_IA_DWELL_TIME',\n                    'universal_pos_normalized_IA_DWELL_TIME',\n                    'LengthCategory_normalized_IA_FIRST_FIXATION_DURATION',\n                    'universal_pos_normalized_IA_FIRST_FIXATION_DURATION',\n                    'entity_type',\n                ]\n            )\n        )\n\n    def dataset_specific_processing(\n        self, data_dict: dict[str, pd.DataFrame]\n    ) -&gt; dict[str, pd.DataFrame]:\n        \"\"\"OneStop-specific processing steps\"\"\"\n        surprisal_models = ['gpt2']\n\n        for data_type in [DataType.IA, DataType.FIXATIONS]:\n            if data_type not in data_dict or data_dict[data_type] is None:\n                continue\n\n            df = data_dict[data_type]\n\n            args = [\n                '--mode',\n                data_type,\n                '--SURPRISAL_MODELS',\n                *surprisal_models,\n                '--onestopqa_path',\n                str(self.data_args.onestopqa_path),\n            ]\n            cfg = ArgsParser().parse_args(args)\n            if data_type == DataType.IA:\n                df = self.query_onestop_data(df, query=self.data_args.ia_query)\n            elif data_type == DataType.FIXATIONS:\n                df = self.query_onestop_data(df, query=self.data_args.fixation_query)\n\n            df = df.drop(columns=['ptb_pos']).rename(columns={'Reduced_POS': 'ptb_pos'})\n            df = our_processing(df=df, args=cfg)\n\n            # add unique_trial_id column\n            df['unique_trial_id'] = (\n                df['participant_id'].astype(str)\n                + '_'\n                + df['unique_paragraph_id'].astype(str)\n                + '_'\n                + df['repeated_reading_trial'].astype(str)\n                + '_'\n                + df['practice_trial'].astype(str)\n            )\n\n            # add is_correct column\n            df['is_correct'] = (df.selected_answer == 'A').astype(int)\n\n            df[Fields.LEVEL] = (\n                df[Fields.LEVEL].replace({'Adv': 1, 'Ele': 0}).astype(int)\n            )\n            if data_type == DataType.IA:\n                df['head_direction'] = df['distance_to_head'] &gt; 0\n                df['head_direction'] = df['head_direction'].astype(int)\n\n            data_dict[data_type] = df\n\n        data_dict['fixations'] = self.add_ia_report_features_to_fixation_data(\n            data_dict['ia'], data_dict['fixations']\n        )\n\n        for data_type in [DataType.IA, DataType.FIXATIONS]:\n            data_dict[data_type] = add_missing_features(\n                et_data=data_dict[data_type],\n                trial_groupby_columns=self.data_args.groupby_columns,\n                mode=data_type,\n            )\n\n        trial_level_features = compute_trial_level_features(\n            raw_fixation_data=data_dict[DataType.FIXATIONS],\n            raw_ia_data=data_dict[DataType.IA],\n            trial_groupby_columns=self.data_args.groupby_columns,\n            processed_data_path=self.data_args.processed_data_path,\n        )\n        data_dict[DataType.TRIAL_LEVEL] = trial_level_features\n\n        return data_dict\n\n    def query_onestop_data(self, data: pd.DataFrame, query: str | None) -&gt; pd.DataFrame:\n        \"\"\"Process the raw data by applying a query\"\"\"\n        if query is not None:\n            data = data.query(query)\n            logger.info(f'Number of rows after query ({query}): {len(data)}')\n        else:\n            logger.info('***** No query! *****')\n        return data\n\n    def add_ia_report_features_to_fixation_data(\n        self, ia_df: pd.DataFrame, fix_df: pd.DataFrame\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        Merge per\u2011IA (interest\u2011area) features into the fixation\u2011level data.\n\n        Result: one row per fixation, enriched with IA\u2011level attributes.\n        \"\"\"\n        # --- 1. Unify IA\u2011ID column name ----------------------------------------\n        ia_df = ia_df.rename(\n            columns={\n                Fields.IA_DATA_IA_ID_COL_NAME: Fields.FIXATION_REPORT_IA_ID_COL_NAME\n            }\n        )\n\n        # --- 2. Build the list of IA features we plan to add -------------------\n        ia_features = (\n            BEyeLSTMArgs().ia_features_to_add_to_fixation_data\n            + BaseModelArgs().ia_features_to_add_to_fixation_data\n            + PLMASfArgs().ia_features_to_add_to_fixation_data\n            + ['entity_type']\n        )\n\n        required_cols = (\n            self.data_args.groupby_columns\n            + [Fields.FIXATION_REPORT_IA_ID_COL_NAME]\n            + ia_features\n        )\n        ia_df = ia_df[list(set(required_cols))]\n\n        # --- 3. Drop columns that also exist in fixation table -----------------\n        merge_keys = set(\n            self.data_args.groupby_columns + [Fields.FIXATION_REPORT_IA_ID_COL_NAME]\n        )\n        dup_cols = (set(fix_df.columns) &amp; set(ia_df.columns)) - merge_keys\n        ia_df = ia_df.drop(columns=list(dup_cols))\n\n        # --- 4. Clean nuisance column -----------------------------------------\n        if 'normalized_part_ID' in fix_df.columns:\n            if fix_df['normalized_part_ID'].isna().any():\n                logger.warning('normalized_part_ID contains NaNs; dropping it.')\n            fix_df = fix_df.drop(columns='normalized_part_ID')\n\n        # --- 5. Merge ----------------------------------------------------------\n        enriched_fix_df = fix_df.merge(\n            ia_df,\n            on=list(merge_keys),\n            how='left',\n            validate='many_to_one',\n        )\n\n        num_of_words_in_trials_series = ia_df.groupby(\n            self.data_args.groupby_columns,\n        ).apply(len)\n        num_of_words_in_trials_series.name = 'num_of_words_in_trial'\n        merge_keys = set(self.data_args.groupby_columns)\n        enriched_fix_df = enriched_fix_df.merge(\n            num_of_words_in_trials_series,\n            on=self.data_args.groupby_columns,\n            how='left',\n        )\n\n        return enriched_fix_df\n</code></pre>"},{"location":"reference/data/preprocessing/dataset_preprocessing/onestop/#data.preprocessing.dataset_preprocessing.onestop.OneStopProcessor.add_ia_report_features_to_fixation_data","title":"<code>add_ia_report_features_to_fixation_data(ia_df, fix_df)</code>","text":"<p>Merge per\u2011IA (interest\u2011area) features into the fixation\u2011level data.</p> <p>Result: one row per fixation, enriched with IA\u2011level attributes.</p> Source code in <code>src/data/preprocessing/dataset_preprocessing/onestop.py</code> <pre><code>def add_ia_report_features_to_fixation_data(\n    self, ia_df: pd.DataFrame, fix_df: pd.DataFrame\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Merge per\u2011IA (interest\u2011area) features into the fixation\u2011level data.\n\n    Result: one row per fixation, enriched with IA\u2011level attributes.\n    \"\"\"\n    # --- 1. Unify IA\u2011ID column name ----------------------------------------\n    ia_df = ia_df.rename(\n        columns={\n            Fields.IA_DATA_IA_ID_COL_NAME: Fields.FIXATION_REPORT_IA_ID_COL_NAME\n        }\n    )\n\n    # --- 2. Build the list of IA features we plan to add -------------------\n    ia_features = (\n        BEyeLSTMArgs().ia_features_to_add_to_fixation_data\n        + BaseModelArgs().ia_features_to_add_to_fixation_data\n        + PLMASfArgs().ia_features_to_add_to_fixation_data\n        + ['entity_type']\n    )\n\n    required_cols = (\n        self.data_args.groupby_columns\n        + [Fields.FIXATION_REPORT_IA_ID_COL_NAME]\n        + ia_features\n    )\n    ia_df = ia_df[list(set(required_cols))]\n\n    # --- 3. Drop columns that also exist in fixation table -----------------\n    merge_keys = set(\n        self.data_args.groupby_columns + [Fields.FIXATION_REPORT_IA_ID_COL_NAME]\n    )\n    dup_cols = (set(fix_df.columns) &amp; set(ia_df.columns)) - merge_keys\n    ia_df = ia_df.drop(columns=list(dup_cols))\n\n    # --- 4. Clean nuisance column -----------------------------------------\n    if 'normalized_part_ID' in fix_df.columns:\n        if fix_df['normalized_part_ID'].isna().any():\n            logger.warning('normalized_part_ID contains NaNs; dropping it.')\n        fix_df = fix_df.drop(columns='normalized_part_ID')\n\n    # --- 5. Merge ----------------------------------------------------------\n    enriched_fix_df = fix_df.merge(\n        ia_df,\n        on=list(merge_keys),\n        how='left',\n        validate='many_to_one',\n    )\n\n    num_of_words_in_trials_series = ia_df.groupby(\n        self.data_args.groupby_columns,\n    ).apply(len)\n    num_of_words_in_trials_series.name = 'num_of_words_in_trial'\n    merge_keys = set(self.data_args.groupby_columns)\n    enriched_fix_df = enriched_fix_df.merge(\n        num_of_words_in_trials_series,\n        on=self.data_args.groupby_columns,\n        how='left',\n    )\n\n    return enriched_fix_df\n</code></pre>"},{"location":"reference/data/preprocessing/dataset_preprocessing/onestop/#data.preprocessing.dataset_preprocessing.onestop.OneStopProcessor.dataset_specific_processing","title":"<code>dataset_specific_processing(data_dict)</code>","text":"<p>OneStop-specific processing steps</p> Source code in <code>src/data/preprocessing/dataset_preprocessing/onestop.py</code> <pre><code>def dataset_specific_processing(\n    self, data_dict: dict[str, pd.DataFrame]\n) -&gt; dict[str, pd.DataFrame]:\n    \"\"\"OneStop-specific processing steps\"\"\"\n    surprisal_models = ['gpt2']\n\n    for data_type in [DataType.IA, DataType.FIXATIONS]:\n        if data_type not in data_dict or data_dict[data_type] is None:\n            continue\n\n        df = data_dict[data_type]\n\n        args = [\n            '--mode',\n            data_type,\n            '--SURPRISAL_MODELS',\n            *surprisal_models,\n            '--onestopqa_path',\n            str(self.data_args.onestopqa_path),\n        ]\n        cfg = ArgsParser().parse_args(args)\n        if data_type == DataType.IA:\n            df = self.query_onestop_data(df, query=self.data_args.ia_query)\n        elif data_type == DataType.FIXATIONS:\n            df = self.query_onestop_data(df, query=self.data_args.fixation_query)\n\n        df = df.drop(columns=['ptb_pos']).rename(columns={'Reduced_POS': 'ptb_pos'})\n        df = our_processing(df=df, args=cfg)\n\n        # add unique_trial_id column\n        df['unique_trial_id'] = (\n            df['participant_id'].astype(str)\n            + '_'\n            + df['unique_paragraph_id'].astype(str)\n            + '_'\n            + df['repeated_reading_trial'].astype(str)\n            + '_'\n            + df['practice_trial'].astype(str)\n        )\n\n        # add is_correct column\n        df['is_correct'] = (df.selected_answer == 'A').astype(int)\n\n        df[Fields.LEVEL] = (\n            df[Fields.LEVEL].replace({'Adv': 1, 'Ele': 0}).astype(int)\n        )\n        if data_type == DataType.IA:\n            df['head_direction'] = df['distance_to_head'] &gt; 0\n            df['head_direction'] = df['head_direction'].astype(int)\n\n        data_dict[data_type] = df\n\n    data_dict['fixations'] = self.add_ia_report_features_to_fixation_data(\n        data_dict['ia'], data_dict['fixations']\n    )\n\n    for data_type in [DataType.IA, DataType.FIXATIONS]:\n        data_dict[data_type] = add_missing_features(\n            et_data=data_dict[data_type],\n            trial_groupby_columns=self.data_args.groupby_columns,\n            mode=data_type,\n        )\n\n    trial_level_features = compute_trial_level_features(\n        raw_fixation_data=data_dict[DataType.FIXATIONS],\n        raw_ia_data=data_dict[DataType.IA],\n        trial_groupby_columns=self.data_args.groupby_columns,\n        processed_data_path=self.data_args.processed_data_path,\n    )\n    data_dict[DataType.TRIAL_LEVEL] = trial_level_features\n\n    return data_dict\n</code></pre>"},{"location":"reference/data/preprocessing/dataset_preprocessing/onestop/#data.preprocessing.dataset_preprocessing.onestop.OneStopProcessor.get_column_map","title":"<code>get_column_map(data_type)</code>","text":"<p>Get column mapping for OneStop dataset</p> Source code in <code>src/data/preprocessing/dataset_preprocessing/onestop.py</code> <pre><code>def get_column_map(self, data_type: DataType) -&gt; dict:\n    \"\"\"Get column mapping for OneStop dataset\"\"\"\n    return {\n        # Empty for now as it's handled in onestop processing\n    }\n</code></pre>"},{"location":"reference/data/preprocessing/dataset_preprocessing/onestop/#data.preprocessing.dataset_preprocessing.onestop.OneStopProcessor.get_columns_to_keep","title":"<code>get_columns_to_keep()</code>","text":"<p>Get list of columns to keep after filtering</p> Source code in <code>src/data/preprocessing/dataset_preprocessing/onestop.py</code> <pre><code>def get_columns_to_keep(self) -&gt; list:\n    \"\"\"Get list of columns to keep after filtering\"\"\"\n    return list(\n        set(\n            list(Fields)\n            + BaseModelArgs().word_features\n            + BaseModelArgs().eye_features\n            + BaseModelArgs().fixation_features\n            + BEyeLSTMArgs().fixation_features\n            + BEyeLSTMArgs().eye_features\n            + BEyeLSTMArgs().word_features\n            + ['unique_trial_id']\n            + self.data_args.groupby_columns\n            + [\n                'CURRENT_FIX_NEAREST_INTEREST_AREA_DISTANCE',\n                'IA_FIRST_RUN_LANDING_POSITION',\n                'IA_LAST_RUN_LANDING_POSITION',\n                'NEXT_SAC_START_Y',\n                'NEXT_SAC_END_X',\n                'NEXT_SAC_END_Y',\n                'NEXT_SAC_START_X',\n                'ptb_pos',\n                'is_content_word',\n                'LengthCategory',\n                'is_reg_sum',\n                'is_progressive_sum',\n                'IA_REGRESSION_IN_COUNT_sum',\n                'normalized_outgoing_regression_count',\n                'normalized_outgoing_progressive_count',\n                'normalized_incoming_regression_count',\n                'LengthCategory_normalized_IA_DWELL_TIME',\n                'universal_pos_normalized_IA_DWELL_TIME',\n                'LengthCategory_normalized_IA_FIRST_FIXATION_DURATION',\n                'universal_pos_normalized_IA_FIRST_FIXATION_DURATION',\n                'entity_type',\n            ]\n        )\n    )\n</code></pre>"},{"location":"reference/data/preprocessing/dataset_preprocessing/onestop/#data.preprocessing.dataset_preprocessing.onestop.OneStopProcessor.query_onestop_data","title":"<code>query_onestop_data(data, query)</code>","text":"<p>Process the raw data by applying a query</p> Source code in <code>src/data/preprocessing/dataset_preprocessing/onestop.py</code> <pre><code>def query_onestop_data(self, data: pd.DataFrame, query: str | None) -&gt; pd.DataFrame:\n    \"\"\"Process the raw data by applying a query\"\"\"\n    if query is not None:\n        data = data.query(query)\n        logger.info(f'Number of rows after query ({query}): {len(data)}')\n    else:\n        logger.info('***** No query! *****')\n    return data\n</code></pre>"},{"location":"reference/data/preprocessing/dataset_preprocessing/onestop/#data.preprocessing.dataset_preprocessing.onestop.add_additional_metrics","title":"<code>add_additional_metrics(df)</code>","text":"<p>Add additional metrics to the DataFrame.</p> <p>Adds columns for regression rate, total skip, and part length.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Input DataFrame</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: DataFrame with added metrics</p> Source code in <code>src/data/preprocessing/dataset_preprocessing/onestop.py</code> <pre><code>def add_additional_metrics(df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    Add additional metrics to the DataFrame.\n\n    Adds columns for regression rate, total skip, and part length.\n\n    Args:\n        df (pd.DataFrame): Input DataFrame\n\n    Returns:\n        pd.DataFrame: DataFrame with added metrics\n    \"\"\"\n\n    logger.info('Adding additional metrics...')\n    df['regression_rate'] = df['IA_REGRESSION_OUT_FULL_COUNT'] / df['IA_RUN_COUNT']\n    df['total_skip'] = df['IA_DWELL_TIME'] == 0\n    df['is_content_word'] = df['universal_pos'].apply(is_content_word)\n\n    return df\n</code></pre>"},{"location":"reference/data/preprocessing/dataset_preprocessing/onestop/#data.preprocessing.dataset_preprocessing.onestop.add_unique_paragraph_id","title":"<code>add_unique_paragraph_id(df)</code>","text":"<p>Add unique paragraph ID to the DataFrame.</p> <p>Creates a new column 'unique_paragraph_id' by combining article_batch, article_id, difficulty_level, and paragraph_id.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Input DataFrame</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: DataFrame with added unique paragraph ID</p> Source code in <code>src/data/preprocessing/dataset_preprocessing/onestop.py</code> <pre><code>def add_unique_paragraph_id(df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    Add unique paragraph ID to the DataFrame.\n\n    Creates a new column 'unique_paragraph_id' by combining article_batch,\n    article_id, difficulty_level, and paragraph_id.\n\n    Args:\n        df (pd.DataFrame): Input DataFrame\n\n    Returns:\n        pd.DataFrame: DataFrame with added unique paragraph ID\n    \"\"\"\n    logger.info('Adding unique paragraph id...')\n    df['unique_paragraph_id'] = (\n        df[['article_batch', 'article_id', 'difficulty_level', 'paragraph_id']]\n        .astype(str)\n        .apply('_'.join, axis=1)\n    )\n    return df\n</code></pre>"},{"location":"reference/data/preprocessing/dataset_preprocessing/onestop/#data.preprocessing.dataset_preprocessing.onestop.adjust_indexing","title":"<code>adjust_indexing(df, args)</code>","text":"<p>Adjust indexing to be 0-indexed.</p> <p>Subtracts 1 from specified columns based on whether in IA or FIXATION mode.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Input DataFrame</p> required <code>args</code> <code>ArgsParser</code> <p>Contains mode configuration</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: DataFrame with adjusted indexing</p> Source code in <code>src/data/preprocessing/dataset_preprocessing/onestop.py</code> <pre><code>def adjust_indexing(df: pd.DataFrame, args: ArgsParser) -&gt; pd.DataFrame:\n    \"\"\"\n    Adjust indexing to be 0-indexed.\n\n    Subtracts 1 from specified columns based on whether in IA or FIXATION mode.\n\n    Args:\n        df (pd.DataFrame): Input DataFrame\n        args (ArgsParser): Contains mode configuration\n\n    Returns:\n        pd.DataFrame: DataFrame with adjusted indexing\n    \"\"\"\n    if args.mode == Mode.IA:\n        subtract_one_fields = [IA_ID_COL]\n    elif args.mode == Mode.FIXATION:\n        subtract_one_fields = [\n            FIXATION_ID_COL,\n            NEXT_FIXATION_ID_COL,\n        ]\n    else:\n        raise ValueError(f'Unknown mode: {args.mode}')\n\n    df[subtract_one_fields] -= 1\n    logger.info('%s values adjusted to be 0-indexed.', subtract_one_fields)\n    return df\n</code></pre>"},{"location":"reference/data/preprocessing/dataset_preprocessing/onestop/#data.preprocessing.dataset_preprocessing.onestop.compute_normalized_features","title":"<code>compute_normalized_features(df, duration_col, ia_field)</code>","text":"<p>Calculate normalized versions of key metrics.</p> <p>Adds columns for: - Normalized dwell times (total and by part) - Normalized word positions (total and by part) - Reverse indices from end</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Input DataFrame</p> required <code>duration_col</code> <code>str</code> <p>Column name for duration values</p> required <code>ia_field</code> <code>str</code> <p>Column name for word/fixation index</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: DataFrame with normalized metrics</p> Source code in <code>src/data/preprocessing/dataset_preprocessing/onestop.py</code> <pre><code>def compute_normalized_features(\n    df: pd.DataFrame, duration_col: str, ia_field: str\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Calculate normalized versions of key metrics.\n\n    Adds columns for:\n    - Normalized dwell times (total and by part)\n    - Normalized word positions (total and by part)\n    - Reverse indices from end\n\n    Args:\n        df (pd.DataFrame): Input DataFrame\n        duration_col (str): Column name for duration values\n        ia_field (str): Column name for word/fixation index\n\n    Returns:\n        pd.DataFrame: DataFrame with normalized metrics\n    \"\"\"\n    logger.info('Computing normalized dwell time, and normalized word indices...')\n    df = df.assign(\n        normalized_ID=(df[ia_field] - df.min_IA_ID) / (df.max_IA_ID - df.min_IA_ID),\n    ).copy()\n    return df\n</code></pre>"},{"location":"reference/data/preprocessing/dataset_preprocessing/onestop/#data.preprocessing.dataset_preprocessing.onestop.compute_span_level_metrics","title":"<code>compute_span_level_metrics(df, ia_field, mode, duration_col)</code>","text":"<p>Calculate aggregated metrics for different text spans.</p> <p>Computes: - Total dwell time per trial/span - Min/max word indices per trial/span - For fixations: count per span - Normalizes indices to start at 0</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Input DataFrame</p> required <code>ia_field</code> <code>str</code> <p>Column name for word/fixation index</p> required <code>mode</code> <code>Mode</code> <p>IA or FIXATION processing mode</p> required <code>duration_col</code> <code>str</code> <p>Column name for duration values</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: DataFrame with added span-level metrics</p> Source code in <code>src/data/preprocessing/dataset_preprocessing/onestop.py</code> <pre><code>def compute_span_level_metrics(\n    df: pd.DataFrame, ia_field: str, mode: Mode, duration_col: str\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Calculate aggregated metrics for different text spans.\n\n    Computes:\n    - Total dwell time per trial/span\n    - Min/max word indices per trial/span\n    - For fixations: count per span\n    - Normalizes indices to start at 0\n\n    Args:\n        df (pd.DataFrame): Input DataFrame\n        ia_field (str): Column name for word/fixation index\n        mode (Mode): IA or FIXATION processing mode\n        duration_col (str): Column name for duration values\n\n    Returns:\n        pd.DataFrame: DataFrame with added span-level metrics\n    \"\"\"\n    logger.info('Computing span-level metrics...')\n\n    group_by_fields = [\n        'participant_id',\n        'unique_paragraph_id',\n        'repeated_reading_trial',\n    ]\n\n    # Fix trials where ID does not start at 0\n    if mode == Mode.IA:\n        temp_max_per_trial = df.groupby(group_by_fields).agg(\n            min_IA_ID=pd.NamedAgg(column=ia_field, aggfunc='min'),\n            max_IA_ID=pd.NamedAgg(column=ia_field, aggfunc='max'),\n        )\n        non_zero_min_ia_id_trials = temp_max_per_trial[\n            temp_max_per_trial['min_IA_ID'] != 0\n        ]\n        logger.info(\n            'Number of trials where min_IA_ID is not zero: %d out of %d trials.',\n            len(non_zero_min_ia_id_trials),\n            len(temp_max_per_trial),\n        )\n        df = df.merge(\n            temp_max_per_trial,\n            on=group_by_fields,\n            validate='m:1',\n            suffixes=(None, '_y'),\n        )\n        logger.info('Shifting IA_ID to start at 0...')\n        df[ia_field] -= df['min_IA_ID']\n        df.drop(columns=['min_IA_ID', 'max_IA_ID'], inplace=True)\n\n    max_per_trial = df.groupby(group_by_fields).agg(\n        total_IA_DWELL_TIME=pd.NamedAgg(column=duration_col, aggfunc='sum'),\n        min_IA_ID=pd.NamedAgg(column=ia_field, aggfunc='min'),\n        max_IA_ID=pd.NamedAgg(column=ia_field, aggfunc='max'),\n    )\n    df = df.merge(\n        max_per_trial, on=group_by_fields, validate='m:1', suffixes=(None, '_y')\n    )\n    return df\n</code></pre>"},{"location":"reference/data/preprocessing/dataset_preprocessing/onestop/#data.preprocessing.dataset_preprocessing.onestop.compute_start_end_line","title":"<code>compute_start_end_line(df)</code>","text":"<p>Compute for each word  whether it the first/last word in the line (not sentence!).</p> <p>This function adds two new columns to the input DataFrame: 'start_of_line' and 'end_of_line'. A word is considered to be at the start of a line if its     'IA_LEFT' value is smaller than the previous word's. A word is considered to be at the end of a line if its     'IA_LEFT' value is larger than the next word's.</p> <p>df (pd.DataFrame): Input DataFrame. Must contain the columns 'participant_id',     'unique_paragraph_id', and 'IA_LEFT'.</p> <p>Returns: pd.DataFrame: The input DataFrame with two new columns: 'start_of_line' and 'end_of_line'.</p> Source code in <code>src/data/preprocessing/dataset_preprocessing/onestop.py</code> <pre><code>def compute_start_end_line(df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    Compute for each word  whether it the first/last word in the line (not sentence!).\n\n    This function adds two new columns to the input DataFrame: 'start_of_line' and 'end_of_line'.\n    A word is considered to be at the start of a line if its\n        'IA_LEFT' value is smaller than the previous word's.\n    A word is considered to be at the end of a line if its\n        'IA_LEFT' value is larger than the next word's.\n\n    Parameters:\n    df (pd.DataFrame): Input DataFrame. Must contain the columns 'participant_id',\n        'unique_paragraph_id', and 'IA_LEFT'.\n\n    Returns:\n    pd.DataFrame: The input DataFrame with two new columns: 'start_of_line' and 'end_of_line'.\n    \"\"\"\n\n    logger.info('Adding start_of_line and end_of_line columns...')\n    grouped_df = df.groupby(\n        ['participant_id', 'unique_paragraph_id', 'repeated_reading_trial']\n    )\n    df['start_of_line'] = (\n        grouped_df['IA_LEFT'].shift(periods=1, fill_value=1000000) &gt; df['IA_LEFT']\n    )\n    df['end_of_line'] = (\n        grouped_df['IA_LEFT'].shift(periods=-1, fill_value=-1) &lt; df['IA_LEFT']\n    )\n    return df\n</code></pre>"},{"location":"reference/data/preprocessing/dataset_preprocessing/onestop/#data.preprocessing.dataset_preprocessing.onestop.convert_to_float_features","title":"<code>convert_to_float_features(df, args)</code>","text":"<p>Convert specified columns to float type.</p> <p>Handles missing values and dots by replacing them with None before conversion. Different columns are processed based on whether in IA or FIXATION mode.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Input DataFrame</p> required <code>args</code> <code>ArgsParser</code> <p>Contains mode configuration</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: DataFrame with converted float columns</p> Source code in <code>src/data/preprocessing/dataset_preprocessing/onestop.py</code> <pre><code>def convert_to_float_features(df: pd.DataFrame, args: ArgsParser) -&gt; pd.DataFrame:\n    \"\"\"\n    Convert specified columns to float type.\n\n    Handles missing values and dots by replacing them with None before conversion.\n    Different columns are processed based on whether in IA or FIXATION mode.\n\n    Args:\n        df (pd.DataFrame): Input DataFrame\n        args (ArgsParser): Contains mode configuration\n\n    Returns:\n        pd.DataFrame: DataFrame with converted float columns\n    \"\"\"\n    if args.mode == Mode.IA:\n        to_float_features = [\n            'IA_AVERAGE_FIX_PUPIL_SIZE',\n            'IA_DWELL_TIME_%',\n            'IA_FIXATION_%',\n            'IA_FIRST_RUN_FIXATION_%',\n            'IA_FIRST_SACCADE_AMPLITUDE',\n            'IA_FIRST_SACCADE_ANGLE',\n            'IA_LAST_RUN_FIXATION_%',\n            'IA_LAST_SACCADE_AMPLITUDE',\n            'IA_LAST_SACCADE_ANGLE',\n            'IA_FIRST_RUN_LANDING_POSITION',\n            'IA_LAST_RUN_LANDING_POSITION',\n        ]\n    elif args.mode == Mode.FIXATION:\n        to_float_features = [\n            FIXATION_ID_COL,\n            NEXT_FIXATION_ID_COL,\n            'NEXT_FIX_ANGLE',\n            'PREVIOUS_FIX_ANGLE',\n            'NEXT_FIX_DISTANCE',\n            'PREVIOUS_FIX_DISTANCE',\n            'NEXT_SAC_AMPLITUDE',\n            'NEXT_SAC_ANGLE',\n            'NEXT_SAC_AVG_VELOCITY',\n            'NEXT_SAC_PEAK_VELOCITY',\n            'NEXT_SAC_END_X',\n            'NEXT_SAC_START_X',\n            'NEXT_SAC_END_Y',\n            'NEXT_SAC_START_Y',\n        ]\n    else:\n        raise ValueError(f'Unknown mode: {args.mode}')\n    df[to_float_features] = (\n        df[to_float_features].replace(to_replace={'.': None}).astype(float)\n    )\n    logger.info(\n        \"%s fields converted to float, nan ('.') values replaced with None.\",\n        to_float_features,\n    )\n    return df\n</code></pre>"},{"location":"reference/data/preprocessing/dataset_preprocessing/onestop/#data.preprocessing.dataset_preprocessing.onestop.convert_to_int_features","title":"<code>convert_to_int_features(df, args)</code>","text":"<p>Convert specified columns to integer type.</p> <p>Handles missing values and dots by replacing them with 0 before conversion. Different columns are processed based on whether in IA or FIXATION mode.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Input DataFrame</p> required <code>args</code> <code>ArgsParser</code> <p>Contains mode configuration</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: DataFrame with converted integer columns</p> Source code in <code>src/data/preprocessing/dataset_preprocessing/onestop.py</code> <pre><code>def convert_to_int_features(df: pd.DataFrame, args: ArgsParser) -&gt; pd.DataFrame:\n    \"\"\"\n    Convert specified columns to integer type.\n\n    Handles missing values and dots by replacing them with 0 before conversion.\n    Different columns are processed based on whether in IA or FIXATION mode.\n\n    Args:\n        df (pd.DataFrame): Input DataFrame\n        args (ArgsParser): Contains mode configuration\n\n    Returns:\n        pd.DataFrame: DataFrame with converted integer columns\n    \"\"\"\n    # In general, only features that have '.' or NaN or not automatically converted.\n\n    to_int_features = [\n        'article_batch',\n        'article_id',\n        'paragraph_id',\n        'repeated_reading_trial',\n        'practice_trial',\n        # \"question_preview\",\n    ]\n    if args.mode == Mode.IA:\n        to_int_features += [\n            'IA_DWELL_TIME',\n            'IA_FIRST_FIXATION_DURATION',\n            'IA_REGRESSION_PATH_DURATION',\n            'IA_FIRST_RUN_DWELL_TIME',\n            'IA_FIXATION_COUNT',\n            'IA_REGRESSION_IN_COUNT',\n            'IA_REGRESSION_OUT_FULL_COUNT',\n            'IA_RUN_COUNT',\n            'IA_FIRST_FIXATION_VISITED_IA_COUNT',\n            'IA_FIRST_RUN_FIXATION_COUNT',\n            'IA_SKIP',\n            'IA_REGRESSION_OUT_COUNT',\n            'IA_SELECTIVE_REGRESSION_PATH_DURATION',\n            'IA_SPILLOVER',\n            'IA_LAST_FIXATION_DURATION',\n            'IA_LAST_RUN_DWELL_TIME',\n            'IA_LAST_RUN_FIXATION_COUNT',\n            'IA_LEFT',\n            'IA_TOP',\n            'TRIAL_DWELL_TIME',\n            'TRIAL_FIXATION_COUNT',\n            'TRIAL_IA_COUNT',\n            'TRIAL_INDEX',\n            'TRIAL_TOTAL_VISITED_IA_COUNT',\n            'IA_FIRST_FIX_PROGRESSIVE',\n        ]\n    elif args.mode == Mode.FIXATION:\n        to_int_features += [\n            FIXATION_ID_COL,\n            NEXT_FIXATION_ID_COL,\n            'CURRENT_FIX_DURATION',\n            'CURRENT_FIX_PUPIL',\n            'CURRENT_FIX_X',\n            'CURRENT_FIX_Y',\n            'CURRENT_FIX_INDEX',\n            'NEXT_SAC_DURATION',\n        ]\n    df[to_int_features] = df[to_int_features].replace({'.': 0, np.nan: 0}).astype(int)\n    logger.info(\n        \"%s fields converted to int, nan ('.') values replaced with 0.\", to_int_features\n    )\n    return df\n</code></pre>"},{"location":"reference/data/preprocessing/dataset_preprocessing/onestop/#data.preprocessing.dataset_preprocessing.onestop.drop_missing_fixation_data","title":"<code>drop_missing_fixation_data(df, args)</code>","text":"<p>Drop rows with missing fixation data.</p> <p>Drops rows with missing values in specified columns for FIXATION mode.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Input DataFrame</p> required <code>args</code> <code>ArgsParser</code> <p>Contains mode configuration</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: DataFrame with dropped rows</p> Source code in <code>src/data/preprocessing/dataset_preprocessing/onestop.py</code> <pre><code>def drop_missing_fixation_data(df: pd.DataFrame, args: ArgsParser) -&gt; pd.DataFrame:\n    \"\"\"\n    Drop rows with missing fixation data.\n\n    Drops rows with missing values in specified columns for FIXATION mode.\n\n    Args:\n        df (pd.DataFrame): Input DataFrame\n        args (ArgsParser): Contains mode configuration\n\n    Returns:\n        pd.DataFrame: DataFrame with dropped rows\n    \"\"\"\n    if args.mode == Mode.FIXATION:\n        dropna_fields = [FIXATION_ID_COL, NEXT_FIXATION_ID_COL]\n        df = df.dropna(subset=dropna_fields)\n        logger.info(\n            'After dropping rows with missing data in %s: %d records left in total.',\n            dropna_fields,\n            len(df),\n        )\n    return df\n</code></pre>"},{"location":"reference/data/preprocessing/dataset_preprocessing/onestop/#data.preprocessing.dataset_preprocessing.onestop.get_article_data","title":"<code>get_article_data(article_id, raw_text)</code>","text":"<p>Retrieve article data from raw text by article ID.</p> <p>Parameters:</p> Name Type Description Default <code>article_id</code> <code>str</code> <p>Article identifier to look up</p> required <code>raw_text</code> <code>dict</code> <p>Raw text data containing articles</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Article data if found</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If article ID not found</p> Source code in <code>src/data/preprocessing/dataset_preprocessing/onestop.py</code> <pre><code>def get_article_data(article_id: str, raw_text) -&gt; dict:\n    \"\"\"\n    Retrieve article data from raw text by article ID.\n\n    Args:\n        article_id (str): Article identifier to look up\n        raw_text (dict): Raw text data containing articles\n\n    Returns:\n        dict: Article data if found\n\n    Raises:\n        ValueError: If article ID not found\n    \"\"\"\n    for article in raw_text:\n        if article['article_id'] == article_id:\n            return article\n    raise ValueError(f'Article id {article_id} not found')\n</code></pre>"},{"location":"reference/data/preprocessing/dataset_preprocessing/onestop/#data.preprocessing.dataset_preprocessing.onestop.get_constants_by_mode","title":"<code>get_constants_by_mode(mode)</code>","text":"<p>Get constants based on processing mode.</p> <p>Returns duration and IA field names based on whether in IA or FIXATION mode.</p> <p>Parameters:</p> Name Type Description Default <code>mode</code> <code>Mode</code> <p>Processing mode (IA or FIXATION)</p> required <p>Returns:</p> Type Description <code>tuple[str, str]</code> <p>tuple[str, str]: Duration and IA field names</p> Source code in <code>src/data/preprocessing/dataset_preprocessing/onestop.py</code> <pre><code>def get_constants_by_mode(mode: Mode) -&gt; tuple[str, str]:\n    \"\"\"\n    Get constants based on processing mode.\n\n    Returns duration and IA field names based on whether in IA or FIXATION mode.\n\n    Args:\n        mode (Mode): Processing mode (IA or FIXATION)\n\n    Returns:\n        tuple[str, str]: Duration and IA field names\n    \"\"\"\n    duration_field = 'IA_DWELL_TIME' if mode == Mode.IA else 'CURRENT_FIX_DURATION'\n    ia_field = IA_ID_COL if mode == Mode.IA else FIXATION_ID_COL\n\n    return duration_field, ia_field\n</code></pre>"},{"location":"reference/data/preprocessing/dataset_preprocessing/onestop/#data.preprocessing.dataset_preprocessing.onestop.get_raw_text","title":"<code>get_raw_text(args)</code>","text":"<p>Load raw text data from OneStopQA JSON file.</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <code>object</code> <p>Configuration containing onestopqa_path</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Raw text data from OneStopQA JSON</p> Source code in <code>src/data/preprocessing/dataset_preprocessing/onestop.py</code> <pre><code>def get_raw_text(args: object) -&gt; dict:\n    \"\"\"\n    Load raw text data from OneStopQA JSON file.\n\n    Args:\n        args: Configuration containing onestopqa_path\n\n    Returns:\n        dict: Raw text data from OneStopQA JSON\n    \"\"\"\n    with open(\n        file=args.onestopqa_path,\n        mode='r',\n        encoding='utf-8',\n    ) as f:\n        raw_text = json.load(f)\n    return raw_text['data']\n</code></pre>"},{"location":"reference/data/preprocessing/dataset_preprocessing/onestop/#data.preprocessing.dataset_preprocessing.onestop.our_processing","title":"<code>our_processing(df, args)</code>","text":"<p>LaCC lab-specific processing pipeline for OneStop dataset.</p> <p>Extends the public dataset with additional features including: - Integer and float feature conversions - Index adjustments - Fixation data cleaning - Unique paragraph ID addition - Word span metrics computation - Span-level metrics computation - Feature normalization - Question difficulty data integration - Previous word metrics (for IA mode) - Line position metrics (for IA mode)</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Input DataFrame from public preprocessing</p> required <code>args</code> <code>ArgsParser</code> <p>Configuration parameters</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: Extended DataFrame with LaCC lab features</p> Source code in <code>src/data/preprocessing/dataset_preprocessing/onestop.py</code> <pre><code>def our_processing(df: pd.DataFrame, args: ArgsParser) -&gt; pd.DataFrame:\n    \"\"\"\n    LaCC lab-specific processing pipeline for OneStop dataset.\n\n    Extends the public dataset with additional features including:\n    - Integer and float feature conversions\n    - Index adjustments\n    - Fixation data cleaning\n    - Unique paragraph ID addition\n    - Word span metrics computation\n    - Span-level metrics computation\n    - Feature normalization\n    - Question difficulty data integration\n    - Previous word metrics (for IA mode)\n    - Line position metrics (for IA mode)\n\n    Args:\n        df (pd.DataFrame): Input DataFrame from public preprocessing\n        args (ArgsParser): Configuration parameters\n\n    Returns:\n        pd.DataFrame: Extended DataFrame with LaCC lab features\n    \"\"\"\n\n    duration_field, ia_field = get_constants_by_mode(args.mode)\n\n    df = convert_to_int_features(df, args)\n    df = convert_to_float_features(df, args)\n    df = adjust_indexing(df, args)\n    df = drop_missing_fixation_data(df, args)\n    df = add_unique_paragraph_id(df)\n    df = compute_span_level_metrics(df, ia_field, args.mode, duration_field)\n    df = compute_normalized_features(df, duration_field, ia_field)\n    if args.mode == Mode.IA:\n        df = compute_start_end_line(df)\n        df = add_additional_metrics(df)\n\n    return df\n</code></pre>"},{"location":"reference/data/preprocessing/dataset_preprocessing/potec/","title":"potec","text":""},{"location":"reference/data/preprocessing/dataset_preprocessing/potec/#data.preprocessing.dataset_preprocessing.potec.PoTeCProcessor","title":"<code>PoTeCProcessor</code>","text":"<p>               Bases: <code>DatasetProcessor</code></p> <p>Processor for PoTeC dataset with optimized performance</p> Source code in <code>src/data/preprocessing/dataset_preprocessing/potec.py</code> <pre><code>class PoTeCProcessor(DatasetProcessor):\n    \"\"\"Processor for PoTeC dataset with optimized performance\"\"\"\n\n    IA_COLUMN_MAP = {\n        'word': 'IA_LABEL',\n        'FFD': 'IA_FIRST_FIXATION_DURATION',\n        'text_surprisal_gpt2-base': 'gpt2_surprisal',\n        'RPD_in': 'IA_REGRESSION_IN_COUNT',\n        'document_frequency_normalized': 'wordfreq_frequency',\n        'text_id': Fields.UNIQUE_PARAGRAPH_ID,\n        'PoS_tag': 'universal_pos',\n        'FD': 'IA_DWELL_TIME',\n        'TFC': 'IA_FIXATION_COUNT',\n        'FPRT': 'IA_FIRST_RUN_DWELL_TIME',\n        'word_index_in_text': Fields.IA_DATA_IA_ID_COL_NAME,\n        'reader_id': 'participant_id',\n    }\n\n    FIXATION_COLUMN_MAP = {\n        'fixation_index': 'CURRENT_FIX_INDEX',\n        'fixation_duration': 'CURRENT_FIX_DURATION',\n        'fixation_position_x': 'CURRENT_FIX_X',\n        'fixation_position_y': 'CURRENT_FIX_Y',\n        'aoi': 'IA_ID',\n        'next_saccade_duration': 'NEXT_SAC_DURATION',\n        'reader_id': Fields.SUBJECT_ID,\n        'item_id': Fields.UNIQUE_PARAGRAPH_ID,\n    }\n\n    LINGUISTIC_FEATURE_RENAMES = {\n        'POS': 'universal_pos',\n        'Length': 'word_length_no_punctuation',\n        'subtlex_Frequency': 'subtlex_frequency',\n        'Reduced_POS': 'ptb_pos',\n        'Head_word_idx': 'head_word_index',\n        'Dependency_Relation': 'dependency_relation',\n        'Entity': 'entity_type',\n        'Head_Direction': 'head_direction',\n        'Is_Content_Word': 'is_content_word',\n        'n_Lefts': 'left_dependents_count',\n        'n_Rights': 'right_dependents_count',\n        'Distance2Head': 'distance_to_head',\n    }\n\n    def get_column_map(self, data_type: DataType) -&gt; dict[str, str]:\n        return (\n            self.IA_COLUMN_MAP if data_type == DataType.IA else self.FIXATION_COLUMN_MAP\n        )\n\n    def get_columns_to_keep(self) -&gt; list[str]:\n        return []\n\n    def _build_stim_dict(self, stim_df: pd.DataFrame) -&gt; dict[str, pd.DataFrame]:\n        return {name: data for name, data in stim_df.groupby('text_name')}\n\n    def _process_single_page(\n        self,\n        page_name: str,\n        page_fixes: pd.DataFrame,\n        stim_dict: dict[str, pd.DataFrame],\n    ) -&gt; tuple:\n        if page_name not in stim_dict:\n            return None, None, None, None, None\n\n        page_stim = stim_dict[page_name]\n        page_indices = page_fixes.index\n        n_fixes = len(page_indices)\n\n        word_values = np.full(n_fixes, '.', dtype=object)\n        aoi_values = np.full(n_fixes, np.nan)\n        start_line = np.full(n_fixes, False)\n        end_line = np.full(n_fixes, False)\n\n        fix_coords = page_fixes.loc[\n            page_indices, ['CURRENT_FIX_X', 'CURRENT_FIX_Y']\n        ].values\n\n        for i, (fix_x, fix_y) in enumerate(fix_coords):\n            x_match = (page_stim['start_x'] &lt;= fix_x) &amp; (fix_x &lt; page_stim['end_x'])\n            y_match = (page_stim['start_y'] &lt;= fix_y) &amp; (fix_y &lt; page_stim['end_y'])\n            aoi_match = page_stim[x_match &amp; y_match]\n\n            if not aoi_match.empty:\n                first_match = aoi_match.iloc[0]\n                word_values[i] = first_match['character']\n                aoi_values[i] = int(first_match['aoi'])\n\n        return page_indices, word_values, aoi_values, start_line, end_line\n\n    def map_to_aois(self, fix_df: pd.DataFrame, stim_df: pd.DataFrame) -&gt; pd.DataFrame:\n        fix_df['word'] = '.'\n        fix_df['CURRENT_FIX_INTEREST_AREA_INDEX'] = np.nan\n        fix_df['start_of_line'] = False\n        fix_df['end_of_line'] = False\n\n        stim_dict = self._build_stim_dict(stim_df)\n        page_groups = list(fix_df.groupby(Fields.UNIQUE_PARAGRAPH_ID))\n\n        results = Parallel(n_jobs=-1, backend='threading')(\n            delayed(self._process_single_page)(page_name, page_fixes, stim_dict)\n            for page_name, page_fixes in tqdm(\n                page_groups, desc='Mapping fixations to AOIs'\n            )\n        )\n\n        for page_indices, word_values, aoi_values, start_line, end_line in results:\n            if page_indices is not None:\n                fix_df.loc[page_indices, 'word'] = word_values\n                fix_df.loc[page_indices, 'CURRENT_FIX_INTEREST_AREA_INDEX'] = aoi_values\n                fix_df.loc[page_indices, 'start_of_line'] = start_line\n                fix_df.loc[page_indices, 'end_of_line'] = end_line\n\n        return fix_df\n\n    def _extract_linguistic_features_for_group(\n        self, group: pd.DataFrame, nlp, surp_extractor\n    ) -&gt; pd.DataFrame:\n        \"\"\"Extract linguistic features for a single group\"\"\"\n        words = group['IA_LABEL'].fillna('Null').tolist()\n        sentence = ' '.join(words)\n\n        metrics = get_metrics(\n            target_text=sentence,\n            parsing_model=nlp,\n            surp_extractor=surp_extractor,\n            parsing_mode='re-tokenize',\n            add_parsing_features=True,\n            language='de',\n        )\n\n        metrics['unique_paragraph_id'] = group['unique_paragraph_id'].iloc[0]\n        metrics['unique_trial_id'] = group['unique_trial_id'].iloc[0]\n        metrics[Fields.FIXATION_REPORT_IA_ID_COL_NAME] = metrics['Token_idx']\n\n        return metrics\n\n    def _initialize_missing_columns(\n        self, ia_df: pd.DataFrame, fix_df: pd.DataFrame\n    ) -&gt; tuple[pd.DataFrame, pd.DataFrame]:\n        ia_zero_cols = [\n            'IA_FIRST_RUN_FIXATION_COUNT',\n            'IA_FIRST_FIXATION_VISITED_IA_COUNT',\n            'IA_REGRESSION_IN_COUNT',\n            'TRIAL_IA_COUNT',\n            'IA_SELECTIVE_REGRESSION_PATH_DURATION',\n            'IA_LAST_RUN_FIXATION_COUNT',\n            'IA_TOP',\n            'IA_LAST_RUN_DWELL_TIME',\n            'IA_REGRESSION_OUT_FULL_COUNT',\n            'start_of_line',\n            'end_of_line',\n            'IA_LEFT',\n            'IA_LAST_FIXATION_DURATION',\n            'IA_REGRESSION_PATH_DURATION',\n            'IA_REGRESSION_OUT_COUNT',\n            'IA_FIRST_FIX_PROGRESSIVE',\n            'IA_RUN_COUNT',\n            'NEXT_SAC_START_Y',\n            'NEXT_SAC_START_X',\n            'NEXT_SAC_END_X',\n            'NEXT_SAC_END_Y',\n        ]\n        for col in ia_zero_cols:\n            ia_df[col] = 0\n\n        fix_zero_cols = ['NEXT_SAC_AVG_VELOCITY', 'NEXT_SAC_AMPLITUDE']\n\n        for col in fix_zero_cols:\n            fix_df[col] = 0\n\n        ia_df['normalized_ID'] = ia_df['CURRENT_FIX_INTEREST_AREA_INDEX']\n        ia_df['PARAGRAPH_RT'] = ia_df.groupby('unique_trial_id')[\n            'IA_DWELL_TIME'\n        ].transform('sum')\n        ia_df['IA_DWELL_TIME_%'] = ia_df.groupby('unique_trial_id')[\n            'IA_DWELL_TIME'\n        ].transform(lambda x: x / np.sum(x))\n        ia_df['total_skip'] = (ia_df['IA_DWELL_TIME'] &gt; 0).astype(int)\n        ia_df['IA_SKIP'] = (ia_df['IA_DWELL_TIME'] &gt; 0).astype(int)\n        ia_df['IA_FIXATION_%'] = ia_df.groupby('unique_trial_id')[\n            'IA_FIXATION_COUNT'\n        ].transform(lambda x: x / np.sum(x))\n\n        return ia_df, fix_df\n\n    def add_ia_report_features_to_fixation_data(\n        self,\n        ia_df: pd.DataFrame,\n        fix_df: pd.DataFrame,\n    ) -&gt; tuple[pd.DataFrame, pd.DataFrame]:\n        # remove duplicate groupby columns\n        if len(set(self.data_args.groupby_columns)) != len(\n            self.data_args.groupby_columns\n        ):\n            logger.warning('Removing duplicate groupby_columns')\n            self.data_args.groupby_columns = list(\n                dict.fromkeys(self.data_args.groupby_columns)\n            )\n\n        # unify IA ID column name\n        ia_df = ia_df.rename(\n            columns={\n                Fields.IA_DATA_IA_ID_COL_NAME: Fields.FIXATION_REPORT_IA_ID_COL_NAME\n            }\n        )\n\n        # prepare IA dataframe\n        ia_df['IA_LABEL'] = ia_df['IA_LABEL'].fillna('Null')\n        paragraphs = ia_df.groupby('unique_trial_id')['IA_LABEL'].apply(\n            lambda x: ' '.join(x)\n        )\n        ia_df['paragraph'] = ia_df['unique_trial_id'].map(paragraphs)\n        ia_df['CURRENT_FIX_NEAREST_INTEREST_AREA_DISTANCE'] = 0\n\n        # extract linguistic features\n        logger.info(\n            'Processing linguistic features in batches. This might take a while.'\n        )\n        nlp = spacy.load('de_core_news_sm')\n        surp_extractor = get_surp_extractor(\n            extractor_type=SurpExtractorType.CAT_CTX_LEFT,\n            model_name='gpt2',\n        )\n\n        groups = [group for _, group in ia_df.groupby('unique_trial_id')]\n        metrics_list = []\n        batch_size = 100\n\n        for i in tqdm(range(0, len(groups), batch_size), desc='Processing batches'):\n            batch = Parallel(n_jobs=-1, backend='threading')(\n                delayed(self._extract_linguistic_features_for_group)(\n                    g, nlp, surp_extractor\n                )\n                for g in groups[i : i + batch_size]\n            )\n            metrics_list.extend(batch)\n\n        metrics_df = pd.concat(metrics_list, ignore_index=True)\n\n        # merge linguistic features\n        merge_keys = ['unique_trial_id', Fields.FIXATION_REPORT_IA_ID_COL_NAME]\n        drop_cols = (set(ia_df.columns) &amp; set(metrics_df.columns)) - set(merge_keys)\n        ia_df = ia_df.merge(\n            metrics_df.drop(columns=list(drop_cols) + ['Morph']).drop_duplicates(),\n            on=merge_keys,\n            how='left',\n        )\n\n        # rename linguistic feature columns\n        ia_df = ia_df.rename(columns=self.LINGUISTIC_FEATURE_RENAMES)\n\n        # initialize missing columns\n        ia_df, fix_df = self._initialize_missing_columns(ia_df, fix_df)\n\n        # add stratify column DE_RC\n        ia_df['DE_RC'] = ia_df['DE'].astype(str) + '_' + ia_df['RC'].astype(str)\n        fix_df['DE_RC'] = fix_df['DE'].astype(str) + '_' + fix_df['RC'].astype(str)\n\n        # map fixations to aois\n        word_aoi_df = self._read_word_aoi_df()\n        fix_df = self.map_to_aois(fix_df, word_aoi_df)\n        fix_df['NEXT_FIX_INTEREST_AREA_INDEX'] = fix_df[\n            'CURRENT_FIX_INTEREST_AREA_INDEX'\n        ].shift(-1)\n\n        # prepare merge\n        merge_keys = self.data_args.groupby_columns + [\n            Fields.FIXATION_REPORT_IA_ID_COL_NAME\n        ]\n        dup_cols = (set(fix_df.columns) &amp; set(ia_df.columns)) - set(merge_keys)\n        _ia_df = ia_df.drop(columns=list(dup_cols))\n\n        if 'normalized_part_ID' in fix_df.columns:\n            fix_df = fix_df.drop(columns='normalized_part_ID')\n\n        enriched_fix_df = fix_df.merge(\n            _ia_df.drop_duplicates(subset=merge_keys, keep='first'),\n            on=merge_keys,\n            how='left',\n            validate='many_to_one',\n        )\n\n        num_words = (\n            ia_df.groupby(self.data_args.groupby_columns)\n            .size()\n            .rename('num_of_words_in_trial')\n        )\n        enriched_fix_df = enriched_fix_df.merge(\n            num_words,\n            on=self.data_args.groupby_columns,\n            how='left',\n        )\n\n        enriched_fix_df['normalized_ID'] = enriched_fix_df[\n            'CURRENT_FIX_INTEREST_AREA_INDEX'\n        ]\n\n        return enriched_fix_df, ia_df\n\n    def _read_word_aoi_df(self) -&gt; pd.DataFrame:\n        stim_path = 'data/PoTeC/stimuli'\n        dfs = []\n\n        for file_name in os.listdir(stim_path):\n            if file_name.startswith('word_aoi_'):\n                file_path = os.path.join(stim_path, file_name)\n                tmp_df = pd.read_csv(file_path, delimiter='\\t')\n                tmp_df['text_name'] = file_name.removesuffix('.tsv').split('_')[-1]\n                dfs.append(tmp_df)\n\n        return pd.concat(dfs, ignore_index=True)\n\n    def _load_and_merge_labels(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n        # participant data\n        label_df = pd.read_csv(\n            'data/PoTeC/labels/participant_data.tsv',\n            delimiter='\\t',\n        ).rename(columns={'reader_id': Fields.SUBJECT_ID})\n\n        merge_keys = [Fields.SUBJECT_ID]\n        drop_cols = (set(df.columns) &amp; set(label_df.columns)) - set(merge_keys)\n        df = df.merge(\n            label_df.drop(columns=list(drop_cols)),\n            on=merge_keys,\n            validate='many_to_one',\n        )\n\n        # response accuracy data\n        text_spec_df = pd.read_csv(\n            'data/PoTeC/labels/participant_response_accuracy.tsv',\n            delimiter='\\t',\n        ).rename(\n            columns={\n                'reader_id': Fields.SUBJECT_ID,\n                'text_id': Fields.UNIQUE_PARAGRAPH_ID,\n            }\n        )\n\n        # drop existing accuracy columns if present\n        acc_cols = ['mean_acc_bq', 'acc_tq_1', 'acc_tq_2', 'acc_tq_3']\n        df = df.drop(columns=[col for col in acc_cols if col in df.columns])\n\n        merge_keys = [Fields.SUBJECT_ID, Fields.UNIQUE_PARAGRAPH_ID]\n        drop_cols = (set(df.columns) &amp; set(text_spec_df.columns)) - set(merge_keys)\n        df = df.merge(\n            text_spec_df.drop(columns=list(drop_cols)),\n            on=merge_keys,\n            how='left',\n            validate='many_to_one',\n        )\n\n        # this is equivalent to everything correct\n        df['DE'] = (df['mean_acc_bq'] &gt; 0.9).astype(int)\n\n        return df\n\n    def _expand_for_questions(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"Expand dataframe to have separate rows for each question\"\"\"\n        dfs = []\n        question_indices = ['tq_1', 'tq_2', 'tq_3']\n\n        for q in tqdm(question_indices, desc='Expanding for questions'):\n            df_temp = df.copy()\n            df_temp['RC'] = df_temp[f'acc_{q}']\n            df_temp['question_index'] = q\n            df_temp['question'] = df_temp[q]\n            dfs.append(df_temp)\n\n        df = pd.concat(dfs, ignore_index=True)\n        df['unique_trial_id'] = (\n            df[Fields.SUBJECT_ID].astype(str)\n            + '_'\n            + df[Fields.UNIQUE_PARAGRAPH_ID].astype(str)\n            + '_'\n            + df['question_index']\n        )\n\n        return df\n\n    def dataset_specific_processing(\n        self, data_dict: dict[str, pd.DataFrame]\n    ) -&gt; dict[str, pd.DataFrame]:\n        \"\"\"PoTeC-specific processing pipeline\"\"\"\n        for data_type in [DataType.IA, DataType.FIXATIONS]:\n            logger.info(f'Processing {data_type} data')\n            if data_type not in data_dict or data_dict[data_type] is None:\n                continue\n\n            df = data_dict[data_type]\n\n            # exclude trials without comprehension scores\n            question_indices = ['tq_1', 'tq_2', 'tq_3']\n            acc_columns = [f'acc_{q}' for q in question_indices]\n            df = df.dropna(subset=acc_columns).copy()\n\n            # load and merge labels\n            df = self._load_and_merge_labels(df)\n\n            # merge stimuli data\n            stim_df = pd.read_csv(\n                'data/PoTeC/stimuli/stimuli.tsv', delimiter='\\t'\n            ).rename(\n                columns={\n                    'reader_id': Fields.SUBJECT_ID,\n                    'text_id': Fields.UNIQUE_PARAGRAPH_ID,\n                }\n            )\n            df = df.merge(\n                stim_df, on=[Fields.UNIQUE_PARAGRAPH_ID], validate='many_to_one'\n            )\n\n            # duplicate df for questions\n            df = self._expand_for_questions(df)\n\n            data_dict[data_type] = df\n\n        data_dict['fixations'], data_dict['ia'] = (\n            self.add_ia_report_features_to_fixation_data(\n                data_dict['ia'],\n                data_dict['fixations'],\n            )\n        )\n\n        for data_type in [DataType.IA, DataType.FIXATIONS]:\n            data_dict[data_type] = add_missing_features(\n                et_data=data_dict[data_type],\n                trial_groupby_columns=self.data_args.groupby_columns,\n                mode=data_type,\n            )\n\n        for data_type in data_dict.keys():\n            df = data_dict[data_type]\n            data_dict[data_type] = df.loc[:, ~df.columns.duplicated()]\n\n        trial_level_features = compute_trial_level_features(\n            raw_fixation_data=data_dict[DataType.FIXATIONS],\n            raw_ia_data=data_dict[DataType.IA],\n            trial_groupby_columns=self.data_args.groupby_columns,\n            processed_data_path=self.data_args.processed_data_path,\n        )\n        data_dict[DataType.TRIAL_LEVEL] = trial_level_features\n\n        data_dict = replace_missing_values(data_dict)\n\n        return data_dict\n</code></pre>"},{"location":"reference/data/preprocessing/dataset_preprocessing/potec/#data.preprocessing.dataset_preprocessing.potec.PoTeCProcessor.dataset_specific_processing","title":"<code>dataset_specific_processing(data_dict)</code>","text":"<p>PoTeC-specific processing pipeline</p> Source code in <code>src/data/preprocessing/dataset_preprocessing/potec.py</code> <pre><code>def dataset_specific_processing(\n    self, data_dict: dict[str, pd.DataFrame]\n) -&gt; dict[str, pd.DataFrame]:\n    \"\"\"PoTeC-specific processing pipeline\"\"\"\n    for data_type in [DataType.IA, DataType.FIXATIONS]:\n        logger.info(f'Processing {data_type} data')\n        if data_type not in data_dict or data_dict[data_type] is None:\n            continue\n\n        df = data_dict[data_type]\n\n        # exclude trials without comprehension scores\n        question_indices = ['tq_1', 'tq_2', 'tq_3']\n        acc_columns = [f'acc_{q}' for q in question_indices]\n        df = df.dropna(subset=acc_columns).copy()\n\n        # load and merge labels\n        df = self._load_and_merge_labels(df)\n\n        # merge stimuli data\n        stim_df = pd.read_csv(\n            'data/PoTeC/stimuli/stimuli.tsv', delimiter='\\t'\n        ).rename(\n            columns={\n                'reader_id': Fields.SUBJECT_ID,\n                'text_id': Fields.UNIQUE_PARAGRAPH_ID,\n            }\n        )\n        df = df.merge(\n            stim_df, on=[Fields.UNIQUE_PARAGRAPH_ID], validate='many_to_one'\n        )\n\n        # duplicate df for questions\n        df = self._expand_for_questions(df)\n\n        data_dict[data_type] = df\n\n    data_dict['fixations'], data_dict['ia'] = (\n        self.add_ia_report_features_to_fixation_data(\n            data_dict['ia'],\n            data_dict['fixations'],\n        )\n    )\n\n    for data_type in [DataType.IA, DataType.FIXATIONS]:\n        data_dict[data_type] = add_missing_features(\n            et_data=data_dict[data_type],\n            trial_groupby_columns=self.data_args.groupby_columns,\n            mode=data_type,\n        )\n\n    for data_type in data_dict.keys():\n        df = data_dict[data_type]\n        data_dict[data_type] = df.loc[:, ~df.columns.duplicated()]\n\n    trial_level_features = compute_trial_level_features(\n        raw_fixation_data=data_dict[DataType.FIXATIONS],\n        raw_ia_data=data_dict[DataType.IA],\n        trial_groupby_columns=self.data_args.groupby_columns,\n        processed_data_path=self.data_args.processed_data_path,\n    )\n    data_dict[DataType.TRIAL_LEVEL] = trial_level_features\n\n    data_dict = replace_missing_values(data_dict)\n\n    return data_dict\n</code></pre>"},{"location":"reference/data/preprocessing/dataset_preprocessing/sbsat/","title":"sbsat","text":""},{"location":"reference/data/preprocessing/dataset_preprocessing/sbsat/#data.preprocessing.dataset_preprocessing.sbsat.SBSATProcessor","title":"<code>SBSATProcessor</code>","text":"<p>               Bases: <code>DatasetProcessor</code></p> <p>Processor for SBSAT dataset</p> Source code in <code>src/data/preprocessing/dataset_preprocessing/sbsat.py</code> <pre><code>class SBSATProcessor(DatasetProcessor):\n    \"\"\"Processor for SBSAT dataset\"\"\"\n\n    N_QUESTION_DUPLICATES = 5\n\n    # Encoding fixes\n    ENCODING_MAP = {\n        '\\x92': \"'\",\n        '\\x93': '\"',\n        '\\x94': '\"',\n        '\\x97': '\u2014',\n    }\n\n    # AOI label corrections: (paragraph_id, aoi_id, correct_label)\n    AOI_FIXES = [\n        # https://github.com/ahnchive/SB-SAT/blob/master/stimuli/reading%20screenshot/reading-dickens-3.png\n        ('reading-dickens-3', 45, 'Sempere &amp;'),\n        # https://github.com/ahnchive/SB-SAT/blob/master/stimuli/reading%20screenshot/reading-dickens-5.png\n        ('reading-dickens-5', 112, 'Mr.'),\n        ('reading-dickens-5', 113, 'Dickens'),\n        # https://github.com/ahnchive/SB-SAT/blob/master/stimuli/reading%20screenshot/reading-flytrap-3.png\n        ('reading-flytrap-3', 30, 'Burdon-'),\n        ('reading-flytrap-3', 31, \"Sanderson's\"),\n        # https://github.com/ahnchive/SB-SAT/blob/master/stimuli/reading%20screenshot/reading-genome-2.png\n        ('reading-genome-2', 70, 'species\u2014'),\n        ('reading-genome-2', 71, 'in'),\n        # https://github.com/ahnchive/SB-SAT/blob/master/stimuli/reading%20screenshot/reading-genome-3.png\n        ('reading-genome-3', 45, 'gee-'),\n        ('reading-genome-3', 46, 'whiz,'),\n    ]\n\n    # Linguistic feature column renames\n    FEATURE_RENAMES = {\n        'POS': 'universal_pos',\n        'Length': 'word_length_no_punctuation',\n        'Wordfreq_Frequency': 'wordfreq_frequency',\n        'subtlex_Frequency': 'subtlex_frequency',\n        'Reduced_POS': 'ptb_pos',\n        'Head_word_idx': 'head_word_index',\n        'Dependency_Relation': 'dependency_relation',\n        'Entity': 'entity_type',\n        'gpt2_Surprisal': 'gpt2_surprisal',\n        'Head_Direction': 'head_direction',\n        'Is_Content_Word': 'is_content_word',\n        'n_Lefts': 'left_dependents_count',\n        'n_Rights': 'right_dependents_count',\n        'Distance2Head': 'distance_to_head',\n    }\n\n    def add_ia_report_features_to_fixation_data(\n        self,\n        ia_df: pd.DataFrame,\n        fix_df: pd.DataFrame,\n    ) -&gt; tuple[pd.DataFrame, pd.DataFrame]:\n        \"\"\"\n        Merge per-IA (interest area) features into the fixation-level data.\n        Result: one row per fixation, enriched with IA-level attributes.\n        \"\"\"\n        # Prepare IA dataframe\n        ia_df = self._prepare_ia_dataframe(ia_df)\n\n        # Prepare fixation dataframe\n        fix_df = self._prepare_fixation_dataframe(fix_df, ia_df)\n\n        # Load and merge labels\n        ia_df, fix_df = self._merge_trial_labels(ia_df, fix_df)\n\n        # Add IA features\n        ia_df = self._add_ia_features(ia_df)\n\n        # Compute linguistic features\n        ia_df = self._add_linguistic_features(ia_df)\n\n        # Finalize and merge\n        fix_df, ia_df = self._finalize_dataframes(ia_df, fix_df)\n\n        return fix_df, ia_df\n\n    def _prepare_ia_dataframe(self, ia_df: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"Prepare IA dataframe with metadata and reindexing\"\"\"\n        ia_df = ia_df.copy()\n        ia_df['old_original_word_index'] = ia_df['word_index']\n\n        # extract story metadata\n        ia_df['story_name'] = ia_df['unique_paragraph_id'].str.extract(\n            r'reading-(.*)-\\d+'\n        )[0]\n        ia_df['page_number'] = (\n            ia_df['unique_paragraph_id'].str.extract(r'reading-.*-(\\d+)')[0].astype(int)\n        )\n\n        # sort and reconstruct full paragraphs\n        ia_df = ia_df.sort_values(\n            by=[\n                'participant_id',\n                'story_name',\n                'page_number',\n                'old_original_word_index',\n            ]\n        )\n\n        full_paragraphs = ia_df.groupby(['participant_id', 'story_name']).agg(\n            {\n                'word': lambda x: ' '.join(x),\n                'old_original_word_index': lambda x: list(x),\n            }\n        )\n\n        ia_df = ia_df.merge(\n            full_paragraphs,\n            how='left',\n            on=['participant_id', 'story_name'],\n            suffixes=('', '_combined'),\n        ).rename(\n            columns={\n                'word_combined': 'full_paragraph',\n                'old_original_word_index_combined': 'full_old_original_word_index',\n            }\n        )\n\n        # Update paragraph ID and reindex\n        ia_df['unique_paragraph_id'] = ia_df['story_name']\n        ia_df = self._reindex_by_group(\n            ia_df,\n            ['participant_id', 'unique_paragraph_id'],\n            ['page_number', 'word_index'],\n            'word_index',\n        )\n\n        # Rename IA ID column\n        ia_df = ia_df.rename(\n            columns={\n                Fields.IA_DATA_IA_ID_COL_NAME: Fields.FIXATION_REPORT_IA_ID_COL_NAME\n            }\n        )\n\n        return ia_df\n\n    def _prepare_fixation_dataframe(\n        self, fix_df: pd.DataFrame, ia_df: pd.DataFrame\n    ) -&gt; pd.DataFrame:\n        \"\"\"Prepare fixation dataframe with metadata and reindexing\"\"\"\n        fix_df = fix_df.copy()\n        fix_df['old_original_word_index'] = fix_df['word_index']\n\n        # Extract story metadata\n        fix_df['story_name'] = fix_df['unique_paragraph_id'].str.extract(\n            r'reading-(.*)-\\d+'\n        )[0]\n        fix_df['page_number'] = (\n            fix_df['unique_paragraph_id']\n            .str.extract(r'reading-.*-(\\d+)')[0]\n            .astype(int)\n        )\n        fix_df['unique_paragraph_id'] = fix_df['story_name']\n\n        # Reset index and reindex by group\n        fix_df = fix_df.reset_index().rename(\n            columns={'index': 'original_fixation_index'}\n        )\n        fix_df = self._reindex_by_group(\n            fix_df,\n            ['participant_id', 'unique_paragraph_id'],\n            ['page_number', 'original_fixation_index'],\n            'CURRENT_FIX_INDEX',\n        )\n\n        # Sort and merge full paragraphs\n        fix_df = fix_df.sort_values(\n            by=['participant_id', 'story_name', 'page_number', 'CURRENT_FIX_INDEX']\n        )\n\n        full_paragraphs = (\n            ia_df.groupby(['participant_id', 'story_name'])\n            .agg(\n                {\n                    'word': lambda x: ' '.join(x),\n                    'old_original_word_index': lambda x: list(x),\n                }\n            )\n            .reset_index()\n        )\n\n        fix_df = fix_df.merge(\n            full_paragraphs,\n            how='left',\n            on=['participant_id', 'story_name'],\n            suffixes=('', '_combined'),\n        ).rename(\n            columns={\n                'word_combined': 'full_paragraph',\n                'old_original_word_index_combined': 'full_old_original_word_index',\n            }\n        )\n\n        return fix_df\n\n    def _reindex_by_group(\n        self,\n        df: pd.DataFrame,\n        groupby_cols: list[str],\n        sort_cols: list[str],\n        index_col: str,\n    ) -&gt; pd.DataFrame:\n        \"\"\"Reindex dataframe within groups\"\"\"\n\n        def reindex_group(group):\n            group = group.copy().sort_values(by=sort_cols)\n            group[index_col] = range(len(group))\n            return group\n\n        return df.groupby(groupby_cols, group_keys=False).apply(reindex_group)\n\n    def _add_ia_features(self, ia_df: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"Add all IA-level features with default values\"\"\"\n        feature_defaults = [\n            'IA_FIRST_FIXATION_VISITED_IA_COUNT',\n            'TRIAL_IA_COUNT',\n            'IA_SELECTIVE_REGRESSION_PATH_DURATION',\n            'IA_LAST_RUN_FIXATION_COUNT',\n            'IA_TOP',\n            'IA_LAST_RUN_DWELL_TIME',\n            'IA_REGRESSION_OUT_FULL_COUNT',\n            'start_of_line',\n            'end_of_line',\n            'IA_LEFT',\n            'IA_LAST_FIXATION_DURATION',\n            'IA_REGRESSION_PATH_DURATION',\n            'IA_REGRESSION_OUT_COUNT',\n            'IA_FIRST_FIX_PROGRESSIVE',\n            'IA_RUN_COUNT',\n            'NEXT_SAC_DURATION',\n        ]\n\n        # Add features with special calculations\n        ia_df['IA_REGRESSION_PATH_DURATION'] = ia_df['RPD_inc']\n        ia_df['IA_FIRST_RUN_FIXATION_COUNT'] = ia_df['FFID'].apply(\n            lambda x: 1 if x &gt; 0 else 0\n        )\n        ia_df['IA_FIRST_FIXATION_TIME'] = ia_df['FFD']\n        ia_df['IA_FIRST_FIXATION_DURATION'] = ia_df['FFD']\n        ia_df['IA_REGRESSION_IN_COUNT'] = ia_df['TRC_in']\n        ia_df['IA_REGRESSION_IN'] = ia_df['TRC_in']\n        ia_df['NEXT_SAC_END_X'] = ia_df['NEXT_SAC_START_X'] + ia_df['SL_out']\n        ia_df['NEXT_SAC_END_Y'] = ia_df['NEXT_SAC_START_Y'] + ia_df['SL_out']\n        ia_df['IA_REGRESSION_OUT_COUNT'] = ia_df['TRC_out']\n        ia_df['IA_REGRESSION_OUT'] = ia_df['TRC_out']\n        ia_df['IA_DWELL_TIME'] = ia_df['TFT']\n        ia_df['IA_DWELL_TIME_%'] = ia_df.groupby('unique_trial_id')[\n            'IA_DWELL_TIME'\n        ].transform(lambda x: x / x.sum())\n        ia_df['IA_FIRST_RUN_FIXATION_%'] = ia_df.groupby('unique_trial_id')[\n            'FFD'\n        ].transform(lambda x: x / x.sum())\n        ia_df['PARAGRAPH_RT'] = ia_df.groupby(Fields.UNIQUE_PARAGRAPH_ID)[\n            'TFT'\n        ].transform('sum')\n        ia_df['IA_FIXATION_COUNT'] = ia_df['TFC']\n        ia_df['IA_FIXATION_%'] = ia_df.groupby('unique_trial_id')['TFC'].transform(\n            lambda x: x / x.sum()\n        )\n        ia_df['IA_RUN_COUNT'] = ia_df['word_index']\n        ia_df['IA_FIRST_FIXATION_DURATION'] = ia_df['FFD']\n\n        # Add all default features\n        for feature in feature_defaults:\n            if feature not in ia_df.columns:\n                ia_df[feature] = 0\n\n        return ia_df\n\n    def _merge_trial_labels(\n        self,\n        ia_df: pd.DataFrame,\n        fix_df: pd.DataFrame,\n    ) -&gt; tuple[pd.DataFrame, pd.DataFrame]:\n        \"\"\"Merge trial labels and duplicate for multiple questions\"\"\"\n        # Load labels\n        labels_df = pd.read_csv('data/SBSAT/labels/18sat_trialfinal.csv').rename(\n            columns={\n                'RECORDING_SESSION_LABEL': Fields.SUBJECT_ID,\n                'page_name': Fields.UNIQUE_PARAGRAPH_ID,\n            }\n        )\n\n        # Drop conflicting columns from fixation data\n        fix_df = fix_df.drop(columns=['correct_answer', 'answer'], errors='ignore')\n\n        # Duplicate both dataframes for multiple questions\n        fix_df = self._duplicate_df(fix_df, self.N_QUESTION_DUPLICATES)\n        fix_df['unique_trial_id'] = (\n            fix_df['participant_id'].astype(str)\n            + '_'\n            + fix_df['unique_paragraph_id'].astype(str)\n            + '_'\n            + fix_df['question_index'].astype(str)\n        )\n\n        # Prepare labels\n        labels_df = labels_df.loc[labels_df['correct_answer'] != -99].copy()\n        labels_df['question_index'] = labels_df['unique_paragraph_id'].apply(\n            lambda x: int(x.split('-')[-1])\n        )\n        labels_df[Fields.UNIQUE_PARAGRAPH_ID] = labels_df['unique_paragraph_id'].apply(\n            lambda x: x.split('-')[1]\n        )\n        labels_df.drop('page', axis=1, inplace=True, errors='ignore')\n\n        # Merge fixation labels\n        merge_keys = {'book', 'participant_id', 'question_index'}\n        drop_keys = (\n            (set(fix_df.columns) &amp; set(labels_df.columns))\n            - merge_keys\n            - {'correct_answer', 'answer'}\n        )\n        fix_df = fix_df.merge(\n            labels_df.drop(columns=list(drop_keys)).drop_duplicates(),\n            on=list(merge_keys),\n            how='left',\n            validate='many_to_one',\n        )\n\n        # Prepare and merge IA labels\n        ia_df = self._duplicate_df(ia_df, self.N_QUESTION_DUPLICATES)\n        ia_df['unique_trial_id'] = (\n            ia_df['participant_id'].astype(str)\n            + '_'\n            + ia_df['unique_paragraph_id'].astype(str)\n            + '_'\n            + ia_df['question_index'].astype(str)\n        )\n        ia_df['book_name'] = ia_df['story_name']\n        ia_df['page'] = ia_df['page_number']\n\n        merge_keys = {'book_name', 'participant_id', 'question_index'}\n        drop_keys = (\n            (set(ia_df.columns) &amp; set(labels_df.columns))\n            - merge_keys\n            - {'correct_answer', 'answer'}\n        )\n        ia_df = ia_df.merge(\n            labels_df.drop(columns=list(drop_keys)).drop_duplicates(),\n            on=list(merge_keys),\n            how='left',\n            validate='many_to_one',\n        )\n\n        # Merge additional labels\n        _labels_df = pd.read_csv('data/SBSAT/labels/18sat_labels.csv').rename(\n            columns={\n                'subj': Fields.SUBJECT_ID,\n                'page_name': Fields.UNIQUE_PARAGRAPH_ID,\n                'book': 'book_name',\n            }\n        )\n\n        merge_keys = {'book_name', 'participant_id'}\n        for df in [fix_df, ia_df]:\n            drop_keys = (\n                (set(df.columns) &amp; set(_labels_df.columns))\n                - merge_keys\n                - {'correct_answer', 'answer'}\n            )\n            df_merged = df.merge(\n                _labels_df.drop(columns=list(drop_keys)).drop_duplicates(),\n                on=list(merge_keys),\n                how='left',\n            )\n            if df is fix_df:\n                fix_df = df_merged\n            else:\n                ia_df = df_merged\n\n        return ia_df, fix_df\n\n    def _duplicate_df(self, df: pd.DataFrame, n_duplicates: int) -&gt; pd.DataFrame:\n        \"\"\"Duplicate dataframe with question_index\"\"\"\n        df_list = [\n            df.copy().assign(question_index=i) for i in range(1, n_duplicates + 1)\n        ]\n        return pd.concat(df_list, ignore_index=True)\n\n    def _add_linguistic_features(self, ia_df: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"Add linguistic features using spacy and language models\"\"\"\n\n        # Initialize models\n        logger.info('Initializing linguistic models...')\n        surp_extractor = get_surp_extractor(\n            extractor_type=SurpExtractorType.CAT_CTX_LEFT, model_name='gpt2'\n        )\n        nlp = spacy.load('en_core_web_sm')\n\n        # Process groups\n        logger.info('Computing linguistic features...')\n        groups = list(ia_df.groupby('unique_trial_id'))\n        metrics_list = [\n            self._process_linguistic_group(group, surp_extractor, nlp)\n            for _, group in tqdm(groups, desc='Extracting features')\n        ]\n\n        # Combine and prepare metrics\n        metrics_df = pd.concat(metrics_list, ignore_index=True)\n        metrics_df = self._duplicate_df(metrics_df, self.N_QUESTION_DUPLICATES)\n        metrics_df['unique_trial_id'] = (\n            metrics_df['unique_trial_id']\n            + '_'\n            + metrics_df['question_index'].astype(str)\n        )\n\n        # Merge with IA data\n        ia_df['IA_ID'] = ia_df['word_index']\n        merge_keys = {'unique_trial_id', 'IA_ID'}\n        drop_keys = (set(ia_df.columns) &amp; set(metrics_df.columns)) - merge_keys\n\n        ia_df = ia_df.merge(\n            metrics_df.drop(columns=list(drop_keys) + ['Morph']).drop_duplicates(),\n            on=list(merge_keys),\n            how='left',\n            validate='one_to_one',\n        )\n\n        # Rename linguistic features\n        ia_df = ia_df.rename(columns=self.FEATURE_RENAMES)\n\n        return ia_df\n\n    def _process_linguistic_group(\n        self, group: pd.DataFrame, surp_extractor, nlp\n    ) -&gt; pd.DataFrame:\n        \"\"\"Process a single group for linguistic features\"\"\"\n\n        words = list(group['word'])\n        sentence = ' '.join(words)\n\n        metrics = get_metrics(\n            target_text=sentence,\n            surp_extractor=surp_extractor,\n            parsing_model=nlp,\n            parsing_mode='re-tokenize',\n            add_parsing_features=True,\n            language='en',\n        )\n\n        metrics['unique_trial_id'] = (\n            f'{group[\"participant_id\"].iloc[0]}_{group[\"unique_paragraph_id\"].iloc[0]}'\n        )\n        metrics['IA_ID'] = list(group['word_index'])\n        metrics['CURRENT_FIX_INTEREST_AREA_INDEX'] = list(group['word_index'])\n\n        return metrics\n\n    def _finalize_dataframes(\n        self, ia_df: pd.DataFrame, fix_df: pd.DataFrame\n    ) -&gt; tuple[pd.DataFrame, pd.DataFrame]:\n        \"\"\"Finalize both dataframes with computed features\"\"\"\n        # Add derived features to IA data\n        ia_df['word_length'] = ia_df['word'].apply(len)\n        ia_df['total_skip'] = ia_df['FFD'].apply(lambda x: x &gt; 0)\n        ia_df['TRIAL_IA_COUNT'] = ia_df.groupby('unique_trial_id')[\n            'unique_trial_id'\n        ].transform('count')\n        ia_df['RC'] = (ia_df['answer'] == ia_df['correct_answer']).astype(int)\n        ia_df = ia_df[~ia_df['RC'].isna()]\n        ia_df['binary_difficulty'] = (\n            ia_df['difficulty'] &gt; ia_df['difficulty'].median()\n        ).astype(int)\n        ia_df['difficulty'] = ia_df['difficulty'].astype(float)\n        ia_df['IA_FIXATION_%'] = ia_df.groupby('unique_trial_id')[\n            'IA_FIXATION_COUNT'\n        ].transform(lambda x: x / np.sum(x) if np.sum(x) &gt; 0 else 0)\n        ia_df['CURRENT_FIX_NEAREST_INTEREST_AREA_DISTANCE'] = 0\n        ia_df['CURRENT_FIX_INTEREST_AREA_INDEX'] = ia_df['IA_ID']\n        ia_df['NEXT_FIX_INTEREST_AREA_INDEX'] = (\n            ia_df['CURRENT_FIX_INTEREST_AREA_INDEX'].shift(-1).fillna(-1).astype(int)\n        )\n        ia_df['paragraph'] = ia_df.groupby('unique_trial_id')['word'].transform(\n            lambda x: ' '.join(x)\n        )\n        ia_df['IA_SKIP'] = (ia_df['FPF'] &gt; 0).astype(int)\n        ia_df['IA_FIRST_RUN_DWELL_TIME'] = ia_df['FPRT']\n        fix_df['CURRENT_FIX_INTEREST_AREA_INDEX'] = (\n            fix_df['word_index'].fillna(-1).astype(int)\n        )\n        fix_df['NEXT_FIX_INTEREST_AREA_INDEX'] = (\n            fix_df['CURRENT_FIX_INTEREST_AREA_INDEX'].shift(-1).fillna(-1).astype(int)\n        )\n\n        # Map trial-level features to fixation data\n        for col in ['RC', 'difficulty', 'binary_difficulty']:\n            trial_values = ia_df.groupby('unique_trial_id')[col].first()\n            fix_df[col] = fix_df['unique_trial_id'].map(trial_values)\n\n        fix_df['difficulty'] = fix_df['difficulty'].astype(float)\n\n        # Prepare for merge\n        merge_keys = set(\n            self.data_args.groupby_columns + [Fields.FIXATION_REPORT_IA_ID_COL_NAME]\n        )\n        dup_cols = (set(fix_df.columns) &amp; set(ia_df.columns)) - merge_keys\n        _ia_df = ia_df.drop(columns=list(dup_cols))\n\n        # Clean problematic columns\n        if (\n            'normalized_part_ID' in fix_df.columns\n            and fix_df['normalized_part_ID'].isna().any()\n        ):\n            logger.info('Dropping normalized_part_ID due to NaN values')\n            fix_df = fix_df.drop(columns='normalized_part_ID')\n\n        # Merge fixations with IA features\n        enriched_fix_df = fix_df.merge(\n            _ia_df, on=list(merge_keys), how='left', validate='many_to_one'\n        )\n\n        # Remove duplicate groupby columns if present\n        if len(set(self.data_args.groupby_columns)) != len(\n            self.data_args.groupby_columns\n        ):\n            logger.warning('Removing duplicate groupby_columns')\n            seen = set()\n            self.data_args.groupby_columns = [\n                x\n                for x in self.data_args.groupby_columns\n                if not (x in seen or seen.add(x))\n            ]\n\n        # Add word count per trial\n        num_of_words_in_trials_series = _ia_df.groupby(\n            self.data_args.groupby_columns\n        ).apply(len)\n        num_of_words_in_trials_series.name = 'num_of_words_in_trial'\n        enriched_fix_df = enriched_fix_df.merge(\n            num_of_words_in_trials_series,\n            on=self.data_args.groupby_columns,\n            how='left',\n            validate='many_to_one',\n        )\n\n        return enriched_fix_df, ia_df\n\n    def compute_word_level_reading_measures(\n        self,\n        fix_df: pd.DataFrame,\n        stim_df: pd.DataFrame,\n    ) -&gt; tuple[pd.DataFrame, pd.DataFrame]:\n        \"\"\"Compute word-level reading measures from fixation data\"\"\"\n        stim_df = stim_df.copy()\n        stim_df['word_index'] = stim_df['word_number'] - 1\n\n        results = []\n\n        for text_name in tqdm(\n            fix_df['unique_paragraph_id'].unique(),\n            desc='Computing word-level reading measures',\n        ):\n            if text_name.startswith('question'):\n                continue\n\n            logger.info(f'Processing text: {text_name}')\n            aoi_df = stim_df[stim_df['filename'] == f'{text_name}.png']\n\n            # Build text strings list\n            _text_strs = ['previous_PAGE', 'next_PAGE', 'GO_TO_QUESTION'] + list(\n                aoi_df['word']\n            )[3 : -(aoi_df.index[-1] - aoi_df[aoi_df['word'].eq('Previous')].index)[0]]\n            text_strs = [s.replace(\"'\", \"'\") for s in _text_strs]\n\n            # Special case for dickens-3\n            if text_name == 'reading-dickens-3':\n                text_strs = [\n                    s.replace('Sempere', 'Sempere &amp;') for s in text_strs if s != '&amp;'\n                ]\n\n            # Process each participant\n            tmp_df = fix_df[fix_df['unique_paragraph_id'] == text_name]\n            for participant_id in tqdm(\n                tmp_df['participant_id'].unique(), desc='Participants'\n            ):\n                fixations_df = tmp_df[tmp_df['participant_id'] == participant_id]\n\n                result_df = self._compute_participant_measures(\n                    text_name, participant_id, fixations_df, aoi_df\n                )\n                results.append(result_df)\n\n        return pd.concat(results, ignore_index=True), fix_df\n\n    def _compute_participant_measures(\n        self,\n        text_name: str,\n        participant_id: str,\n        fixations_df: pd.DataFrame,\n        aoi_df: pd.DataFrame,\n    ) -&gt; pd.DataFrame:\n        \"\"\"Compute reading measures for a single participant-text combination\"\"\"\n        # Add dummy fixation at end\n        fixations_df = pd.concat(\n            [\n                fixations_df,\n                pd.DataFrame(\n                    [[0] * len(fixations_df.columns)], columns=fixations_df.columns\n                ),\n            ],\n            ignore_index=True,\n        )\n\n        # Initialize word dictionary with measures\n        text_aois = list(aoi_df['word_index'])\n        text_strs = list(aoi_df['word'])\n\n        word_dict = {}\n        for word_index, word in zip(text_aois, text_strs):\n            word_dict[int(word_index)] = {\n                'word': word,\n                'word_index': word_index,\n                'FFD': 0,  # First Fixation Duration\n                'SFD': 0,  # Single Fixation Duration\n                'FD': 0,  # Fixation Duration\n                'FPRT': 0,  # First Pass Reading Time\n                'FRT': 0,  # First Run Time\n                'TFT': 0,  # Total Fixation Time\n                'RRT': 0,  # Re-Reading Time\n                'RPD_inc': 0,  # Regression Path Duration (inclusive)\n                'RPD_exc': 0,  # Regression Path Duration (exclusive)\n                'RBRT': 0,  # Right-Bounded Reading Time\n                'Fix': 0,\n                'FPF': 0,  # First Pass Fixation\n                'RR': 0,  # Re-Reading\n                'FPReg': 0,  # First Pass Regression\n                'TRC_out': 0,  # Total Regression Count (outgoing)\n                'TRC_in': 0,  # Total Regression Count (incoming)\n                'SL_in': 0,  # Saccade Length (incoming)\n                'SL_out': 0,  # Saccade Length (outgoing)\n                'TFC': 0,  # Total Fixation Count\n                'FFID': 0,  # First Fixation Index\n                'IA_FIRST_FIXATION_X': 0,\n                'IA_FIRST_FIXATION_Y': 0,\n                'IA_FIRST_FIXATION_PREVIOUS_FIX_IA': 0,\n                'IA_FIRST_FIXATION_RUN_INDEX': 0,\n                'IA_FIRST_FIXATION_PREVIOUS_IAREAS': 0,\n                'NEXT_SAC_START_X_tmp': 0,\n                'NEXT_SAC_START_Y_tmp': 0,\n                'NEXT_SAC_START_X': 0,\n                'NEXT_SAC_START_Y': 0,\n            }\n\n        # Process fixations\n        right_most_word = -1\n        cur_fix_word_idx = -1\n        next_fix_word_idx = -1\n        next_fix_dur = -1\n\n        for index, fixation in fixations_df.iterrows():\n            # Skip invalid fixations\n            try:\n                aoi = int(fixation['word_index'])\n            except (ValueError, TypeError):\n                continue\n\n            if fixation['word'] == '.':\n                continue\n\n            # Shift fixation window\n            last_fix_word_idx = cur_fix_word_idx\n            cur_fix_word_idx = next_fix_word_idx\n            cur_fix_dur = next_fix_dur\n\n            next_fix_word_idx = aoi - 1\n            next_fix_dur = fixation['CURRENT_FIX_DURATION']\n\n            # Skip zero-duration fixations\n            if next_fix_dur == 0:\n                next_fix_word_idx = cur_fix_word_idx\n\n            # Update rightmost position\n            if right_most_word &lt; cur_fix_word_idx:\n                right_most_word = cur_fix_word_idx\n\n            # Skip uninitialized fixations\n            if cur_fix_word_idx == -1 or cur_fix_word_idx not in word_dict:\n                continue\n\n            # Update word measures\n            wd = word_dict[cur_fix_word_idx]\n            wd['TFT'] += int(cur_fix_dur)\n            wd['TFC'] += 1\n\n            if wd['FD'] == 0:\n                wd['FD'] += int(cur_fix_dur)\n                wd['FFID'] = index\n                wd['IA_FIRST_FIXATION_X'] = fixation['CURRENT_FIX_X']\n                wd['IA_FIRST_FIXATION_Y'] = fixation['CURRENT_FIX_Y']\n                wd['IA_FIRST_FIXATION_PREVIOUS_FIX_IA'] = last_fix_word_idx\n                wd['IA_FIRST_FIXATION_RUN_INDEX'] = index\n                wd['IA_FIRST_FIXATION_PREVIOUS_IAREAS'] = last_fix_word_idx\n            wd['NEXT_SAC_START_X_tmp'] += fixation['CURRENT_FIX_X']\n            wd['NEXT_SAC_START_Y_tmp'] += fixation['CURRENT_FIX_Y']\n            wd['NEXT_SAC_START_X'] = np.mean(wd['NEXT_SAC_START_X_tmp'])\n            wd['NEXT_SAC_START_Y'] = np.mean(wd['NEXT_SAC_START_Y_tmp'])\n            # First pass measures\n            if right_most_word == cur_fix_word_idx:\n                if wd['TRC_out'] == 0:\n                    wd['FPRT'] += int(cur_fix_dur)\n                    if last_fix_word_idx &lt; cur_fix_word_idx:\n                        wd['FFD'] += int(cur_fix_dur)\n            else:\n                if right_most_word &lt; cur_fix_word_idx:\n                    logger.warning('Rightmost word inconsistency detected')\n                if right_most_word in word_dict:\n                    word_dict[right_most_word]['RPD_exc'] += int(cur_fix_dur)\n\n            # Regression tracking\n            if cur_fix_word_idx &lt; last_fix_word_idx:\n                wd['TRC_in'] += 1\n            if cur_fix_word_idx &gt; next_fix_word_idx:\n                wd['TRC_out'] += 1\n\n            # Right-bounded reading time\n            if cur_fix_word_idx == right_most_word:\n                wd['RBRT'] += int(cur_fix_dur)\n\n            # First run time\n            if wd['FRT'] == 0 and (\n                next_fix_word_idx != cur_fix_word_idx or next_fix_dur == 0\n            ):\n                wd['FRT'] = wd['TFT']\n\n            # Saccade lengths\n            if wd['SL_in'] == 0:\n                wd['SL_in'] = cur_fix_word_idx - last_fix_word_idx\n            if wd['SL_out'] == 0:\n                wd['SL_out'] = next_fix_word_idx - cur_fix_word_idx\n\n        # Finalize measures\n        word_measures = []\n        for word_idx, wd in sorted(word_dict.items()):\n            if wd['FFD'] == wd['FPRT']:\n                wd['SFD'] = wd['FFD']\n            wd['RRT'] = wd['TFT'] - wd['FPRT']\n            wd['FPF'] = int(wd['FFD'] &gt; 0)\n            wd['RR'] = int(wd['RRT'] &gt; 0)\n            wd['FPReg'] = int(wd['RPD_exc'] &gt; 0)\n            wd['Fix'] = int(wd['TFT'] &gt; 0)\n            wd['RPD_inc'] = wd['RPD_exc'] + wd['RBRT']\n            wd['participant_id'] = participant_id\n            wd['unique_paragraph_id'] = text_name\n            word_measures.append(wd)\n\n        return pd.DataFrame(word_measures)\n\n    def fix_issues_with_aois_and_stimuli(\n        self, fix_df: pd.DataFrame, stim_df: pd.DataFrame\n    ) -&gt; pd.DataFrame:\n        \"\"\"Fix known issues with AOI labels and apply corrections\"\"\"\n        logger.info('Fixing AOI and stimulus issues...')\n        fix_df = fix_df.copy()\n\n        # Fix text encoding issues\n        for bad_char, good_char in self.ENCODING_MAP.items():\n            fix_df['CURRENT_FIX_INTEREST_AREA_LABEL'] = fix_df[\n                'CURRENT_FIX_INTEREST_AREA_LABEL'\n            ].str.replace(bad_char, good_char, regex=False)\n\n        # Apply AOI label corrections\n        for para_id, aoi_id, correct_label in self.AOI_FIXES:\n            mask = (fix_df['unique_paragraph_id'] == para_id) &amp; (\n                fix_df['CURRENT_FIX_INTEREST_AREA_ID'] == aoi_id\n            )\n            fix_df.loc[mask, 'CURRENT_FIX_INTEREST_AREA_LABEL'] = correct_label\n\n        # Add derived columns\n        fix_df['word'] = fix_df['CURRENT_FIX_INTEREST_AREA_LABEL']\n        fix_df['word_index'] = fix_df['CURRENT_FIX_INTEREST_AREA_ID']\n        fix_df['NEXT_FIX_INTEREST_AREA_INDEX'] = (\n            fix_df['word_index'].shift(-1).replace([np.inf, -np.inf], 0)\n        )\n        fix_df['PREVIOUS_FIX_INTEREST_AREA_INDEX'] = (\n            fix_df['word_index'].shift(1).replace([np.inf, -np.inf], 0)\n        )\n        fix_df['CURRENT_FIX_NEAREST_INTEREST_AREA_DISTANCE'] = 0\n        fix_df['NEXT_SAC_AMPLITUDE'] = 0\n        fix_df['NEXT_SAC_AVG_VELOCITY'] = 0\n        fix_df['NEXT_SAC_DURATION'] = 0\n        fix_df['start_of_line'] = 0\n        fix_df['end_of_line'] = 0\n\n        return fix_df\n\n    def get_column_map(self, data_type: DataType) -&gt; dict[str, str]:\n        \"\"\"Get column mapping for SBSAT dataset\"\"\"\n        if data_type == DataType.IA:\n            return {\n                'FFD': 'IA_FIRST_FIXATION_DURATION',\n                'SFD': 'IA_SINGLE_FIXATION_DURATION',\n                'TFC': 'IA_FIXATION_COUNT',\n                'TRC_in': 'IA_REGRESSION_IN_COUNT',\n                'TRC_out': 'IA_REGRESSION_OUT_COUNT',\n                'FD': 'IA_FIRST_FIXATION_TIME',\n            }\n        elif data_type == DataType.FIXATIONS:\n            return {\n                'page_name': 'unique_paragraph_id',\n                'RECORDING_SESSION_LABEL': 'participant_id',\n            }\n        return {}\n\n    def get_columns_to_keep(self) -&gt; list:\n        \"\"\"Get list of columns to keep after filtering\"\"\"\n        return []\n\n    def dataset_specific_processing(\n        self, data_dict: dict[str, pd.DataFrame]\n    ) -&gt; dict[str, pd.DataFrame]:\n        \"\"\"SBSAT-specific processing steps\"\"\"\n        # Filter to reading trials only\n        data_dict['fixations'] = data_dict['fixations'].loc[\n            data_dict['fixations']['unique_paragraph_id'].str.startswith('reading')\n        ]\n        data_dict['ia'] = data_dict['ia'].loc[\n            data_dict['ia']['filename'].str.startswith('reading')\n        ]\n\n        # Fix AOI and stimuli issues\n        data_dict['fixations'] = self.fix_issues_with_aois_and_stimuli(\n            data_dict['fixations'], data_dict['ia']\n        )\n\n        # Compute word-level reading measures\n        data_dict['ia'], data_dict['fixations'] = (\n            self.compute_word_level_reading_measures(\n                data_dict['fixations'], data_dict['ia']\n            )\n        )\n\n        # Add IA features to fixation data\n        data_dict['fixations'], data_dict['ia'] = (\n            self.add_ia_report_features_to_fixation_data(\n                data_dict['ia'], data_dict['fixations']\n            )\n        )\n\n        # Add missing features for both data types\n        for data_type in [DataType.IA, DataType.FIXATIONS]:\n            data_dict[data_type] = add_missing_features(\n                et_data=data_dict[data_type],\n                trial_groupby_columns=self.data_args.groupby_columns,\n                mode=data_type,\n            )\n            data_dict[data_type] = data_dict[data_type].assign(\n                normalized_ID=(\n                    data_dict[data_type]['IA_ID'] - data_dict[data_type]['IA_ID'].min()\n                )\n                / (\n                    data_dict[data_type]['IA_ID'].max()\n                    - data_dict[data_type]['IA_ID'].min()\n                ),\n            )\n\n            # Merge questions\n            questions = (\n                pd.read_csv(\n                    'data/SBSAT/stimuli/combined_stimulus.csv',\n                    usecols=['stimulus_type', 'sequence_num', 'question'],\n                )\n                .rename(\n                    columns={\n                        'stimulus_type': 'book_name',\n                        'sequence_num': 'question_index',\n                    }\n                )\n                .drop_duplicates()\n            )\n            data_dict[data_type] = data_dict[data_type].merge(\n                questions,\n                on=['book_name', 'question_index'],\n                how='left',\n                validate='many_to_one',\n            )\n\n        # Compute trial-level features\n        trial_level_features = compute_trial_level_features(\n            raw_fixation_data=data_dict[DataType.FIXATIONS],\n            raw_ia_data=data_dict[DataType.IA],\n            trial_groupby_columns=self.data_args.groupby_columns,\n            processed_data_path=self.data_args.processed_data_path,\n        )\n        data_dict[DataType.TRIAL_LEVEL] = trial_level_features\n\n        # Replace missing values\n        data_dict = replace_missing_values(data_dict)\n\n        return data_dict\n</code></pre>"},{"location":"reference/data/preprocessing/dataset_preprocessing/sbsat/#data.preprocessing.dataset_preprocessing.sbsat.SBSATProcessor.add_ia_report_features_to_fixation_data","title":"<code>add_ia_report_features_to_fixation_data(ia_df, fix_df)</code>","text":"<p>Merge per-IA (interest area) features into the fixation-level data. Result: one row per fixation, enriched with IA-level attributes.</p> Source code in <code>src/data/preprocessing/dataset_preprocessing/sbsat.py</code> <pre><code>def add_ia_report_features_to_fixation_data(\n    self,\n    ia_df: pd.DataFrame,\n    fix_df: pd.DataFrame,\n) -&gt; tuple[pd.DataFrame, pd.DataFrame]:\n    \"\"\"\n    Merge per-IA (interest area) features into the fixation-level data.\n    Result: one row per fixation, enriched with IA-level attributes.\n    \"\"\"\n    # Prepare IA dataframe\n    ia_df = self._prepare_ia_dataframe(ia_df)\n\n    # Prepare fixation dataframe\n    fix_df = self._prepare_fixation_dataframe(fix_df, ia_df)\n\n    # Load and merge labels\n    ia_df, fix_df = self._merge_trial_labels(ia_df, fix_df)\n\n    # Add IA features\n    ia_df = self._add_ia_features(ia_df)\n\n    # Compute linguistic features\n    ia_df = self._add_linguistic_features(ia_df)\n\n    # Finalize and merge\n    fix_df, ia_df = self._finalize_dataframes(ia_df, fix_df)\n\n    return fix_df, ia_df\n</code></pre>"},{"location":"reference/data/preprocessing/dataset_preprocessing/sbsat/#data.preprocessing.dataset_preprocessing.sbsat.SBSATProcessor.compute_word_level_reading_measures","title":"<code>compute_word_level_reading_measures(fix_df, stim_df)</code>","text":"<p>Compute word-level reading measures from fixation data</p> Source code in <code>src/data/preprocessing/dataset_preprocessing/sbsat.py</code> <pre><code>def compute_word_level_reading_measures(\n    self,\n    fix_df: pd.DataFrame,\n    stim_df: pd.DataFrame,\n) -&gt; tuple[pd.DataFrame, pd.DataFrame]:\n    \"\"\"Compute word-level reading measures from fixation data\"\"\"\n    stim_df = stim_df.copy()\n    stim_df['word_index'] = stim_df['word_number'] - 1\n\n    results = []\n\n    for text_name in tqdm(\n        fix_df['unique_paragraph_id'].unique(),\n        desc='Computing word-level reading measures',\n    ):\n        if text_name.startswith('question'):\n            continue\n\n        logger.info(f'Processing text: {text_name}')\n        aoi_df = stim_df[stim_df['filename'] == f'{text_name}.png']\n\n        # Build text strings list\n        _text_strs = ['previous_PAGE', 'next_PAGE', 'GO_TO_QUESTION'] + list(\n            aoi_df['word']\n        )[3 : -(aoi_df.index[-1] - aoi_df[aoi_df['word'].eq('Previous')].index)[0]]\n        text_strs = [s.replace(\"'\", \"'\") for s in _text_strs]\n\n        # Special case for dickens-3\n        if text_name == 'reading-dickens-3':\n            text_strs = [\n                s.replace('Sempere', 'Sempere &amp;') for s in text_strs if s != '&amp;'\n            ]\n\n        # Process each participant\n        tmp_df = fix_df[fix_df['unique_paragraph_id'] == text_name]\n        for participant_id in tqdm(\n            tmp_df['participant_id'].unique(), desc='Participants'\n        ):\n            fixations_df = tmp_df[tmp_df['participant_id'] == participant_id]\n\n            result_df = self._compute_participant_measures(\n                text_name, participant_id, fixations_df, aoi_df\n            )\n            results.append(result_df)\n\n    return pd.concat(results, ignore_index=True), fix_df\n</code></pre>"},{"location":"reference/data/preprocessing/dataset_preprocessing/sbsat/#data.preprocessing.dataset_preprocessing.sbsat.SBSATProcessor.dataset_specific_processing","title":"<code>dataset_specific_processing(data_dict)</code>","text":"<p>SBSAT-specific processing steps</p> Source code in <code>src/data/preprocessing/dataset_preprocessing/sbsat.py</code> <pre><code>def dataset_specific_processing(\n    self, data_dict: dict[str, pd.DataFrame]\n) -&gt; dict[str, pd.DataFrame]:\n    \"\"\"SBSAT-specific processing steps\"\"\"\n    # Filter to reading trials only\n    data_dict['fixations'] = data_dict['fixations'].loc[\n        data_dict['fixations']['unique_paragraph_id'].str.startswith('reading')\n    ]\n    data_dict['ia'] = data_dict['ia'].loc[\n        data_dict['ia']['filename'].str.startswith('reading')\n    ]\n\n    # Fix AOI and stimuli issues\n    data_dict['fixations'] = self.fix_issues_with_aois_and_stimuli(\n        data_dict['fixations'], data_dict['ia']\n    )\n\n    # Compute word-level reading measures\n    data_dict['ia'], data_dict['fixations'] = (\n        self.compute_word_level_reading_measures(\n            data_dict['fixations'], data_dict['ia']\n        )\n    )\n\n    # Add IA features to fixation data\n    data_dict['fixations'], data_dict['ia'] = (\n        self.add_ia_report_features_to_fixation_data(\n            data_dict['ia'], data_dict['fixations']\n        )\n    )\n\n    # Add missing features for both data types\n    for data_type in [DataType.IA, DataType.FIXATIONS]:\n        data_dict[data_type] = add_missing_features(\n            et_data=data_dict[data_type],\n            trial_groupby_columns=self.data_args.groupby_columns,\n            mode=data_type,\n        )\n        data_dict[data_type] = data_dict[data_type].assign(\n            normalized_ID=(\n                data_dict[data_type]['IA_ID'] - data_dict[data_type]['IA_ID'].min()\n            )\n            / (\n                data_dict[data_type]['IA_ID'].max()\n                - data_dict[data_type]['IA_ID'].min()\n            ),\n        )\n\n        # Merge questions\n        questions = (\n            pd.read_csv(\n                'data/SBSAT/stimuli/combined_stimulus.csv',\n                usecols=['stimulus_type', 'sequence_num', 'question'],\n            )\n            .rename(\n                columns={\n                    'stimulus_type': 'book_name',\n                    'sequence_num': 'question_index',\n                }\n            )\n            .drop_duplicates()\n        )\n        data_dict[data_type] = data_dict[data_type].merge(\n            questions,\n            on=['book_name', 'question_index'],\n            how='left',\n            validate='many_to_one',\n        )\n\n    # Compute trial-level features\n    trial_level_features = compute_trial_level_features(\n        raw_fixation_data=data_dict[DataType.FIXATIONS],\n        raw_ia_data=data_dict[DataType.IA],\n        trial_groupby_columns=self.data_args.groupby_columns,\n        processed_data_path=self.data_args.processed_data_path,\n    )\n    data_dict[DataType.TRIAL_LEVEL] = trial_level_features\n\n    # Replace missing values\n    data_dict = replace_missing_values(data_dict)\n\n    return data_dict\n</code></pre>"},{"location":"reference/data/preprocessing/dataset_preprocessing/sbsat/#data.preprocessing.dataset_preprocessing.sbsat.SBSATProcessor.fix_issues_with_aois_and_stimuli","title":"<code>fix_issues_with_aois_and_stimuli(fix_df, stim_df)</code>","text":"<p>Fix known issues with AOI labels and apply corrections</p> Source code in <code>src/data/preprocessing/dataset_preprocessing/sbsat.py</code> <pre><code>def fix_issues_with_aois_and_stimuli(\n    self, fix_df: pd.DataFrame, stim_df: pd.DataFrame\n) -&gt; pd.DataFrame:\n    \"\"\"Fix known issues with AOI labels and apply corrections\"\"\"\n    logger.info('Fixing AOI and stimulus issues...')\n    fix_df = fix_df.copy()\n\n    # Fix text encoding issues\n    for bad_char, good_char in self.ENCODING_MAP.items():\n        fix_df['CURRENT_FIX_INTEREST_AREA_LABEL'] = fix_df[\n            'CURRENT_FIX_INTEREST_AREA_LABEL'\n        ].str.replace(bad_char, good_char, regex=False)\n\n    # Apply AOI label corrections\n    for para_id, aoi_id, correct_label in self.AOI_FIXES:\n        mask = (fix_df['unique_paragraph_id'] == para_id) &amp; (\n            fix_df['CURRENT_FIX_INTEREST_AREA_ID'] == aoi_id\n        )\n        fix_df.loc[mask, 'CURRENT_FIX_INTEREST_AREA_LABEL'] = correct_label\n\n    # Add derived columns\n    fix_df['word'] = fix_df['CURRENT_FIX_INTEREST_AREA_LABEL']\n    fix_df['word_index'] = fix_df['CURRENT_FIX_INTEREST_AREA_ID']\n    fix_df['NEXT_FIX_INTEREST_AREA_INDEX'] = (\n        fix_df['word_index'].shift(-1).replace([np.inf, -np.inf], 0)\n    )\n    fix_df['PREVIOUS_FIX_INTEREST_AREA_INDEX'] = (\n        fix_df['word_index'].shift(1).replace([np.inf, -np.inf], 0)\n    )\n    fix_df['CURRENT_FIX_NEAREST_INTEREST_AREA_DISTANCE'] = 0\n    fix_df['NEXT_SAC_AMPLITUDE'] = 0\n    fix_df['NEXT_SAC_AVG_VELOCITY'] = 0\n    fix_df['NEXT_SAC_DURATION'] = 0\n    fix_df['start_of_line'] = 0\n    fix_df['end_of_line'] = 0\n\n    return fix_df\n</code></pre>"},{"location":"reference/data/preprocessing/dataset_preprocessing/sbsat/#data.preprocessing.dataset_preprocessing.sbsat.SBSATProcessor.get_column_map","title":"<code>get_column_map(data_type)</code>","text":"<p>Get column mapping for SBSAT dataset</p> Source code in <code>src/data/preprocessing/dataset_preprocessing/sbsat.py</code> <pre><code>def get_column_map(self, data_type: DataType) -&gt; dict[str, str]:\n    \"\"\"Get column mapping for SBSAT dataset\"\"\"\n    if data_type == DataType.IA:\n        return {\n            'FFD': 'IA_FIRST_FIXATION_DURATION',\n            'SFD': 'IA_SINGLE_FIXATION_DURATION',\n            'TFC': 'IA_FIXATION_COUNT',\n            'TRC_in': 'IA_REGRESSION_IN_COUNT',\n            'TRC_out': 'IA_REGRESSION_OUT_COUNT',\n            'FD': 'IA_FIRST_FIXATION_TIME',\n        }\n    elif data_type == DataType.FIXATIONS:\n        return {\n            'page_name': 'unique_paragraph_id',\n            'RECORDING_SESSION_LABEL': 'participant_id',\n        }\n    return {}\n</code></pre>"},{"location":"reference/data/preprocessing/dataset_preprocessing/sbsat/#data.preprocessing.dataset_preprocessing.sbsat.SBSATProcessor.get_columns_to_keep","title":"<code>get_columns_to_keep()</code>","text":"<p>Get list of columns to keep after filtering</p> Source code in <code>src/data/preprocessing/dataset_preprocessing/sbsat.py</code> <pre><code>def get_columns_to_keep(self) -&gt; list:\n    \"\"\"Get list of columns to keep after filtering\"\"\"\n    return []\n</code></pre>"},{"location":"reference/data/preprocessing/dataset_preprocessing/sbsat_add_is_question_column/","title":"sbsat_add_is_question_column","text":"<p>Script to add an 'is_question' column to SBSAT question CSV files. The column is True from the start until the first \"1)\" (excluding it), and False for the rest.</p>"},{"location":"reference/data/preprocessing/dataset_preprocessing/sbsat_add_is_question_column/#data.preprocessing.dataset_preprocessing.sbsat_add_is_question_column.add_is_question_column","title":"<code>add_is_question_column(csv_path)</code>","text":"<p>Add an 'is_question' column to a CSV file.</p> <p>Parameters:</p> Name Type Description Default <code>csv_path</code> <p>Path to the CSV file</p> required Source code in <code>src/data/preprocessing/dataset_preprocessing/sbsat_add_is_question_column.py</code> <pre><code>def add_is_question_column(csv_path):\n    \"\"\"\n    Add an 'is_question' column to a CSV file.\n\n    Args:\n        csv_path: Path to the CSV file\n    \"\"\"\n    # Read the CSV file\n    df = pd.read_csv(csv_path)\n\n    # Check if this file has a hardcoded split index\n    if csv_path.name in HARDCODED_SPLIT_INDICES:\n        split_index = HARDCODED_SPLIT_INDICES[csv_path.name]\n        df['is_question'] = False\n        df.loc[: split_index - 1, 'is_question'] = True\n        df.to_csv(csv_path, index=False)\n        print(\n            f'Updated {csv_path.name} - added is_question column using HARDCODED split at index {split_index}'\n        )\n        return\n\n    # Find the first occurrence of any answer choice pattern (1), 2), 3), 4), etc.)\n    # or a word ending with \"?\"\n    answer_patterns = ['1)', '2)', '3)', '4)']\n    first_answer_index = None\n    first_pattern = None\n\n    for pattern in answer_patterns:\n        pattern_index = df[df['word'] == pattern].index\n        if len(pattern_index) &gt; 0:\n            if first_answer_index is None or pattern_index[0] &lt; first_answer_index:\n                first_answer_index = pattern_index[0]\n                first_pattern = pattern\n\n    # Also check for words ending with \"?\"\n    question_mark_indices = df[df['word'].astype(str).str.endswith('?')].index\n    if len(question_mark_indices) &gt; 0:\n        # The split should be AFTER the last word ending with \"?\"\n        last_question_mark_index = question_mark_indices[-1] + 1\n        if first_answer_index is None or last_question_mark_index &lt; first_answer_index:\n            first_answer_index = last_question_mark_index\n            first_pattern = (\n                f\"word ending with '?' at index {last_question_mark_index - 1}\"\n            )\n\n    if first_answer_index is not None:\n        # Get the first index where an answer choice appears\n        split_index = first_answer_index\n\n        # Create the is_question column\n        df['is_question'] = False\n        df.loc[: split_index - 1, 'is_question'] = True\n\n        # Save the updated CSV\n        df.to_csv(csv_path, index=False)\n        print(\n            f\"Updated {csv_path.name} - added is_question column (True for first {split_index} rows, split at '{first_pattern}')\"\n        )\n    else:\n        print(f'Warning: No answer choice pattern found in {csv_path.name}')\n</code></pre>"},{"location":"reference/data/preprocessing/dataset_preprocessing/sbsat_add_is_question_column/#data.preprocessing.dataset_preprocessing.sbsat_add_is_question_column.main","title":"<code>main()</code>","text":"<p>Process all question-* CSV files in the SBSAT stimuli directory.</p> Source code in <code>src/data/preprocessing/dataset_preprocessing/sbsat_add_is_question_column.py</code> <pre><code>def main():\n    \"\"\"Process all question-* CSV files in the SBSAT stimuli directory.\"\"\"\n    stimuli_dir = Path('data/SBSAT/stimuli')\n\n    # Find all question-*_words.csv files\n    question_csv_files = sorted(stimuli_dir.glob('question-*_words.csv'))\n\n    print(f'Found {len(question_csv_files)} question CSV files')\n    print('=' * 60)\n\n    for csv_path in question_csv_files:\n        add_is_question_column(csv_path)\n\n    print('=' * 60)\n    print('Done!')\n</code></pre>"},{"location":"reference/data/preprocessing/dataset_preprocessing/template/","title":"template","text":""},{"location":"reference/data/preprocessing/dataset_preprocessing/template/#data.preprocessing.dataset_preprocessing.template.TemplateProcessor","title":"<code>TemplateProcessor</code>","text":"<p>               Bases: <code>DatasetProcessor</code></p> <p>Processor for DATASET_NAME dataset</p> Source code in <code>src/data/preprocessing/dataset_preprocessing/template.py</code> <pre><code>class TemplateProcessor(DatasetProcessor):\n    \"\"\"Processor for DATASET_NAME dataset\"\"\"\n\n    def get_column_map(self, data_type: DataType) -&gt; dict:\n        \"\"\"Get column mapping for DATASET_NAME dataset\"\"\"\n        # TODO: add docs\n        if data_type == DataType.IA:\n            return {}\n        elif data_type == DataType.FIXATIONS:\n            return {}\n\n    def get_columns_to_keep(self) -&gt; list:\n        \"\"\"Get list of columns to keep after filtering\"\"\"\n        # TODO: add docs\n        return []\n\n    def dataset_specific_processing(\n        self, data_dict: dict[str, pd.DataFrame]\n    ) -&gt; dict[str, pd.DataFrame]:\n        \"\"\"Dataset-specific processing steps\"\"\"\n        # TODO: add docs\n        for data_type in [DataType.IA, DataType.FIXATIONS]:\n            if data_type not in data_dict or data_dict[data_type] is None:\n                continue\n            # load data\n            df = data_dict[data_type]\n\n            # add ids\n            # add unique_trial_id column\n            df['unique_trial_id'] = (\n                df['participant_id'].astype(str)\n                + '_'\n                + df['unique_paragraph_id'].astype(str)\n                + '_'\n                + df['practice_trial'].astype(str)\n            )\n            # filter rows?\n            # add labels of tasks?\n\n            data_dict[data_type] = df\n\n        # add_ia_report_features_to_fixation_data ?\n        # add_missing_features ?\n        # compute_trial_level_features ?\n\n        return data_dict\n\n    def add_ia_report_features_to_fixation_data(\n        self,\n        ia_df: pd.DataFrame,\n        fix_df: pd.DataFrame,\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        # TODO: add docs\n        #     # --- 1. Unify IA\u2011ID column name ----------------------------------------\n        #     # --- 2. Build the list of IA features we plan to add -------------------\n        #     # --- 3. Drop columns that also exist in fixation table -----------------\n        #     # --- 4. Clean nuisance column ------------------------------------------\n        #     # --- 5. Merge ----------------------------------------------------------\n        #     return\n        \"\"\"\n</code></pre>"},{"location":"reference/data/preprocessing/dataset_preprocessing/template/#data.preprocessing.dataset_preprocessing.template.TemplateProcessor.add_ia_report_features_to_fixation_data","title":"<code>add_ia_report_features_to_fixation_data(ia_df, fix_df)</code>","text":""},{"location":"reference/data/preprocessing/dataset_preprocessing/template/#data.preprocessing.dataset_preprocessing.template.TemplateProcessor.add_ia_report_features_to_fixation_data--todo-add-docs","title":"TODO: add docs","text":""},{"location":"reference/data/preprocessing/dataset_preprocessing/template/#data.preprocessing.dataset_preprocessing.template.TemplateProcessor.add_ia_report_features_to_fixation_data---1-unify-iaid-column-name-","title":"# --- 1. Unify IA\u2011ID column name ----------------------------------------","text":""},{"location":"reference/data/preprocessing/dataset_preprocessing/template/#data.preprocessing.dataset_preprocessing.template.TemplateProcessor.add_ia_report_features_to_fixation_data---2-build-the-list-of-ia-features-we-plan-to-add-","title":"# --- 2. Build the list of IA features we plan to add -------------------","text":""},{"location":"reference/data/preprocessing/dataset_preprocessing/template/#data.preprocessing.dataset_preprocessing.template.TemplateProcessor.add_ia_report_features_to_fixation_data---3-drop-columns-that-also-exist-in-fixation-table-","title":"# --- 3. Drop columns that also exist in fixation table -----------------","text":""},{"location":"reference/data/preprocessing/dataset_preprocessing/template/#data.preprocessing.dataset_preprocessing.template.TemplateProcessor.add_ia_report_features_to_fixation_data---4-clean-nuisance-column-","title":"# --- 4. Clean nuisance column ------------------------------------------","text":""},{"location":"reference/data/preprocessing/dataset_preprocessing/template/#data.preprocessing.dataset_preprocessing.template.TemplateProcessor.add_ia_report_features_to_fixation_data---5-merge-","title":"# --- 5. Merge ----------------------------------------------------------","text":""},{"location":"reference/data/preprocessing/dataset_preprocessing/template/#data.preprocessing.dataset_preprocessing.template.TemplateProcessor.add_ia_report_features_to_fixation_data--return","title":"return","text":"Source code in <code>src/data/preprocessing/dataset_preprocessing/template.py</code> <pre><code>def add_ia_report_features_to_fixation_data(\n    self,\n    ia_df: pd.DataFrame,\n    fix_df: pd.DataFrame,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    # TODO: add docs\n    #     # --- 1. Unify IA\u2011ID column name ----------------------------------------\n    #     # --- 2. Build the list of IA features we plan to add -------------------\n    #     # --- 3. Drop columns that also exist in fixation table -----------------\n    #     # --- 4. Clean nuisance column ------------------------------------------\n    #     # --- 5. Merge ----------------------------------------------------------\n    #     return\n    \"\"\"\n</code></pre>"},{"location":"reference/data/preprocessing/dataset_preprocessing/template/#data.preprocessing.dataset_preprocessing.template.TemplateProcessor.dataset_specific_processing","title":"<code>dataset_specific_processing(data_dict)</code>","text":"<p>Dataset-specific processing steps</p> Source code in <code>src/data/preprocessing/dataset_preprocessing/template.py</code> <pre><code>def dataset_specific_processing(\n    self, data_dict: dict[str, pd.DataFrame]\n) -&gt; dict[str, pd.DataFrame]:\n    \"\"\"Dataset-specific processing steps\"\"\"\n    # TODO: add docs\n    for data_type in [DataType.IA, DataType.FIXATIONS]:\n        if data_type not in data_dict or data_dict[data_type] is None:\n            continue\n        # load data\n        df = data_dict[data_type]\n\n        # add ids\n        # add unique_trial_id column\n        df['unique_trial_id'] = (\n            df['participant_id'].astype(str)\n            + '_'\n            + df['unique_paragraph_id'].astype(str)\n            + '_'\n            + df['practice_trial'].astype(str)\n        )\n        # filter rows?\n        # add labels of tasks?\n\n        data_dict[data_type] = df\n\n    # add_ia_report_features_to_fixation_data ?\n    # add_missing_features ?\n    # compute_trial_level_features ?\n\n    return data_dict\n</code></pre>"},{"location":"reference/data/preprocessing/dataset_preprocessing/template/#data.preprocessing.dataset_preprocessing.template.TemplateProcessor.get_column_map","title":"<code>get_column_map(data_type)</code>","text":"<p>Get column mapping for DATASET_NAME dataset</p> Source code in <code>src/data/preprocessing/dataset_preprocessing/template.py</code> <pre><code>def get_column_map(self, data_type: DataType) -&gt; dict:\n    \"\"\"Get column mapping for DATASET_NAME dataset\"\"\"\n    # TODO: add docs\n    if data_type == DataType.IA:\n        return {}\n    elif data_type == DataType.FIXATIONS:\n        return {}\n</code></pre>"},{"location":"reference/data/preprocessing/dataset_preprocessing/template/#data.preprocessing.dataset_preprocessing.template.TemplateProcessor.get_columns_to_keep","title":"<code>get_columns_to_keep()</code>","text":"<p>Get list of columns to keep after filtering</p> Source code in <code>src/data/preprocessing/dataset_preprocessing/template.py</code> <pre><code>def get_columns_to_keep(self) -&gt; list:\n    \"\"\"Get list of columns to keep after filtering\"\"\"\n    # TODO: add docs\n    return []\n</code></pre>"},{"location":"reference/models/ahn_model/","title":"ahn_model","text":"<p>Ahn et al. baseline models Based on https://github.com/aeye-lab/etra-reading-comprehension/blob/main/ahn_baseline/evaluate_ahn_baseline.py https://github.com/ahnchive/SB-SAT/blob/master/model/model_training.ipynb</p>"},{"location":"reference/models/ahn_model/#models.ahn_model.AhnCNNModel","title":"<code>AhnCNNModel</code>","text":"<p>               Bases: <code>AhnModel</code></p> <p>CNN model for Ahn et al. baseline</p> <p>Parameters:</p> Name Type Description Default <code>model_args</code> <code>AhnCNN</code> <p>The model arguments.</p> required <code>trainer_args</code> <code>TrainerDL</code> <p>The trainer arguments.</p> required <code>data_args</code> <code>DataArgs</code> <p>The data arguments.</p> required Source code in <code>src/models/ahn_model.py</code> <pre><code>@register_model\nclass AhnCNNModel(AhnModel):\n    \"\"\"\n    CNN model for Ahn et al. baseline\n\n    Args:\n        model_args (AhnCNN): The model arguments.\n        trainer_args (TrainerDL): The trainer arguments.\n        data_args (DataArgs): The data arguments.\n    \"\"\"\n\n    def __init__(\n        self,\n        model_args: AhnCNN,\n        trainer_args: TrainerDL,\n        data_args: DataArgs,\n    ):\n        super().__init__(model_args, trainer_args, data_args=data_args)\n\n        self.input_dim = self.input_dim\n        self.model_args = model_args\n        hidden_dim = self.model_args.hidden_dim\n        kernel_size = self.model_args.conv_kernel_size\n        fc_dropout = self.model_args.fc_dropout\n        fc_hidden_dim1 = self.model_args.fc_hidden_dim1\n        fc_hidden_dim2 = self.model_args.fc_hidden_dim2\n\n        self.conv_model = nn.Sequential(\n            # (batch size, number of features, max seq len)\n            nn.Conv1d(\n                in_channels=self.input_dim,\n                out_channels=hidden_dim,\n                kernel_size=kernel_size,\n            ),  # (batch size, hidden_dim, max seq len - 2)\n            nn.ReLU(),\n            nn.Conv1d(\n                in_channels=hidden_dim, out_channels=hidden_dim, kernel_size=kernel_size\n            ),  # (batch size, hidden_dim, max seq len - 4)\n            nn.ReLU(),\n            nn.Conv1d(\n                in_channels=hidden_dim, out_channels=hidden_dim, kernel_size=kernel_size\n            ),  # (batch size, hidden_dim, max seq len - 6)\n            nn.ReLU(),\n            nn.MaxPool1d(\n                kernel_size=self.model_args.pooling_kernel_size\n            ),  # (batch size, hidden_dim, (max seq len -6) / 2)\n            nn.Dropout(fc_dropout),  # (batch size, hidden_dim, (max seq len -6) / 2)\n            nn.Flatten(),  # (batch size, hidden_dim * ((max seq len -6) / 2))\n        )\n        self.fc = nn.Sequential(\n            nn.Linear(\n                ((self.max_scanpath_length - 6) // 2) * hidden_dim, fc_hidden_dim1\n            ),  # (batch size, 50)\n            nn.ReLU(),\n            nn.Dropout(fc_dropout),  # (batch size, 50)\n            nn.Linear(fc_hidden_dim1, fc_hidden_dim2),  # (batch size, 20)\n            nn.ReLU(),\n            nn.Linear(fc_hidden_dim2, self.num_classes),  # (batch size, 2)\n        )\n\n    def forward(self, x: torch.Tensor) -&gt; tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"\n        Forward pass of the CNN model.\n\n        Args:\n            x (torch.Tensor): The input tensor.\n\n        Returns:\n            tuple: A tuple containing the output tensor and hidden representations.\n        \"\"\"\n        x = x.transpose(1, 2)  # (batch size, number of features, max seq len)\n        hidden_representations = self.conv_model(x)\n        x = self.fc(hidden_representations)\n        return x, hidden_representations\n</code></pre>"},{"location":"reference/models/ahn_model/#models.ahn_model.AhnCNNModel.forward","title":"<code>forward(x)</code>","text":"<p>Forward pass of the CNN model.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>The input tensor.</p> required <p>Returns:</p> Name Type Description <code>tuple</code> <code>tuple[Tensor, Tensor]</code> <p>A tuple containing the output tensor and hidden representations.</p> Source code in <code>src/models/ahn_model.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Forward pass of the CNN model.\n\n    Args:\n        x (torch.Tensor): The input tensor.\n\n    Returns:\n        tuple: A tuple containing the output tensor and hidden representations.\n    \"\"\"\n    x = x.transpose(1, 2)  # (batch size, number of features, max seq len)\n    hidden_representations = self.conv_model(x)\n    x = self.fc(hidden_representations)\n    return x, hidden_representations\n</code></pre>"},{"location":"reference/models/ahn_model/#models.ahn_model.AhnModel","title":"<code>AhnModel</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Base model for Ahn et al.</p> <p>Parameters:</p> Name Type Description Default <code>model_args</code> <code>AhnArgs</code> <p>The model arguments.</p> required <code>trainer_args</code> <code>TrainerDL</code> <p>The trainer arguments.</p> required <code>data_args</code> <code>DataArgs</code> <p>The data arguments.</p> required Source code in <code>src/models/ahn_model.py</code> <pre><code>class AhnModel(BaseModel):\n    \"\"\"\n    Base model for Ahn et al.\n\n    Args:\n        model_args (AhnArgs): The model arguments.\n        trainer_args (TrainerDL): The trainer arguments.\n        data_args (DataArgs): The data arguments.\n    \"\"\"\n\n    def __init__(\n        self,\n        model_args: Ahn,\n        trainer_args: TrainerDL,\n        data_args: DataArgs,\n    ):\n        super().__init__(\n            model_args=model_args, trainer_args=trainer_args, data_args=data_args\n        )\n        self.model_args = model_args\n        self.input_dim = (\n            model_args.fixation_dim\n            if model_args.use_fixation_report\n            else model_args.eyes_dim\n        )\n        self.preorder = model_args.preorder\n        self.model: nn.Module\n\n        self.train()\n        self.save_hyperparameters()\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Forward pass of the model.\n\n        Args:\n            x (torch.Tensor): The input tensor.\n\n        Returns:\n            torch.Tensor: The output tensor.\n        \"\"\"\n        raise NotImplementedError\n\n    def shared_step(\n        self, batch: list\n    ) -&gt; tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n        \"\"\"\n        Shared step for training and validation.\n\n        Args:\n            batch (list): The input batch.\n\n        Returns:\n            tuple: A tuple containing ordered labels, loss, ordered logits, labels, and logits.\n        \"\"\"\n        batch_data = self.unpack_batch(batch)\n        assert batch_data.fixation_features is not None, 'eyes_tensor not in batch_dict'\n        labels = batch_data.labels\n        logits, unused_hidden = self(x=batch_data.fixation_features)\n\n        if logits.ndim == 1:\n            logits = logits.unsqueeze(0)\n        loss = self.loss(logits, labels)\n\n        return labels, loss, logits\n</code></pre>"},{"location":"reference/models/ahn_model/#models.ahn_model.AhnModel.forward","title":"<code>forward(x)</code>","text":"<p>Forward pass of the model.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>The input tensor.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: The output tensor.</p> Source code in <code>src/models/ahn_model.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Forward pass of the model.\n\n    Args:\n        x (torch.Tensor): The input tensor.\n\n    Returns:\n        torch.Tensor: The output tensor.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/models/ahn_model/#models.ahn_model.AhnModel.shared_step","title":"<code>shared_step(batch)</code>","text":"<p>Shared step for training and validation.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>list</code> <p>The input batch.</p> required <p>Returns:</p> Name Type Description <code>tuple</code> <code>tuple[Tensor, Tensor, Tensor]</code> <p>A tuple containing ordered labels, loss, ordered logits, labels, and logits.</p> Source code in <code>src/models/ahn_model.py</code> <pre><code>def shared_step(\n    self, batch: list\n) -&gt; tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Shared step for training and validation.\n\n    Args:\n        batch (list): The input batch.\n\n    Returns:\n        tuple: A tuple containing ordered labels, loss, ordered logits, labels, and logits.\n    \"\"\"\n    batch_data = self.unpack_batch(batch)\n    assert batch_data.fixation_features is not None, 'eyes_tensor not in batch_dict'\n    labels = batch_data.labels\n    logits, unused_hidden = self(x=batch_data.fixation_features)\n\n    if logits.ndim == 1:\n        logits = logits.unsqueeze(0)\n    loss = self.loss(logits, labels)\n\n    return labels, loss, logits\n</code></pre>"},{"location":"reference/models/ahn_model/#models.ahn_model.AhnRNNModel","title":"<code>AhnRNNModel</code>","text":"<p>               Bases: <code>AhnModel</code></p> <p>RNN model for Ahn et al. baseline</p> <p>Parameters:</p> Name Type Description Default <code>model_args</code> <code>AhnRNN</code> <p>The model arguments.</p> required <code>trainer_args</code> <code>TrainerDL</code> <p>The trainer arguments.</p> required <code>data_args</code> <code>DataArgs</code> <p>The data arguments.</p> required Source code in <code>src/models/ahn_model.py</code> <pre><code>@register_model\nclass AhnRNNModel(AhnModel):\n    \"\"\"\n    RNN model for Ahn et al. baseline\n\n    Args:\n        model_args (AhnRNN): The model arguments.\n        trainer_args (TrainerDL): The trainer arguments.\n        data_args (DataArgs): The data arguments.\n    \"\"\"\n\n    def __init__(\n        self,\n        model_args: AhnRNN,\n        trainer_args: TrainerDL,\n        data_args: DataArgs,\n    ):\n        super().__init__(model_args, trainer_args, data_args=data_args)\n        self.lstm = nn.LSTM(\n            input_size=self.input_dim,\n            hidden_size=self.model_args.hidden_dim,\n            bidirectional=True,\n            batch_first=True,\n            num_layers=model_args.num_lstm_layers,\n        )\n        self.fc = nn.Sequential(\n            nn.Dropout(self.model_args.fc_dropout),  # (batch_size, hidden_size * 2)\n            nn.Linear(\n                model_args.hidden_dim * 2, model_args.hidden_dim * 2\n            ),  # (batch_size, 50)\n            nn.ReLU(),\n            nn.Dropout(self.model_args.fc_dropout),\n            nn.Linear(\n                model_args.hidden_dim * 2,\n                model_args.fc_hidden_dim,\n            ),  # (batch_size, 2)\n            nn.ReLU(),\n            nn.Linear(model_args.fc_hidden_dim, self.num_classes),  # (batch_size, 2)\n            nn.ReLU(),\n        )\n\n    def forward(self, x: torch.Tensor) -&gt; tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"\n        Forward pass of the RNN model.\n\n        Args:\n            x (torch.Tensor): The input tensor.\n\n        Returns:\n            tuple: A tuple containing the output tensor and hidden representations.\n        \"\"\"\n        # take the last hidden state of the lstm\n        x, _ = self.lstm(x)  # (batch_size, seq_len, hidden_size * 2)\n        x = x[:, -1, :]  # (batch_size, hidden_size * 2)\n        hidden_representations = x.clone()\n        x = self.fc(x)  # (batch_size, 2)\n        return x, hidden_representations\n</code></pre>"},{"location":"reference/models/ahn_model/#models.ahn_model.AhnRNNModel.forward","title":"<code>forward(x)</code>","text":"<p>Forward pass of the RNN model.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>The input tensor.</p> required <p>Returns:</p> Name Type Description <code>tuple</code> <code>tuple[Tensor, Tensor]</code> <p>A tuple containing the output tensor and hidden representations.</p> Source code in <code>src/models/ahn_model.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Forward pass of the RNN model.\n\n    Args:\n        x (torch.Tensor): The input tensor.\n\n    Returns:\n        tuple: A tuple containing the output tensor and hidden representations.\n    \"\"\"\n    # take the last hidden state of the lstm\n    x, _ = self.lstm(x)  # (batch_size, seq_len, hidden_size * 2)\n    x = x[:, -1, :]  # (batch_size, hidden_size * 2)\n    hidden_representations = x.clone()\n    x = self.fc(x)  # (batch_size, 2)\n    return x, hidden_representations\n</code></pre>"},{"location":"reference/models/base_model/","title":"base_model","text":"<p>This module contains the base model class and the multimodal model class.</p>"},{"location":"reference/models/base_model/#models.base_model.BaseMLModel","title":"<code>BaseMLModel</code>","text":"<p>               Bases: <code>SharedBaseModel</code></p> <p>Base class for all ML models.</p> Source code in <code>src/models/base_model.py</code> <pre><code>class BaseMLModel(SharedBaseModel):\n    \"\"\"\n    Base class for all ML models.\n    \"\"\"\n\n    def __init__(\n        self, model_args: MLModelArgs, trainer_args: TrainerML, data_args: DataArgs\n    ):\n        # Initialize shared base attributes\n        SharedBaseModel.__init__(self, model_args=model_args, data_args=data_args)\n\n        # ML-specific attributes\n        self.num_workers = trainer_args.num_workers\n\n        self.regime_names: list[str] = [\n            'new_item',\n            'new_subject',\n            'new_item_and_subject',\n            'all',\n        ]  # This order is defined in the data module!\n\n        self._init_classifier(model_args)\n        self.balanced_class_accuracies = {}\n        self.stage_count = {}\n\n        self.ia_features = model_args.ia_features\n\n        #### features builder ###\n        self.use_item_level_features: bool = (\n            len(model_args.item_level_features_modes) &gt; 0\n        )\n\n        self.batch_size = model_args.batch_size\n        self.feature_builder_device = 'cpu'\n\n        self.acc_y_true_val = []\n        self.acc_y_pred_val = []\n\n        self.trial_level_feature_names: Union[None, list[str]] = None\n\n        self.pca_explained_variance_ratio_threshold = (\n            model_args.pca_explained_variance_ratio_threshold\n        )\n        self.pca = None\n        self.model_args = model_args\n        if self.task != TaskTypes.REGRESSION:\n            self.label_encoder = LabelEncoder()\n        self.loss_func = (\n            metrics.mean_squared_error\n            if self.task == TaskTypes.REGRESSION\n            else metrics.log_loss\n        )\n\n    def _init_classifier(self, model_args) -&gt; None:\n        \"\"\"\n        Initialize the classifier.\n        \"\"\"\n        # make pipeline from model_params.sklearn_pipeline\n        ## empty pipeline\n        self.classifier = Pipeline([])\n        for step_name, method_import_path in model_args.sklearn_pipeline:\n            module_name, class_name = method_import_path.rsplit('.', 1)\n            module = importlib.import_module(module_name)\n            class_obj = getattr(module, class_name)\n            self.classifier.steps.append((step_name, class_obj()))\n\n        # set params\n        logger.info('Setting model params')\n        model_args.init_sklearn_pipeline_params()\n        self.classifier.set_params(**model_args.sklearn_pipeline_params)\n\n    def shared_fit(self, dm) -&gt; list:\n        \"\"\"\n        Fit the model to the data.\n        \"\"\"\n        logger.info('Fitting model')\n        train_dataloader = DataLoader(\n            dm.train_dataset,\n            batch_size=self.batch_size,\n            num_workers=self.num_workers,\n            shuffle=False,\n            pin_memory=True,\n        )\n        try:\n            self.trial_level_feature_names = dm.train_dataset.trial_level_feature_names\n        except AttributeError:\n            logger.warning(\n                'No trial level feature names found in the training dataset.'\n            )\n\n        train_batches = self.unpack_data(train_dataloader)\n        return train_batches\n\n    def predict(\n        self, dataset\n    ) -&gt; tuple[torch.Tensor, torch.Tensor | None, torch.Tensor]:\n        \"\"\"\n        Predict the model on the data.\n        \"\"\"\n        dev_dataloader = DataLoader(\n            dataset,\n            batch_size=self.batch_size,\n            num_workers=self.num_workers,\n            shuffle=False,\n            pin_memory=True,\n        )\n        dev_batches = self.unpack_data(dev_dataloader)\n        preds_list, probs_list = self.model_specific_predict(dev_batches)\n        y_true_list = []\n        for dev_batch in tqdm(dev_batches, desc='Feature extraction (pred)'):\n            y_true_list.append(dev_batch.labels)\n\n        preds = torch.cat(preds_list, dim=0)\n\n        y_true = torch.cat(y_true_list, dim=0)\n        if self.task != TaskTypes.REGRESSION:\n            probs = torch.cat(probs_list, dim=0)\n        else:\n            probs = None\n\n        return preds, probs, y_true\n\n    def evaluate(self, eval_dataset, stage: str, validation_map: str) -&gt; None:\n        \"\"\"\n        Evaluate the model on the data.\n        \"\"\"\n        self.stage = stage\n        self.validation_map = validation_map\n\n        (\n            self.preds,\n            self.probs,\n            self.y_true,\n        ) = self.predict(eval_dataset)\n\n        if stage == 'val':\n            self.acc_y_true_val.append(self.y_true)\n            self.acc_y_pred_val.append(self.preds)\n\n        self._on_eval_end()\n\n    def _on_eval_end(self) -&gt; None:\n        \"\"\"\n        Function to run at the end of evaluation.\n        \"\"\"\n        assert self.preds is not None\n        assert self.y_true is not None\n\n        # convert tensors to numpy\n        self.preds = self.preds.numpy()\n        self.y_true = self.y_true.numpy()\n\n        if self.task != TaskTypes.REGRESSION:\n            assert self.probs is not None\n            self.probs = self.probs.numpy()\n\n        self._log_metrics()\n\n        self.stage_count[self.validation_map] = len(self.y_true)\n        if self.task != TaskTypes.REGRESSION:\n            self.balanced_class_accuracies[self.validation_map] = (\n                metrics.balanced_accuracy_score(\n                    y_true=self.y_true,\n                    y_pred=self.preds,\n                )\n            )\n\n        (\n            self.preds,\n            self.probs,\n            self.y_true,\n        ) = (None, None, None)\n\n    def _log_metrics(self) -&gt; None:\n        \"\"\"\n        Log the metrics of the evaluation.\n        \"\"\"\n\n        if self.trial_level_feature_names is not None:\n            wandb.log(\n                {\n                    'Feature_names': self.trial_level_feature_names,\n                }\n            )\n        if self.task == TaskTypes.REGRESSION:\n            return\n\n        # Log Confusion Matrices\n        wandb.log(\n            {\n                f'Confusion_Matrix/{self.stage}_{self.validation_map}': wandb.plot.confusion_matrix(\n                    preds=self.preds,\n                    y_true=self.y_true,\n                    class_names=self.class_names,\n                    title=f'Confusion_Matrix/{self.stage}_{self.validation_map}',\n                )\n            }\n        )\n\n        # Log ROC curve\n        try:\n            wandb.log(\n                {\n                    f'ROC_Curve/{self.stage}_{self.validation_map}': wandb.plot.roc_curve(\n                        y_true=self.y_true,\n                        y_probas=self.probs,\n                        labels=self.class_names,\n                        title=f'ROC_Curve/{self.stage}_{self.validation_map}',\n                    )\n                }\n            )\n\n        except TypeError:\n            logger.warning('ROC curve not calculated!!')\n\n        # per class metrics (Unordered)\n        class_names_unordered = np.array(self.class_names)[np.unique(self.y_true)]\n        self.per_class_metrics = metrics.classification_report(\n            y_true=self.y_true,\n            y_pred=self.preds,\n            target_names=class_names_unordered,\n            output_dict=True,\n        )\n        for class_name, metrics_dict in self.per_class_metrics.items():\n            if isinstance(metrics_dict, dict):\n                for metric_name, value in metrics_dict.items():\n                    wandb.summary[\n                        f'{metric_name}_{class_name}/{self.stage}_{self.validation_map}'\n                    ] = value\n            else:\n                wandb.summary[f'{class_name}/{self.stage}_{self.validation_map}'] = (\n                    metrics_dict\n                )\n\n    def on_stage_end(self) -&gt; None:\n        \"\"\"\n        Function to run at the end of a stage.\n        \"\"\"\n        if self.task != TaskTypes.REGRESSION:\n            # Log balanced classless accuracy\n            if self.stage == 'val':\n                wandb.summary[f'Balanced_Accuracy/{self.stage}_average'] = np.mean(\n                    list(self.balanced_class_accuracies.values())\n                )\n                wandb.summary[f'Balanced_Accuracy/{self.stage}_weighted_average'] = (\n                    np.average(\n                        list(self.balanced_class_accuracies.values()),\n                        weights=list(self.stage_count.values()),\n                    )\n                )\n\n            for eval_regime, metric_val in self.balanced_class_accuracies.items():\n                wandb.summary[f'Balanced_Accuracy/{self.stage}_{eval_regime}'] = (\n                    metric_val\n                )\n\n        if self.stage == 'val':\n            all_val_y_true = torch.cat(self.acc_y_true_val)\n            all_val_y_pred = torch.cat(self.acc_y_pred_val)\n\n            if self.task != TaskTypes.REGRESSION:\n                wandb.summary['Balanced_Accuracy/val_all'] = (\n                    metrics.balanced_accuracy_score(\n                        y_true=all_val_y_true, y_pred=all_val_y_pred\n                    )\n                )\n\n            if self.model_args.use_class_weighted_loss:\n                # Map each y_true to its class weight\n                sample_weight = np.array(\n                    [self.class_weights[int(label)] for label in all_val_y_true]\n                )\n                loss = self.loss_func(\n                    y_true=all_val_y_true,\n                    y_pred=all_val_y_pred,\n                    sample_weight=sample_weight,\n                )\n            else:\n                loss = self.loss_func(y_true=all_val_y_true, y_pred=all_val_y_pred)\n            wandb.summary['loss/val_all'] = loss\n\n        # empty caches\n        self.balanced_class_accuracies = {}\n        self.stage_count = {}\n\n    def unpack_data(self, dataloader) -&gt; list:\n        \"\"\"\n        Unpacks the batch into the different tensors.\n        \"\"\"\n        return [BaseModel.unpack_batch(batch) for batch in dataloader]\n\n    def _features_builder(self, train_batch) -&gt; torch.Tensor:\n        \"\"\"\n        Concatenate features for the model from different sources.\n        \"\"\"\n        if train_batch.labels.ndim == 0:\n            for key, value in train_batch.__dict__.items():\n                if isinstance(value, torch.Tensor):\n                    train_batch.__dict__[key] = value.unsqueeze(0)\n\n        features_list = []\n        if self.use_item_level_features:\n            if hasattr(train_batch, 'trial_level_features'):\n                features_list.append(\n                    train_batch.trial_level_features.squeeze().to(\n                        self.feature_builder_device\n                    )\n                )\n            else:\n                raise ValueError('No trial level features found in the batch data.')\n\n        assert len(features_list) &gt; 0, 'No features found for the model.'\n        features = torch.cat(features_list)\n\n        if features.ndim == 1:\n            features = features.unsqueeze(1)\n\n        return features\n\n    def _prepare_features_and_labels(\n        self, batches, training: bool = True\n    ) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray, list[str]]:\n        \"\"\"\n        Prepare features and labels from a list of batches.\n        \"\"\"\n        features_list: list[torch.Tensor] = []\n        y_true_list: list[torch.Tensor] = []\n        trial_groups_keys_list: list[np.ndarray] = []\n        iterator = tqdm(batches, desc='Feature extraction') if training else batches\n        for batch in iterator:\n            features = self._features_builder(batch).to('cpu')\n\n            trial_groups_keys_list.append(\n                np.transpose(np.array(batch.batch_item_keys), axes=(1, 0))\n            )\n\n            features_list.append(features)\n            y_true_list.append(batch.labels)\n        features = torch.cat(features_list, dim=0)\n        y_true = torch.cat(y_true_list, dim=0)\n        trial_groups_keys = np.concatenate(trial_groups_keys_list)\n\n        trial_key_columns: list[str] = list(\n            np.array(batches[0].trial_groupby_columns)[:, 0]\n        )\n\n        return (\n            features.numpy(),\n            y_true.numpy(),\n            trial_groups_keys,\n            trial_key_columns,\n        )\n\n    def _predict_with_fallback(\n        self, features: np.ndarray\n    ) -&gt; Tuple[torch.Tensor, torch.Tensor] | torch.Tensor:\n        \"\"\"\n        Try to use predict_proba, if not available use predict and convert to probabilities.\n        \"\"\"\n        try:\n            probs = torch.tensor(self.classifier.predict_proba(features))\n            preds = probs.argmax(dim=1)\n        except AttributeError:\n            # predict_proba not available\n            preds = torch.tensor(self.classifier.predict(features), dtype=torch.int64)\n            probs = torch.zeros((len(preds), self.num_classes))\n            probs[range(len(preds)), preds.int()] = 1\n        return preds, probs\n\n    def fit(self, dm) -&gt; None:\n        \"\"\"\n        Default shared_fit implementation.\n        Most subclasses can use this unless they have special requirements.\n        \"\"\"\n        train_batches = self.shared_fit(dm)\n        features, y_true, trial_groups_keys, trial_key_columns = (\n            self._prepare_features_and_labels(train_batches, training=True)\n        )\n        if self.task != TaskTypes.REGRESSION:\n            y_true = self.label_encoder.fit_transform(y_true)\n\n        if self.pca_explained_variance_ratio_threshold &lt; 1.0:\n            features = self._apply_pca(features)\n\n        if self.model_args.use_class_weighted_loss:\n            classes_weights = class_weight.compute_sample_weight('balanced', y=y_true)\n            self.classifier.fit(features, y_true, clf__sample_weight=classes_weights)\n        else:\n            self.classifier.fit(features, y_true)\n\n    def model_specific_predict(\n        self, dev_batches: list\n    ) -&gt; tuple[list[torch.Tensor], list[torch.Tensor | None]]:\n        preds_list: list[torch.Tensor] = []\n        probs_list: list[torch.Tensor] = []\n        features_list = []\n        for batch in dev_batches:\n            features = self._features_builder(batch).to('cpu').numpy()\n\n            if self.pca_explained_variance_ratio_threshold &lt; 1.0:\n                features = self._apply_pca(features)\n            features_list.append(features)\n\n        for features in features_list:\n            if self.task == TaskTypes.REGRESSION:\n                preds = torch.tensor(self.classifier.predict(features))\n                probs = None\n            else:\n                preds, probs = self._predict_with_fallback(features)\n                preds = torch.tensor(self.label_encoder.inverse_transform(preds))\n\n            probs_list.append(probs)\n            preds_list.append(preds)\n\n        return preds_list, probs_list\n\n    def _apply_pca(self, features: np.ndarray) -&gt; np.ndarray:\n        if self.pca is None:\n            self.pca = PCA(n_components=self.pca_explained_variance_ratio_threshold)\n            features = self.pca.fit_transform(features)\n            explained_variance = self.pca.explained_variance_ratio_.sum()\n            # check if wandb is available\n            if wandb.run is not None:\n                wandb.log({'PCA_explained_variance': explained_variance})\n                wandb.log({'PCA_num_components': self.pca.n_components_})\n        else:\n            features = self.pca.transform(features)\n        return features\n</code></pre>"},{"location":"reference/models/base_model/#models.base_model.BaseMLModel.evaluate","title":"<code>evaluate(eval_dataset, stage, validation_map)</code>","text":"<p>Evaluate the model on the data.</p> Source code in <code>src/models/base_model.py</code> <pre><code>def evaluate(self, eval_dataset, stage: str, validation_map: str) -&gt; None:\n    \"\"\"\n    Evaluate the model on the data.\n    \"\"\"\n    self.stage = stage\n    self.validation_map = validation_map\n\n    (\n        self.preds,\n        self.probs,\n        self.y_true,\n    ) = self.predict(eval_dataset)\n\n    if stage == 'val':\n        self.acc_y_true_val.append(self.y_true)\n        self.acc_y_pred_val.append(self.preds)\n\n    self._on_eval_end()\n</code></pre>"},{"location":"reference/models/base_model/#models.base_model.BaseMLModel.fit","title":"<code>fit(dm)</code>","text":"<p>Default shared_fit implementation. Most subclasses can use this unless they have special requirements.</p> Source code in <code>src/models/base_model.py</code> <pre><code>def fit(self, dm) -&gt; None:\n    \"\"\"\n    Default shared_fit implementation.\n    Most subclasses can use this unless they have special requirements.\n    \"\"\"\n    train_batches = self.shared_fit(dm)\n    features, y_true, trial_groups_keys, trial_key_columns = (\n        self._prepare_features_and_labels(train_batches, training=True)\n    )\n    if self.task != TaskTypes.REGRESSION:\n        y_true = self.label_encoder.fit_transform(y_true)\n\n    if self.pca_explained_variance_ratio_threshold &lt; 1.0:\n        features = self._apply_pca(features)\n\n    if self.model_args.use_class_weighted_loss:\n        classes_weights = class_weight.compute_sample_weight('balanced', y=y_true)\n        self.classifier.fit(features, y_true, clf__sample_weight=classes_weights)\n    else:\n        self.classifier.fit(features, y_true)\n</code></pre>"},{"location":"reference/models/base_model/#models.base_model.BaseMLModel.on_stage_end","title":"<code>on_stage_end()</code>","text":"<p>Function to run at the end of a stage.</p> Source code in <code>src/models/base_model.py</code> <pre><code>def on_stage_end(self) -&gt; None:\n    \"\"\"\n    Function to run at the end of a stage.\n    \"\"\"\n    if self.task != TaskTypes.REGRESSION:\n        # Log balanced classless accuracy\n        if self.stage == 'val':\n            wandb.summary[f'Balanced_Accuracy/{self.stage}_average'] = np.mean(\n                list(self.balanced_class_accuracies.values())\n            )\n            wandb.summary[f'Balanced_Accuracy/{self.stage}_weighted_average'] = (\n                np.average(\n                    list(self.balanced_class_accuracies.values()),\n                    weights=list(self.stage_count.values()),\n                )\n            )\n\n        for eval_regime, metric_val in self.balanced_class_accuracies.items():\n            wandb.summary[f'Balanced_Accuracy/{self.stage}_{eval_regime}'] = (\n                metric_val\n            )\n\n    if self.stage == 'val':\n        all_val_y_true = torch.cat(self.acc_y_true_val)\n        all_val_y_pred = torch.cat(self.acc_y_pred_val)\n\n        if self.task != TaskTypes.REGRESSION:\n            wandb.summary['Balanced_Accuracy/val_all'] = (\n                metrics.balanced_accuracy_score(\n                    y_true=all_val_y_true, y_pred=all_val_y_pred\n                )\n            )\n\n        if self.model_args.use_class_weighted_loss:\n            # Map each y_true to its class weight\n            sample_weight = np.array(\n                [self.class_weights[int(label)] for label in all_val_y_true]\n            )\n            loss = self.loss_func(\n                y_true=all_val_y_true,\n                y_pred=all_val_y_pred,\n                sample_weight=sample_weight,\n            )\n        else:\n            loss = self.loss_func(y_true=all_val_y_true, y_pred=all_val_y_pred)\n        wandb.summary['loss/val_all'] = loss\n\n    # empty caches\n    self.balanced_class_accuracies = {}\n    self.stage_count = {}\n</code></pre>"},{"location":"reference/models/base_model/#models.base_model.BaseMLModel.predict","title":"<code>predict(dataset)</code>","text":"<p>Predict the model on the data.</p> Source code in <code>src/models/base_model.py</code> <pre><code>def predict(\n    self, dataset\n) -&gt; tuple[torch.Tensor, torch.Tensor | None, torch.Tensor]:\n    \"\"\"\n    Predict the model on the data.\n    \"\"\"\n    dev_dataloader = DataLoader(\n        dataset,\n        batch_size=self.batch_size,\n        num_workers=self.num_workers,\n        shuffle=False,\n        pin_memory=True,\n    )\n    dev_batches = self.unpack_data(dev_dataloader)\n    preds_list, probs_list = self.model_specific_predict(dev_batches)\n    y_true_list = []\n    for dev_batch in tqdm(dev_batches, desc='Feature extraction (pred)'):\n        y_true_list.append(dev_batch.labels)\n\n    preds = torch.cat(preds_list, dim=0)\n\n    y_true = torch.cat(y_true_list, dim=0)\n    if self.task != TaskTypes.REGRESSION:\n        probs = torch.cat(probs_list, dim=0)\n    else:\n        probs = None\n\n    return preds, probs, y_true\n</code></pre>"},{"location":"reference/models/base_model/#models.base_model.BaseMLModel.shared_fit","title":"<code>shared_fit(dm)</code>","text":"<p>Fit the model to the data.</p> Source code in <code>src/models/base_model.py</code> <pre><code>def shared_fit(self, dm) -&gt; list:\n    \"\"\"\n    Fit the model to the data.\n    \"\"\"\n    logger.info('Fitting model')\n    train_dataloader = DataLoader(\n        dm.train_dataset,\n        batch_size=self.batch_size,\n        num_workers=self.num_workers,\n        shuffle=False,\n        pin_memory=True,\n    )\n    try:\n        self.trial_level_feature_names = dm.train_dataset.trial_level_feature_names\n    except AttributeError:\n        logger.warning(\n            'No trial level feature names found in the training dataset.'\n        )\n\n    train_batches = self.unpack_data(train_dataloader)\n    return train_batches\n</code></pre>"},{"location":"reference/models/base_model/#models.base_model.BaseMLModel.unpack_data","title":"<code>unpack_data(dataloader)</code>","text":"<p>Unpacks the batch into the different tensors.</p> Source code in <code>src/models/base_model.py</code> <pre><code>def unpack_data(self, dataloader) -&gt; list:\n    \"\"\"\n    Unpacks the batch into the different tensors.\n    \"\"\"\n    return [BaseModel.unpack_batch(batch) for batch in dataloader]\n</code></pre>"},{"location":"reference/models/base_model/#models.base_model.BaseModel","title":"<code>BaseModel</code>","text":"<p>               Bases: <code>LightningModule</code>, <code>SharedBaseModel</code></p> <p>Base model class for the multi-modal models.</p> Source code in <code>src/models/base_model.py</code> <pre><code>class BaseModel(pl.LightningModule, SharedBaseModel):\n    \"\"\"Base model class for the multi-modal models.\"\"\"\n\n    def __init__(\n        self, model_args: DLModelArgs, trainer_args: TrainerDL, data_args: DataArgs\n    ):\n        # Initialize PyTorch Lightning module\n        pl.LightningModule.__init__(self)\n        # Initialize shared base attributes\n        SharedBaseModel.__init__(self, model_args=model_args, data_args=data_args)\n        self.max_data_seq_len = data_args.max_seq_len\n        self.max_model_supported_len = model_args.max_supported_seq_len\n        self.actual_max_needed_len = min(\n            self.max_data_seq_len, self.max_model_supported_len\n        )\n        # DL-specific attributes\n        logger.info(f'{self.use_eyes_only=}')\n        self.learning_rate = trainer_args.learning_rate\n        self.batch_size = model_args.batch_size\n        self.max_scanpath_length: int = data_args.max_scanpath_length\n\n        self.regime_names: list[SetNames] = REGIMES\n        logger.info(f'{self.regime_names=}')\n\n        self.val_max_acc_dict = {\n            k: {m: 0.0 for m in ['average', 'weighted_average']}\n            for k in ['Balanced', 'Classless']\n        }\n\n        if self.task == TaskTypes.REGRESSION:\n            self.loss = nn.MSELoss()\n        else:\n            if model_args.use_class_weighted_loss:\n                self.loss = nn.CrossEntropyLoss(weight=torch.tensor(self.class_weights))\n            else:\n                self.loss = nn.CrossEntropyLoss()\n        self.validation_step_losses = defaultdict(list)\n\n        (\n            metrics,\n            confusion_matrix,\n            roc,\n            balanced_accuracy,\n        ) = self.configure_metrics()\n\n        self.train_metrics = metrics.clone(postfix='/train')\n        self.val_metrics_list = nn.ModuleList(\n            [\n                metrics.clone(postfix=f'/val_{regime_name}')\n                for regime_name in self.regime_names\n            ]\n        )\n        self.test_metrics_list = nn.ModuleList(\n            [\n                metrics.clone(postfix=f'/test_{regime_name}')\n                for regime_name in self.regime_names\n            ]\n        )\n\n        if self.task != TaskTypes.REGRESSION:\n            self.train_cm = confusion_matrix.clone()\n            self.val_cm_list = nn.ModuleList(\n                [confusion_matrix.clone() for _ in self.regime_names]\n            )\n            self.test_cm_list = nn.ModuleList(\n                [confusion_matrix.clone() for _ in self.regime_names]\n            )\n            self.train_roc = roc.clone()\n            self.val_roc_list = nn.ModuleList([roc.clone() for _ in self.regime_names])\n            self.test_roc_list = nn.ModuleList([roc.clone() for _ in self.regime_names])\n\n            self.train_balanced_accuracy = balanced_accuracy.clone()\n            self.val_balanced_accuracy_list = nn.ModuleList(\n                [balanced_accuracy.clone() for _ in self.regime_names]\n            )\n            self.test_balanced_accuracy_list = nn.ModuleList(\n                [balanced_accuracy.clone() for _ in self.regime_names]\n            )\n\n    @abstractmethod\n    def forward(self, x) -&gt; torch.Tensor:\n        raise NotImplementedError\n\n    @staticmethod\n    def unpack_batch(batch: list) -&gt; tuple:\n        \"\"\"\n        Unpacks the batch into a namedtuple for dot access.\n\n        Args:\n            batch (list): The batch to unpack containing:\n                - batch[0] (dict): Dictionary with features including:\n                    - fixation_features: Optional[torch.Tensor] - fixations feature vectors for each fixation.\n                    - fixation_pads: Optional[torch.Tensor] - Feature vectors for fixations.\n                    - scanpath: Optional[torch.Tensor] - A series of IA_IDs for each trial scanpath.\n                    - scanpath_pads: Optional[torch.Tensor] - Feature vectors for IA_IDs in scanpath.\n                    - paragraph_input_ids: Optional[torch.Tensor] - Input_ids of each tokenized paragraph.\n                    - paragraph_input_masks: Optional[torch.Tensor] - Masks for paragraph input_ids.\n                    - input_ids: Optional[torch.Tensor] - Input_ids of tokenized paragraphs.\n                    - input_masks: Optional[torch.Tensor] - Masks for input_ids.\n                    - answer_mappings: Optional[torch.Tensor] - Mappings associated with answers.\n                    - inversions: Optional[torch.Tensor] - For each paragraph, IA_IDs associated with tokens.\n                    - inversions_pads: Optional[torch.Tensor] - Pads for IA_IDs in inversions.\n                    - eyes: Optional[torch.Tensor] - et_data_enriched feature vectors for each IA_ID.\n                - batch[1]: Labels associated with the batch.\n                - batch[2]: Keys for each item in the batch.\n                - batch[3]: Columns that form a trial.\n\n        Returns:\n            tuple: The unpacked batch with dot access (namedtuple).\n        \"\"\"\n\n        ExampleBatch = namedtuple(\n            'ExampleBatch',\n            list(batch[0].keys())\n            + ['labels', 'batch_item_keys', 'trial_groupby_columns'],\n        )\n        return ExampleBatch(\n            **dict(batch[0].items()),\n            labels=batch[1],\n            batch_item_keys=batch[2],\n            trial_groupby_columns=batch[3],\n        )\n\n    @abstractmethod\n    def shared_step(\n        self, batch: list\n    ) -&gt; tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n        raise NotImplementedError\n\n    def process_step(\n        self, batch: list, step_type: str, dataloader_idx=0\n    ) -&gt; torch.Tensor:\n        labels, loss, logits = self.shared_step(batch)\n\n        metrics = self.get_metrics_map(step_type, dataloader_idx)\n        is_single = (labels.ndim == 0) or (logits.ndim == 0)\n        if logits.ndim == 0:\n            logits = logits.unsqueeze(0)\n\n        if self.task == TaskTypes.REGRESSION:\n            if labels.ndim == 0:\n                labels = labels.unsqueeze(0)\n            if (len(labels) == 1) or (len(logits) == 1):\n                is_single = True\n            if is_single:\n                if logits.ndim != 1:\n                    logits = logits.squeeze(0)\n\n                metrics['metrics'].update(logits, labels)\n            else:\n                metrics['metrics'].update(logits.squeeze(), labels)\n\n        else:\n            # Handle last batch with batch size of one (logits may be 1D)\n            if logits.ndim == 1:\n                logits = logits.unsqueeze(0)\n\n            probs = logits.softmax(dim=1)\n            preds = probs.argmax(dim=1)\n\n            metrics['balanced_accuracy'](\n                preds, labels\n            )  # * must be one entry per class and sample, or after argmax.\n\n            if self.num_classes == 2:\n                probs = probs[:, 1]\n\n            metrics['metrics'].update(probs, labels)\n            metrics['cm'].update(probs, labels)\n            metrics['roc'].update(probs, labels)\n\n        return loss\n\n    def log_loss(self, loss: torch.Tensor, step_type: str, dataloader_idx=0) -&gt; None:\n        if step_type == 'train':\n            name = 'loss/train'\n        else:\n            name = f'loss/{step_type}_{self.regime_names[dataloader_idx]}'\n\n        self.log(\n            name=name,\n            value=loss,\n            prog_bar=False,\n            on_epoch=True,\n            on_step=False,\n            batch_size=self.batch_size,\n            add_dataloader_idx=False,\n            sync_dist=True,\n        )\n\n    def training_step(self, batch: list, _) -&gt; torch.Tensor:\n        loss = self.process_step(batch, 'train')\n        self.log_loss(loss, 'train')\n        return loss\n\n    def validation_step(self, batch: list, _, dataloader_idx=0) -&gt; torch.Tensor:\n        loss = self.process_step(\n            batch=batch, step_type='val', dataloader_idx=dataloader_idx\n        )\n        self.log_loss(loss, 'val', dataloader_idx)\n        self.validation_step_losses[dataloader_idx].append(loss)\n        return loss\n\n    def test_step(self, batch: list, _, dataloader_idx=0) -&gt; torch.Tensor:\n        loss = self.process_step(\n            batch=batch, step_type='test', dataloader_idx=dataloader_idx\n        )\n        self.log_loss(loss=loss, step_type='test', dataloader_idx=dataloader_idx)\n        return loss\n\n    def predict_step(\n        self,\n        batch: list,\n        _,\n        dataloader_idx=0,\n    ) -&gt; tuple[torch.Tensor, torch.Tensor]:\n        (\n            labels,\n            unused_loss,\n            logits,\n        ) = self.shared_step(batch)\n\n        if logits.ndim == 0:\n            logits = logits.unsqueeze(0)\n        if logits.ndim == 1:\n            logits = logits.unsqueeze(0)\n        if self.task == TaskTypes.REGRESSION:\n            probs = logits.squeeze()\n        else:\n            probs = logits.softmax(dim=1)\n\n        if labels.ndim == 0:\n            labels = labels.unsqueeze(0)\n\n        if self.num_classes == 2:\n            probs = probs[:, 1]\n        return labels, probs\n\n    def configure_optimizers(self):\n        # Define the optimizer\n        return torch.optim.AdamW(self.parameters(), lr=self.learning_rate)\n\n    def process_epoch_end(\n        self, step_type: str, regime_name: str, index=0\n    ) -&gt; tuple[int, torch.Tensor] | None:\n        metrics = self.get_metrics_map(step_type=step_type, index=index)\n\n        if self.task == TaskTypes.REGRESSION:\n            computed_metrics = metrics['metrics'].compute()\n            self.log_dict(\n                dictionary=computed_metrics,\n                prog_bar=False,\n                on_epoch=True,\n                on_step=False,\n                batch_size=self.batch_size,\n                add_dataloader_idx=False,\n                sync_dist=True,\n            )\n            metrics['metrics'].reset()\n            return\n\n        cm_torch = metrics['cm'].compute()\n        cm_formatted = self.format_confusion_matrix(cm=cm_torch)\n        self.log_confusion_matrix(\n            cm_data=cm_formatted, title=f'ConfusionMatrix/{step_type}_{regime_name}'\n        )\n\n        metrics['roc'].compute()\n        self.log_roc(roc=metrics['roc'], title=f'ROC/{step_type}_{regime_name}')\n\n        computed_metrics = metrics['metrics'].compute()\n        computed_balanced_accuracy = metrics['balanced_accuracy'].compute()\n        all_metrics = computed_metrics | {\n            f'Balanced_Accuracy/{step_type}_{regime_name}': computed_balanced_accuracy,\n        }\n\n        self.log_dict(\n            dictionary=all_metrics,\n            prog_bar=False,\n            on_epoch=True,\n            on_step=False,\n            batch_size=self.batch_size,\n            add_dataloader_idx=False,\n            sync_dist=True,\n        )\n\n        metrics['roc'].reset()\n        metrics['metrics'].reset()\n        metrics['cm'].reset()\n        metrics['balanced_accuracy'].reset()\n\n        return (\n            int(cm_torch.sum().item()),\n            computed_balanced_accuracy,\n        )  # ::int shouldn't be necessary\n\n    def on_train_epoch_end(self) -&gt; None:\n        name_ = 'train'\n        self.process_epoch_end(step_type=name_, regime_name=name_)\n\n    def on_validation_epoch_end(self) -&gt; None:\n        if self.validation_step_losses:\n            all_losses = torch.cat(\n                [\n                    torch.tensor(losses)\n                    for losses in self.validation_step_losses.values()\n                ]\n            )\n            overall_loss = all_losses.mean()\n            self.log(\n                name='loss/val_all',\n                value=overall_loss,\n                prog_bar=False,\n                on_epoch=True,\n                on_step=False,\n                add_dataloader_idx=False,\n                sync_dist=True,\n            )\n            self.validation_step_losses.clear()\n\n        if self.task == TaskTypes.REGRESSION:\n            return\n\n        # def on_validation_epoch_end(self) -&gt; None:\n        counts, balanced_accuracy_values = zip(\n            *[\n                self.process_epoch_end(\n                    step_type='val', index=val_index, regime_name=regime_name\n                )\n                for val_index, regime_name in enumerate(self.regime_names)\n            ]\n        )\n\n        balanced_accuracy_values = [x.item() for x in balanced_accuracy_values]\n\n        for k in ['Balanced']:\n            accuracy_values = balanced_accuracy_values\n            for m in ['average', 'weighted_average']:\n                # calculate the current value, update the current and maximum values\n                curr_value = (\n                    np.average(accuracy_values)\n                    if m == 'average'\n                    else np.average(accuracy_values, weights=counts)\n                )\n                assert isinstance(curr_value, float)\n                self.val_max_acc_dict[k][m] = max(\n                    [self.val_max_acc_dict[k][m], curr_value]\n                )\n\n                partial_acc_log = partial(\n                    self.log,\n                    prog_bar=False,\n                    on_epoch=True,\n                    on_step=False,\n                    add_dataloader_idx=False,\n                    sync_dist=True,\n                )\n\n                partial_acc_log(\n                    name=f'{k}_Accuracy/val_{m}',\n                    value=curr_value,\n                )\n                partial_acc_log(\n                    name=f'{k}_Accuracy/val_best_epoch_{m}',\n                    value=self.val_max_acc_dict[k][m],\n                )\n\n    def on_test_epoch_end(self) -&gt; None:\n        for test_index, regime_name in enumerate(self.regime_names):\n            self.process_epoch_end(\n                step_type='test', index=test_index, regime_name=regime_name\n            )\n\n    def log_roc(\n        self, roc: cls_metrics.MulticlassROC | cls_metrics.BinaryROC, title: str\n    ) -&gt; None:\n        ax_: plt.axes.Axes\n        fig_: Any\n        fig_, ax_ = roc.plot(score=True)\n        ax_.set_title(title)\n        ax_.set_xlabel('False Positive Rate')\n        ax_.set_ylabel('True Positive Rate')\n\n        # Add a straight line from (0,0) to (1,1) with the legend \"Random\"\n        line = matplotlib_lines.Line2D(\n            [0, 1], [0, 1], color='red', linestyle='--', label='Random'\n        )\n        ax_.add_line(line)\n        ax_.legend()\n\n        self.logger.experiment.log(  # type: ignore\n            {\n                title: wandb.Image(fig_, caption=title),\n            }\n        )\n        # close the figure to prevent memory leaks\n        plt.close(fig_)\n\n    def format_confusion_matrix(self, cm: torch.Tensor) -&gt; list[list]:\n        \"\"\"\n        Formats a confusion matrix into a list of lists containing\n        class names and their corresponding values.\n\n        Args:\n            cm (torch.Tensor): The confusion matrix to format.\n\n        Returns:\n            list[list]: A list of lists containing class names and their corresponding values.\n        \"\"\"\n        class_names = self.class_names\n        cm_list = cm.tolist()\n        return [\n            [class_names[i], class_names[j], cm_list[i][j]]\n            for i, j in itertools.product(\n                range(self.num_classes), range(self.num_classes)\n            )\n        ]\n\n    def log_confusion_matrix(self, cm_data: list[list], title: str) -&gt; None:\n        \"\"\"\n        Logs a confusion matrix to the experiment logger.\n\n        Args:\n            cm_data (list[list]): A list of lists representing the confusion matrix.\n                            Each inner list should contain the actual class name,\n                            the predicted and the number of values.\n            title (str): The title to use for the confusion matrix plot.\n\n        Returns:\n            None\n        \"\"\"\n        if isinstance(self.logger, WandbLogger):\n            wandb_logger = self.logger.experiment\n            fields = {\n                'Actual': 'Actual',\n                'Predicted': 'Predicted',\n                'nPredictions': 'nPredictions',\n            }\n\n            wandb_logger.log(\n                {\n                    title: wandb.plot_table(\n                        'EyeRead/multi-run-confusion-matrix',\n                        wandb.Table(\n                            columns=['Actual', 'Predicted', 'nPredictions'],\n                            data=cm_data,\n                        ),\n                        fields,\n                        {'title': title},\n                    ),\n                }\n            )\n        else:\n            warnings.warn('No wandb logger found, cannot log confusion matrix')\n\n    def configure_metrics(self):\n        \"\"\"Configures the metrics for the model.\"\"\"\n        if self.task == TaskTypes.BINARY_CLASSIFICATION:\n            logger.info('Using binary metrics')\n            metrics = torchmetrics.MetricCollection(\n                {\n                    'AUROC': cls_metrics.BinaryAUROC(\n                        validate_args=self.validate_metrics,\n                        # thresholds=10,\n                    ),\n                    'F1Score': cls_metrics.BinaryF1Score(\n                        validate_args=self.validate_metrics,\n                    ),\n                    'Precision': cls_metrics.BinaryPrecision(\n                        validate_args=self.validate_metrics,\n                    ),\n                    'Recall': cls_metrics.BinaryRecall(\n                        validate_args=self.validate_metrics,\n                    ),\n                    'Accuracy': cls_metrics.BinaryAccuracy(\n                        validate_args=self.validate_metrics,\n                    ),\n                }\n            )\n\n            confusion_matrix = cls_metrics.BinaryConfusionMatrix(\n                validate_args=self.validate_metrics\n            )\n\n            roc = cls_metrics.BinaryROC(\n                validate_args=self.validate_metrics,\n            )\n            # * Currently separate because expects preds or probs for each class which is not the case in binary case.\n            balanced_accuracy = cls_metrics.MulticlassAccuracy(\n                num_classes=self.num_classes,\n                average=self.average,\n                validate_args=self.validate_metrics,\n            )\n\n        elif self.task == TaskTypes.REGRESSION:\n            logger.info('Using regression metrics')\n            metrics = torchmetrics.MetricCollection(\n                {\n                    'RMSE': reg_metrics.MeanSquaredError(squared=False),\n                    'R2Score': reg_metrics.R2Score(),\n                }\n            )\n\n            confusion_matrix = None\n            roc = None\n            balanced_accuracy = None\n        else:\n            raise ValueError(f'Unknown task: {self.task}')\n\n        return metrics, confusion_matrix, roc, balanced_accuracy\n\n    def get_metrics_map(self, step_type: str, index=0) -&gt; dict[str, Any]:\n        if self.task == TaskTypes.REGRESSION:\n            metrics_map = {\n                'train': {\n                    'cm': None,\n                    'roc': None,\n                    'metrics': self.train_metrics,\n                    'balanced_accuracy': None,\n                },\n                'val': {\n                    'cm': None,\n                    'roc': None,\n                    'metrics': self.val_metrics_list[index],\n                    'balanced_accuracy': None,\n                },\n                'test': {\n                    'cm': None,\n                    'roc': None,\n                    'metrics': self.test_metrics_list[index],\n                    'balanced_accuracy': None,\n                },\n            }\n        else:\n            metrics_map = {\n                'train': {\n                    'cm': self.train_cm,\n                    'roc': self.train_roc,\n                    'metrics': self.train_metrics,\n                    'balanced_accuracy': self.train_balanced_accuracy,\n                },\n                'val': {\n                    'cm': self.val_cm_list[index],\n                    'roc': self.val_roc_list[index],\n                    'metrics': self.val_metrics_list[index],\n                    'balanced_accuracy': self.val_balanced_accuracy_list[index],\n                },\n                'test': {\n                    'cm': self.test_cm_list[index],\n                    'roc': self.test_roc_list[index],\n                    'metrics': self.test_metrics_list[index],\n                    'balanced_accuracy': self.test_balanced_accuracy_list[index],\n                },\n            }\n\n        return metrics_map[step_type]\n</code></pre>"},{"location":"reference/models/base_model/#models.base_model.BaseModel.configure_metrics","title":"<code>configure_metrics()</code>","text":"<p>Configures the metrics for the model.</p> Source code in <code>src/models/base_model.py</code> <pre><code>def configure_metrics(self):\n    \"\"\"Configures the metrics for the model.\"\"\"\n    if self.task == TaskTypes.BINARY_CLASSIFICATION:\n        logger.info('Using binary metrics')\n        metrics = torchmetrics.MetricCollection(\n            {\n                'AUROC': cls_metrics.BinaryAUROC(\n                    validate_args=self.validate_metrics,\n                    # thresholds=10,\n                ),\n                'F1Score': cls_metrics.BinaryF1Score(\n                    validate_args=self.validate_metrics,\n                ),\n                'Precision': cls_metrics.BinaryPrecision(\n                    validate_args=self.validate_metrics,\n                ),\n                'Recall': cls_metrics.BinaryRecall(\n                    validate_args=self.validate_metrics,\n                ),\n                'Accuracy': cls_metrics.BinaryAccuracy(\n                    validate_args=self.validate_metrics,\n                ),\n            }\n        )\n\n        confusion_matrix = cls_metrics.BinaryConfusionMatrix(\n            validate_args=self.validate_metrics\n        )\n\n        roc = cls_metrics.BinaryROC(\n            validate_args=self.validate_metrics,\n        )\n        # * Currently separate because expects preds or probs for each class which is not the case in binary case.\n        balanced_accuracy = cls_metrics.MulticlassAccuracy(\n            num_classes=self.num_classes,\n            average=self.average,\n            validate_args=self.validate_metrics,\n        )\n\n    elif self.task == TaskTypes.REGRESSION:\n        logger.info('Using regression metrics')\n        metrics = torchmetrics.MetricCollection(\n            {\n                'RMSE': reg_metrics.MeanSquaredError(squared=False),\n                'R2Score': reg_metrics.R2Score(),\n            }\n        )\n\n        confusion_matrix = None\n        roc = None\n        balanced_accuracy = None\n    else:\n        raise ValueError(f'Unknown task: {self.task}')\n\n    return metrics, confusion_matrix, roc, balanced_accuracy\n</code></pre>"},{"location":"reference/models/base_model/#models.base_model.BaseModel.format_confusion_matrix","title":"<code>format_confusion_matrix(cm)</code>","text":"<p>Formats a confusion matrix into a list of lists containing class names and their corresponding values.</p> <p>Parameters:</p> Name Type Description Default <code>cm</code> <code>Tensor</code> <p>The confusion matrix to format.</p> required <p>Returns:</p> Type Description <code>list[list]</code> <p>list[list]: A list of lists containing class names and their corresponding values.</p> Source code in <code>src/models/base_model.py</code> <pre><code>def format_confusion_matrix(self, cm: torch.Tensor) -&gt; list[list]:\n    \"\"\"\n    Formats a confusion matrix into a list of lists containing\n    class names and their corresponding values.\n\n    Args:\n        cm (torch.Tensor): The confusion matrix to format.\n\n    Returns:\n        list[list]: A list of lists containing class names and their corresponding values.\n    \"\"\"\n    class_names = self.class_names\n    cm_list = cm.tolist()\n    return [\n        [class_names[i], class_names[j], cm_list[i][j]]\n        for i, j in itertools.product(\n            range(self.num_classes), range(self.num_classes)\n        )\n    ]\n</code></pre>"},{"location":"reference/models/base_model/#models.base_model.BaseModel.log_confusion_matrix","title":"<code>log_confusion_matrix(cm_data, title)</code>","text":"<p>Logs a confusion matrix to the experiment logger.</p> <p>Parameters:</p> Name Type Description Default <code>cm_data</code> <code>list[list]</code> <p>A list of lists representing the confusion matrix.             Each inner list should contain the actual class name,             the predicted and the number of values.</p> required <code>title</code> <code>str</code> <p>The title to use for the confusion matrix plot.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>src/models/base_model.py</code> <pre><code>def log_confusion_matrix(self, cm_data: list[list], title: str) -&gt; None:\n    \"\"\"\n    Logs a confusion matrix to the experiment logger.\n\n    Args:\n        cm_data (list[list]): A list of lists representing the confusion matrix.\n                        Each inner list should contain the actual class name,\n                        the predicted and the number of values.\n        title (str): The title to use for the confusion matrix plot.\n\n    Returns:\n        None\n    \"\"\"\n    if isinstance(self.logger, WandbLogger):\n        wandb_logger = self.logger.experiment\n        fields = {\n            'Actual': 'Actual',\n            'Predicted': 'Predicted',\n            'nPredictions': 'nPredictions',\n        }\n\n        wandb_logger.log(\n            {\n                title: wandb.plot_table(\n                    'EyeRead/multi-run-confusion-matrix',\n                    wandb.Table(\n                        columns=['Actual', 'Predicted', 'nPredictions'],\n                        data=cm_data,\n                    ),\n                    fields,\n                    {'title': title},\n                ),\n            }\n        )\n    else:\n        warnings.warn('No wandb logger found, cannot log confusion matrix')\n</code></pre>"},{"location":"reference/models/base_model/#models.base_model.BaseModel.unpack_batch","title":"<code>unpack_batch(batch)</code>  <code>staticmethod</code>","text":"<p>Unpacks the batch into a namedtuple for dot access.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>list</code> <p>The batch to unpack containing: - batch[0] (dict): Dictionary with features including:     - fixation_features: Optional[torch.Tensor] - fixations feature vectors for each fixation.     - fixation_pads: Optional[torch.Tensor] - Feature vectors for fixations.     - scanpath: Optional[torch.Tensor] - A series of IA_IDs for each trial scanpath.     - scanpath_pads: Optional[torch.Tensor] - Feature vectors for IA_IDs in scanpath.     - paragraph_input_ids: Optional[torch.Tensor] - Input_ids of each tokenized paragraph.     - paragraph_input_masks: Optional[torch.Tensor] - Masks for paragraph input_ids.     - input_ids: Optional[torch.Tensor] - Input_ids of tokenized paragraphs.     - input_masks: Optional[torch.Tensor] - Masks for input_ids.     - answer_mappings: Optional[torch.Tensor] - Mappings associated with answers.     - inversions: Optional[torch.Tensor] - For each paragraph, IA_IDs associated with tokens.     - inversions_pads: Optional[torch.Tensor] - Pads for IA_IDs in inversions.     - eyes: Optional[torch.Tensor] - et_data_enriched feature vectors for each IA_ID. - batch[1]: Labels associated with the batch. - batch[2]: Keys for each item in the batch. - batch[3]: Columns that form a trial.</p> required <p>Returns:</p> Name Type Description <code>tuple</code> <code>tuple</code> <p>The unpacked batch with dot access (namedtuple).</p> Source code in <code>src/models/base_model.py</code> <pre><code>@staticmethod\ndef unpack_batch(batch: list) -&gt; tuple:\n    \"\"\"\n    Unpacks the batch into a namedtuple for dot access.\n\n    Args:\n        batch (list): The batch to unpack containing:\n            - batch[0] (dict): Dictionary with features including:\n                - fixation_features: Optional[torch.Tensor] - fixations feature vectors for each fixation.\n                - fixation_pads: Optional[torch.Tensor] - Feature vectors for fixations.\n                - scanpath: Optional[torch.Tensor] - A series of IA_IDs for each trial scanpath.\n                - scanpath_pads: Optional[torch.Tensor] - Feature vectors for IA_IDs in scanpath.\n                - paragraph_input_ids: Optional[torch.Tensor] - Input_ids of each tokenized paragraph.\n                - paragraph_input_masks: Optional[torch.Tensor] - Masks for paragraph input_ids.\n                - input_ids: Optional[torch.Tensor] - Input_ids of tokenized paragraphs.\n                - input_masks: Optional[torch.Tensor] - Masks for input_ids.\n                - answer_mappings: Optional[torch.Tensor] - Mappings associated with answers.\n                - inversions: Optional[torch.Tensor] - For each paragraph, IA_IDs associated with tokens.\n                - inversions_pads: Optional[torch.Tensor] - Pads for IA_IDs in inversions.\n                - eyes: Optional[torch.Tensor] - et_data_enriched feature vectors for each IA_ID.\n            - batch[1]: Labels associated with the batch.\n            - batch[2]: Keys for each item in the batch.\n            - batch[3]: Columns that form a trial.\n\n    Returns:\n        tuple: The unpacked batch with dot access (namedtuple).\n    \"\"\"\n\n    ExampleBatch = namedtuple(\n        'ExampleBatch',\n        list(batch[0].keys())\n        + ['labels', 'batch_item_keys', 'trial_groupby_columns'],\n    )\n    return ExampleBatch(\n        **dict(batch[0].items()),\n        labels=batch[1],\n        batch_item_keys=batch[2],\n        trial_groupby_columns=batch[3],\n    )\n</code></pre>"},{"location":"reference/models/base_model/#models.base_model.ModelFactory","title":"<code>ModelFactory</code>","text":"<p>A factory class to register and retrieve models.</p> Source code in <code>src/models/base_model.py</code> <pre><code>class ModelFactory:\n    \"\"\"A factory class to register and retrieve models.\"\"\"\n\n    models = {}\n\n    @classmethod\n    def add(cls, model: Type[BaseModel | BaseMLModel]) -&gt; None:\n        \"\"\"Register a model class.\"\"\"\n        cls.models[model.__name__] = model\n\n    @classmethod\n    def get(cls, model_name: str) -&gt; Type[BaseModel | BaseMLModel]:\n        \"\"\"Retrieve a model class by its name.\"\"\"\n        model = cls.models[model_name]\n        return model\n</code></pre>"},{"location":"reference/models/base_model/#models.base_model.ModelFactory.add","title":"<code>add(model)</code>  <code>classmethod</code>","text":"<p>Register a model class.</p> Source code in <code>src/models/base_model.py</code> <pre><code>@classmethod\ndef add(cls, model: Type[BaseModel | BaseMLModel]) -&gt; None:\n    \"\"\"Register a model class.\"\"\"\n    cls.models[model.__name__] = model\n</code></pre>"},{"location":"reference/models/base_model/#models.base_model.ModelFactory.get","title":"<code>get(model_name)</code>  <code>classmethod</code>","text":"<p>Retrieve a model class by its name.</p> Source code in <code>src/models/base_model.py</code> <pre><code>@classmethod\ndef get(cls, model_name: str) -&gt; Type[BaseModel | BaseMLModel]:\n    \"\"\"Retrieve a model class by its name.\"\"\"\n    model = cls.models[model_name]\n    return model\n</code></pre>"},{"location":"reference/models/base_model/#models.base_model.SharedBaseModel","title":"<code>SharedBaseModel</code>","text":"<p>Shared base class containing common initialization logic for both DL and ML models.</p> Source code in <code>src/models/base_model.py</code> <pre><code>class SharedBaseModel:\n    \"\"\"Shared base class containing common initialization logic for both DL and ML models.\"\"\"\n\n    def __init__(\n        self, model_args: Union[DLModelArgs, MLModelArgs], data_args: DataArgs\n    ):\n        \"\"\"Initialize shared attributes between BaseModel and BaseMLModel.\n\n        Args:\n            model_args: Model configuration arguments (either DL or ML)\n            data_args: Data configuration arguments\n        \"\"\"\n        self.use_eyes_only = model_args.use_eyes_only\n        self.use_fixation_report = model_args.use_fixation_report\n        self.class_names = list(data_args.class_names)\n        self.num_classes = len(self.class_names)\n        self.average: Literal['macro'] = 'macro'\n        self.validate_metrics: bool = False\n        self.prediction_mode: PredMode = data_args.task\n\n        # Determine task type based on number of classes\n        if self.num_classes == 1:\n            self.task: TaskTypes = TaskTypes.REGRESSION\n        elif self.num_classes == 2:\n            self.task: TaskTypes = TaskTypes.BINARY_CLASSIFICATION\n        elif self.num_classes &gt; 2:\n            raise NotImplementedError('Multi-class classification is not implemented.')\n\n        logger.info(f'Using {self.task=} metrics')\n\n        # Handle class weights\n        if model_args.use_class_weighted_loss:\n            self.class_weights = model_args.class_weights\n            logger.info(f'Using class weights: {self.class_weights}')\n        else:\n            self.class_weights = None\n            logger.info('Not using class weights')\n</code></pre>"},{"location":"reference/models/base_model/#models.base_model.SharedBaseModel.__init__","title":"<code>__init__(model_args, data_args)</code>","text":"<p>Initialize shared attributes between BaseModel and BaseMLModel.</p> <p>Parameters:</p> Name Type Description Default <code>model_args</code> <code>Union[DLModelArgs, MLModelArgs]</code> <p>Model configuration arguments (either DL or ML)</p> required <code>data_args</code> <code>DataArgs</code> <p>Data configuration arguments</p> required Source code in <code>src/models/base_model.py</code> <pre><code>def __init__(\n    self, model_args: Union[DLModelArgs, MLModelArgs], data_args: DataArgs\n):\n    \"\"\"Initialize shared attributes between BaseModel and BaseMLModel.\n\n    Args:\n        model_args: Model configuration arguments (either DL or ML)\n        data_args: Data configuration arguments\n    \"\"\"\n    self.use_eyes_only = model_args.use_eyes_only\n    self.use_fixation_report = model_args.use_fixation_report\n    self.class_names = list(data_args.class_names)\n    self.num_classes = len(self.class_names)\n    self.average: Literal['macro'] = 'macro'\n    self.validate_metrics: bool = False\n    self.prediction_mode: PredMode = data_args.task\n\n    # Determine task type based on number of classes\n    if self.num_classes == 1:\n        self.task: TaskTypes = TaskTypes.REGRESSION\n    elif self.num_classes == 2:\n        self.task: TaskTypes = TaskTypes.BINARY_CLASSIFICATION\n    elif self.num_classes &gt; 2:\n        raise NotImplementedError('Multi-class classification is not implemented.')\n\n    logger.info(f'Using {self.task=} metrics')\n\n    # Handle class weights\n    if model_args.use_class_weighted_loss:\n        self.class_weights = model_args.class_weights\n        logger.info(f'Using class weights: {self.class_weights}')\n    else:\n        self.class_weights = None\n        logger.info('Not using class weights')\n</code></pre>"},{"location":"reference/models/base_model/#models.base_model.register_model","title":"<code>register_model(model)</code>","text":"<p>Decorator to register a model class.</p> Source code in <code>src/models/base_model.py</code> <pre><code>def register_model(\n    model: Type[BaseModel | BaseMLModel],\n) -&gt; Type[BaseModel | BaseMLModel]:\n    \"\"\"Decorator to register a model class.\"\"\"\n    ModelFactory.add(model)\n    return model\n</code></pre>"},{"location":"reference/models/base_roberta/","title":"base_roberta","text":"<p>base_roberta.py - Base class for MAG and RoBERTeye models. See 1. On the Stability of Fine-tuning BERT: Misconceptions, Explanations, and Strong Baselines: https://www.semanticscholar.org/reader/8b9d77d5e52a70af37451d3db3d32781b83ea054 for parameters</p>"},{"location":"reference/models/base_roberta/#models.base_roberta.BaseMultiModalRoberta","title":"<code>BaseMultiModalRoberta</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Model for Multiple Choice Question Answering and question prediction tasks.</p> <p>Parameters:</p> Name Type Description Default <code>model_args</code> <code>Roberteye | MAG | PostFusion</code> <p>Model arguments.</p> required <code>trainer_args</code> <code>TrainerDL</code> <p>Trainer arguments.</p> required Source code in <code>src/models/base_roberta.py</code> <pre><code>class BaseMultiModalRoberta(BaseModel):\n    \"\"\"\n    Model for Multiple Choice Question Answering and question prediction tasks.\n\n    Args:\n        model_args (Roberteye | MAG | PostFusion): Model arguments.\n        trainer_args (TrainerDL): Trainer arguments.\n    \"\"\"\n\n    def __init__(\n        self,\n        model_args: RoberteyeArgs | MAG | PostFusion,\n        trainer_args: TrainerDL,\n        data_args: DataArgs,\n    ):\n        super().__init__(\n            model_args=model_args, trainer_args=trainer_args, data_args=data_args\n        )\n\n        self.model_args = model_args\n        self.preorder = model_args.preorder\n        self.warmup_proportion = model_args.warmup_proportion\n\n        self.train()\n        self.save_hyperparameters()\n\n    def forward(\n        self,\n        input_ids,\n        attention_mask,\n        labels,\n        gaze_features,\n        gaze_positions,\n        eye_token_type_ids=None,\n        **kwargs,\n    ) -&gt; torch.Tensor:\n        \"\"\"\n        Forward pass of the model.\n\n        Args:\n            input_ids (torch.Tensor): Input IDs.\n            attention_mask (torch.Tensor): Attention mask.\n            labels (torch.Tensor): Labels.\n            gaze_features (torch.Tensor): Gaze features.\n            gaze_positions (torch.Tensor): Gaze positions.\n            eye_token_type_ids (torch.Tensor, optional): Eye token type IDs. Defaults to None.\n\n        Returns:\n            torch.Tensor: Model output.\n        \"\"\"\n        return self.model(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            labels=labels,\n            gaze_features=gaze_features,\n            gaze_positions=gaze_positions,\n            eye_token_type_ids=eye_token_type_ids,\n            **kwargs,\n        )\n\n    def finalize_eye_data(\n        self, batch\n    ) -&gt; tuple[torch.Tensor | None, torch.Tensor | None, torch.Tensor]:\n        \"\"\"\n        Set the final gaze_features, gaze_positions, and attention_mask\n        based on whether we use fixation reports or not.\n\n        Args:\n            batch (BatchData): Batch data.\n            actual_needed_eye_padding_len (int | None): Actual needed eye padding length.\n\n        Returns:\n            tuple: Gaze features, gaze positions, and attention mask.\n        \"\"\"\n        if self.model_args.use_fixation_report:\n            assert batch.fixation_features is not None, (\n                'fixation_features must be present if using fixation_report'\n            )\n\n            gaze_positions = batch.grouped_inversions\n            gaze_features = batch.fixation_features\n\n            attention_mask = batch.input_masks\n\n        else:\n            # No fixation report =&gt; we must have eyes instead\n            assert batch.eyes is not None\n            assert batch.input_ids is not None\n\n            gaze_features = batch.eyes\n            gaze_positions = batch.grouped_inversions\n            attention_mask = batch.input_masks\n\n        # If we do NOT want to prepend_eye_features_to_text for ROBERTEYE_MODEL, set them to None\n        if (\n            not self.model_args.prepend_eye_features_to_text\n            and self.model_args.base_model_name == DLModelNames.ROBERTEYE_MODEL\n        ):\n            gaze_features = None\n            gaze_positions = None\n\n        return gaze_features, gaze_positions, attention_mask\n\n    def shared_step(\n        self, batch: list\n    ) -&gt; tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n        \"\"\"\n        Shared step for training, validation, and testing.\n\n        Args:\n            batch (list): Batch data.\n\n        Returns:\n            tuple: Ordered labels, loss, ordered logits, labels, and logits.\n        \"\"\"\n        # 1. Unpack the batch\n        batch_data = self.unpack_batch(batch)\n\n        # 2. Process different modes that affect input_masks/grouped_inversions\n        eye_token_type_ids = (\n            None  # if it will stay None, it will be created automatically later\n        )\n\n        # Safety checks\n        assert batch_data.input_masks is not None, 'input_masks cannot be None'\n        assert batch_data.grouped_inversions is not None, (\n            'grouped_inversions cannot be None'\n        )\n\n        # 3. Finalize eye/gaze data\n        gaze_features, gaze_positions, attention_mask = self.finalize_eye_data(\n            batch_data\n        )\n\n        labels = batch_data.labels\n\n        if self.task == TaskTypes.REGRESSION:\n            labels = labels.squeeze().float()\n\n        output = self(\n            input_ids=batch_data.input_ids,\n            attention_mask=attention_mask,\n            labels=labels,\n            gaze_features=gaze_features,\n            gaze_positions=gaze_positions,\n            output_hidden_states=True,\n            eye_token_type_ids=eye_token_type_ids,\n        )\n\n        logits = output.logits\n        if self.task == TaskTypes.REGRESSION:\n            logits = logits.squeeze()\n\n        loss = self.loss(logits, labels)\n\n        return labels, loss, logits\n\n    def configure_optimizers(self) -&gt; tuple[list, list]:\n        \"\"\"\n        Configure the optimizer and learning rate scheduler.\n\n        Returns:\n            tuple: Optimizer and learning rate scheduler.\n        \"\"\"\n        # Define the optimizer\n        assert self.warmup_proportion is not None\n\n        # Copied from bert\n        param_optimizer = list(self.named_parameters())\n\n        # hack to remove pooler, which is not used\n        # thus it produce None grad that break apex\n        param_optimizer = [n for n in param_optimizer if 'pooler' not in n[0]]\n\n        no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n        optimizer_grouped_parameters = [\n            {\n                'params': [\n                    p for n, p in param_optimizer if not any(nd in n for nd in no_decay)\n                ],\n                'weight_decay': 0.1,\n            },\n            {\n                'params': [\n                    p for n, p in param_optimizer if any(nd in n for nd in no_decay)\n                ],\n                'weight_decay': 0.0,\n            },\n        ]\n        optimizer = torch.optim.AdamW(\n            optimizer_grouped_parameters,\n            lr=self.learning_rate,\n            betas=(0.9, 0.98),\n            eps=1e-6,\n        )\n\n        stepping_batches = self.trainer.estimated_stepping_batches\n        scheduler = get_linear_schedule_with_warmup(\n            optimizer,\n            num_warmup_steps=int(stepping_batches * self.warmup_proportion),\n            num_training_steps=stepping_batches,\n        )\n        return [optimizer], [{'scheduler': scheduler, 'interval': 'step'}]\n</code></pre>"},{"location":"reference/models/base_roberta/#models.base_roberta.BaseMultiModalRoberta.configure_optimizers","title":"<code>configure_optimizers()</code>","text":"<p>Configure the optimizer and learning rate scheduler.</p> <p>Returns:</p> Name Type Description <code>tuple</code> <code>tuple[list, list]</code> <p>Optimizer and learning rate scheduler.</p> Source code in <code>src/models/base_roberta.py</code> <pre><code>def configure_optimizers(self) -&gt; tuple[list, list]:\n    \"\"\"\n    Configure the optimizer and learning rate scheduler.\n\n    Returns:\n        tuple: Optimizer and learning rate scheduler.\n    \"\"\"\n    # Define the optimizer\n    assert self.warmup_proportion is not None\n\n    # Copied from bert\n    param_optimizer = list(self.named_parameters())\n\n    # hack to remove pooler, which is not used\n    # thus it produce None grad that break apex\n    param_optimizer = [n for n in param_optimizer if 'pooler' not in n[0]]\n\n    no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n    optimizer_grouped_parameters = [\n        {\n            'params': [\n                p for n, p in param_optimizer if not any(nd in n for nd in no_decay)\n            ],\n            'weight_decay': 0.1,\n        },\n        {\n            'params': [\n                p for n, p in param_optimizer if any(nd in n for nd in no_decay)\n            ],\n            'weight_decay': 0.0,\n        },\n    ]\n    optimizer = torch.optim.AdamW(\n        optimizer_grouped_parameters,\n        lr=self.learning_rate,\n        betas=(0.9, 0.98),\n        eps=1e-6,\n    )\n\n    stepping_batches = self.trainer.estimated_stepping_batches\n    scheduler = get_linear_schedule_with_warmup(\n        optimizer,\n        num_warmup_steps=int(stepping_batches * self.warmup_proportion),\n        num_training_steps=stepping_batches,\n    )\n    return [optimizer], [{'scheduler': scheduler, 'interval': 'step'}]\n</code></pre>"},{"location":"reference/models/base_roberta/#models.base_roberta.BaseMultiModalRoberta.finalize_eye_data","title":"<code>finalize_eye_data(batch)</code>","text":"<p>Set the final gaze_features, gaze_positions, and attention_mask based on whether we use fixation reports or not.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>BatchData</code> <p>Batch data.</p> required <code>actual_needed_eye_padding_len</code> <code>int | None</code> <p>Actual needed eye padding length.</p> required <p>Returns:</p> Name Type Description <code>tuple</code> <code>tuple[Tensor | None, Tensor | None, Tensor]</code> <p>Gaze features, gaze positions, and attention mask.</p> Source code in <code>src/models/base_roberta.py</code> <pre><code>def finalize_eye_data(\n    self, batch\n) -&gt; tuple[torch.Tensor | None, torch.Tensor | None, torch.Tensor]:\n    \"\"\"\n    Set the final gaze_features, gaze_positions, and attention_mask\n    based on whether we use fixation reports or not.\n\n    Args:\n        batch (BatchData): Batch data.\n        actual_needed_eye_padding_len (int | None): Actual needed eye padding length.\n\n    Returns:\n        tuple: Gaze features, gaze positions, and attention mask.\n    \"\"\"\n    if self.model_args.use_fixation_report:\n        assert batch.fixation_features is not None, (\n            'fixation_features must be present if using fixation_report'\n        )\n\n        gaze_positions = batch.grouped_inversions\n        gaze_features = batch.fixation_features\n\n        attention_mask = batch.input_masks\n\n    else:\n        # No fixation report =&gt; we must have eyes instead\n        assert batch.eyes is not None\n        assert batch.input_ids is not None\n\n        gaze_features = batch.eyes\n        gaze_positions = batch.grouped_inversions\n        attention_mask = batch.input_masks\n\n    # If we do NOT want to prepend_eye_features_to_text for ROBERTEYE_MODEL, set them to None\n    if (\n        not self.model_args.prepend_eye_features_to_text\n        and self.model_args.base_model_name == DLModelNames.ROBERTEYE_MODEL\n    ):\n        gaze_features = None\n        gaze_positions = None\n\n    return gaze_features, gaze_positions, attention_mask\n</code></pre>"},{"location":"reference/models/base_roberta/#models.base_roberta.BaseMultiModalRoberta.forward","title":"<code>forward(input_ids, attention_mask, labels, gaze_features, gaze_positions, eye_token_type_ids=None, **kwargs)</code>","text":"<p>Forward pass of the model.</p> <p>Parameters:</p> Name Type Description Default <code>input_ids</code> <code>Tensor</code> <p>Input IDs.</p> required <code>attention_mask</code> <code>Tensor</code> <p>Attention mask.</p> required <code>labels</code> <code>Tensor</code> <p>Labels.</p> required <code>gaze_features</code> <code>Tensor</code> <p>Gaze features.</p> required <code>gaze_positions</code> <code>Tensor</code> <p>Gaze positions.</p> required <code>eye_token_type_ids</code> <code>Tensor</code> <p>Eye token type IDs. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: Model output.</p> Source code in <code>src/models/base_roberta.py</code> <pre><code>def forward(\n    self,\n    input_ids,\n    attention_mask,\n    labels,\n    gaze_features,\n    gaze_positions,\n    eye_token_type_ids=None,\n    **kwargs,\n) -&gt; torch.Tensor:\n    \"\"\"\n    Forward pass of the model.\n\n    Args:\n        input_ids (torch.Tensor): Input IDs.\n        attention_mask (torch.Tensor): Attention mask.\n        labels (torch.Tensor): Labels.\n        gaze_features (torch.Tensor): Gaze features.\n        gaze_positions (torch.Tensor): Gaze positions.\n        eye_token_type_ids (torch.Tensor, optional): Eye token type IDs. Defaults to None.\n\n    Returns:\n        torch.Tensor: Model output.\n    \"\"\"\n    return self.model(\n        input_ids=input_ids,\n        attention_mask=attention_mask,\n        labels=labels,\n        gaze_features=gaze_features,\n        gaze_positions=gaze_positions,\n        eye_token_type_ids=eye_token_type_ids,\n        **kwargs,\n    )\n</code></pre>"},{"location":"reference/models/base_roberta/#models.base_roberta.BaseMultiModalRoberta.shared_step","title":"<code>shared_step(batch)</code>","text":"<p>Shared step for training, validation, and testing.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>list</code> <p>Batch data.</p> required <p>Returns:</p> Name Type Description <code>tuple</code> <code>tuple[Tensor, Tensor, Tensor]</code> <p>Ordered labels, loss, ordered logits, labels, and logits.</p> Source code in <code>src/models/base_roberta.py</code> <pre><code>def shared_step(\n    self, batch: list\n) -&gt; tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Shared step for training, validation, and testing.\n\n    Args:\n        batch (list): Batch data.\n\n    Returns:\n        tuple: Ordered labels, loss, ordered logits, labels, and logits.\n    \"\"\"\n    # 1. Unpack the batch\n    batch_data = self.unpack_batch(batch)\n\n    # 2. Process different modes that affect input_masks/grouped_inversions\n    eye_token_type_ids = (\n        None  # if it will stay None, it will be created automatically later\n    )\n\n    # Safety checks\n    assert batch_data.input_masks is not None, 'input_masks cannot be None'\n    assert batch_data.grouped_inversions is not None, (\n        'grouped_inversions cannot be None'\n    )\n\n    # 3. Finalize eye/gaze data\n    gaze_features, gaze_positions, attention_mask = self.finalize_eye_data(\n        batch_data\n    )\n\n    labels = batch_data.labels\n\n    if self.task == TaskTypes.REGRESSION:\n        labels = labels.squeeze().float()\n\n    output = self(\n        input_ids=batch_data.input_ids,\n        attention_mask=attention_mask,\n        labels=labels,\n        gaze_features=gaze_features,\n        gaze_positions=gaze_positions,\n        output_hidden_states=True,\n        eye_token_type_ids=eye_token_type_ids,\n    )\n\n    logits = output.logits\n    if self.task == TaskTypes.REGRESSION:\n        logits = logits.squeeze()\n\n    loss = self.loss(logits, labels)\n\n    return labels, loss, logits\n</code></pre>"},{"location":"reference/models/beyelstm_model/","title":"beyelstm_model","text":"<p>Beye LSTM baseline model. Based on https://github.com/aeye-lab/etra-reading-comprehension/blob/main/nn/model.py</p>"},{"location":"reference/models/beyelstm_model/#models.beyelstm_model.BEyeLSTMModel","title":"<code>BEyeLSTMModel</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Beye model.</p> Source code in <code>src/models/beyelstm_model.py</code> <pre><code>@register_model\nclass BEyeLSTMModel(BaseModel):\n    \"\"\"Beye model.\"\"\"\n\n    def __init__(\n        self,\n        model_args: BEyeLSTMArgs,\n        trainer_args: TrainerDL,\n        data_args: DataArgs,\n    ) -&gt; None:\n        super().__init__(\n            model_args=model_args, trainer_args=trainer_args, data_args=data_args\n        )\n        self.preorder = False\n        self.model_args = model_args\n        self.pos_block = LSTMBlock(model_args, num_embed=model_args.num_pos)\n        self.content_block = LSTMBlock(model_args, num_embed=model_args.num_content)\n        self.fixations_block = LSTMBlock(model_args, input_dim=model_args.fixations_dim)\n\n        self.gsf_block = nn.Sequential(\n            nn.Dropout(p=model_args.dropout_rate),\n            nn.Linear(\n                in_features=model_args.gsf_dim, out_features=model_args.gsf_out_dim\n            ),\n            nn.ReLU(),\n        )\n        fc1_in_features = model_args.lstm_block_fc2_out_dim * 3 + model_args.gsf_out_dim\n        self.fc1 = nn.Linear(\n            in_features=fc1_in_features,\n            out_features=model_args.after_cat_fc_hidden_dim,\n        )\n        self.fc2 = nn.Linear(\n            in_features=model_args.after_cat_fc_hidden_dim,\n            out_features=self.num_classes,\n        )\n\n        print(f'##### Preorder labels: {self.preorder} #####')\n\n        self.train()\n        self.save_hyperparameters()\n\n    def forward(  # type: ignore\n        self,\n        x_pos: torch.Tensor,\n        x_content: torch.Tensor,\n        x_gsf: torch.Tensor,\n        x_fixations: torch.Tensor,\n        seq_lengths: torch.Tensor | None = None,\n    ) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"Forward pass for NNModel.\n\n        Args:\n            x_pos (torch.Tensor): Position tensor.\n            x_content (torch.Tensor): Content tensor.\n            x_gsf (torch.Tensor): GSF tensor.\n            x_fixations (torch.Tensor): Fixations tensor (batch size x MAX_SCANPATH_LEN x 4).\n                                        Padded with 0s\n            seq_lengths (torch.Tensor): Length of scanpath for each trial.\n\n        Returns:\n            torch.Tensor: Output tensor.\n        \"\"\"\n        concat_list = []\n        concat_list.append(self.pos_block(x_pos, seq_lengths=seq_lengths))\n        concat_list.append(self.content_block(x_content, seq_lengths=seq_lengths))\n        concat_list.append(self.gsf_block(x_gsf.squeeze(1)))\n        concat_list.append(self.fixations_block(x_fixations, seq_lengths=seq_lengths))\n        trial_embd = torch.cat(concat_list, dim=1)\n        x = F.relu(self.fc1(trial_embd))\n        x = self.fc2(x)\n        return x, trial_embd\n\n    def shared_step(\n        self,\n        batch: list,\n    ) -&gt; tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n        batch_data = self.unpack_batch(batch)\n        assert batch_data.fixation_features is not None, 'eyes_tensor not in batch_dict'\n        assert batch_data.scanpath_pads is not None, 'scanpath_pads not in batch_dict'\n        labels = batch_data.labels\n\n        shortest_scanpath_pad = batch_data.scanpath_pads.min()\n        longest_batch_scanpath: int = int(\n            self.max_scanpath_length - shortest_scanpath_pad\n        )\n\n        fixation_features = batch_data.fixation_features[\n            ..., :longest_batch_scanpath, :\n        ]\n        scanpath_lengths = (\n            batch_data.fixation_features.shape[1] - batch_data.scanpath_pads\n        )\n        logits, trial_embd = self(\n            x_fixations=fixation_features[..., :4],\n            x_content=fixation_features[..., -2].int(),\n            x_pos=fixation_features[..., -1].int(),\n            x_gsf=batch_data.trial_level_features,\n            seq_lengths=scanpath_lengths,\n        )\n\n        if logits.ndim == 1:\n            logits = logits.unsqueeze(0)\n        loss = self.loss(logits, labels)\n\n        return labels, loss, logits.squeeze()\n</code></pre>"},{"location":"reference/models/beyelstm_model/#models.beyelstm_model.BEyeLSTMModel.forward","title":"<code>forward(x_pos, x_content, x_gsf, x_fixations, seq_lengths=None)</code>","text":"<p>Forward pass for NNModel.</p> <p>Parameters:</p> Name Type Description Default <code>x_pos</code> <code>Tensor</code> <p>Position tensor.</p> required <code>x_content</code> <code>Tensor</code> <p>Content tensor.</p> required <code>x_gsf</code> <code>Tensor</code> <p>GSF tensor.</p> required <code>x_fixations</code> <code>Tensor</code> <p>Fixations tensor (batch size x MAX_SCANPATH_LEN x 4).                         Padded with 0s</p> required <code>seq_lengths</code> <code>Tensor</code> <p>Length of scanpath for each trial.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tuple[Tensor, Tensor]</code> <p>torch.Tensor: Output tensor.</p> Source code in <code>src/models/beyelstm_model.py</code> <pre><code>def forward(  # type: ignore\n    self,\n    x_pos: torch.Tensor,\n    x_content: torch.Tensor,\n    x_gsf: torch.Tensor,\n    x_fixations: torch.Tensor,\n    seq_lengths: torch.Tensor | None = None,\n) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"Forward pass for NNModel.\n\n    Args:\n        x_pos (torch.Tensor): Position tensor.\n        x_content (torch.Tensor): Content tensor.\n        x_gsf (torch.Tensor): GSF tensor.\n        x_fixations (torch.Tensor): Fixations tensor (batch size x MAX_SCANPATH_LEN x 4).\n                                    Padded with 0s\n        seq_lengths (torch.Tensor): Length of scanpath for each trial.\n\n    Returns:\n        torch.Tensor: Output tensor.\n    \"\"\"\n    concat_list = []\n    concat_list.append(self.pos_block(x_pos, seq_lengths=seq_lengths))\n    concat_list.append(self.content_block(x_content, seq_lengths=seq_lengths))\n    concat_list.append(self.gsf_block(x_gsf.squeeze(1)))\n    concat_list.append(self.fixations_block(x_fixations, seq_lengths=seq_lengths))\n    trial_embd = torch.cat(concat_list, dim=1)\n    x = F.relu(self.fc1(trial_embd))\n    x = self.fc2(x)\n    return x, trial_embd\n</code></pre>"},{"location":"reference/models/beyelstm_model/#models.beyelstm_model.LSTMBlock","title":"<code>LSTMBlock</code>","text":"<p>               Bases: <code>Module</code></p> <p>LSTM block for the Beye model.</p> Source code in <code>src/models/beyelstm_model.py</code> <pre><code>class LSTMBlock(nn.Module):\n    \"\"\"LSTM block for the Beye model.\"\"\"\n\n    def __init__(\n        self,\n        model_args: BEyeLSTMArgs,\n        input_dim: int | None = None,\n        num_embed: int | None = None,\n    ) -&gt; None:\n        \"\"\"Initialize LSTMBlock.\n\n        Args:\n            model_args (BEyeLSTMArgs): Model parameters.\n            input_dim (int | None, optional): Input dimension. Defaults to None.\n            num_embed (int | None, optional): Embedding dimension. Defaults to None.\n        \"\"\"\n        super().__init__()\n        assert (input_dim is None) != (num_embed is None), (\n            'input_dim and num_embeddings cannot both be None or not None.'\n        )\n        self.num_embeddings = num_embed  # for universal_pos and Content\n        if num_embed:\n            self.embedding = nn.Embedding(num_embed, model_args.embedding_dim)\n            lstm_input_dim = model_args.embedding_dim\n        else:  # for Fixations\n            lstm_input_dim = input_dim\n\n        self.lstm = nn.LSTM(\n            lstm_input_dim,\n            model_args.hidden_dim,\n            bidirectional=True,\n            batch_first=True,\n        )\n        self.dropout = nn.Dropout(model_args.dropout_rate)\n        self.fc1 = nn.Linear(\n            2 * model_args.hidden_dim, model_args.lstm_block_fc1_out_dim\n        )\n        self.fc2 = nn.Linear(\n            model_args.lstm_block_fc1_out_dim, model_args.lstm_block_fc2_out_dim\n        )\n        self.relu = nn.ReLU()\n\n    def forward(\n        self, x: torch.Tensor, seq_lengths: torch.Tensor | None = None\n    ) -&gt; torch.Tensor:\n        \"\"\"Forward pass for LSTMBlock.\n\n        Args:\n            seq_lengths (torch.Tensor | None): Length of scanpath for each trial. Defaults to None.\n            x (torch.Tensor): Input tensor.\n\n        Returns:\n            torch.Tensor: Output tensor.\n        \"\"\"\n        if self.num_embeddings:\n            x = self.embedding(x)\n\n        if seq_lengths is not None:\n            sorted_lengths, indices = torch.sort(seq_lengths, descending=True)\n            x = x[indices]\n            # Pass the entire sequence through the LSTM layer\n            packed_x = nn.utils.rnn.pack_padded_sequence(\n                input=x,\n                lengths=sorted_lengths.to('cpu'),\n                batch_first=True,\n                enforce_sorted=True,\n            )\n            assert not torch.isnan(packed_x.data).any()\n\n            unused_packed_output, (ht, unused_ct) = self.lstm(packed_x)\n\n            # from dimension (2, batch_size, hidden_dim) to (batch_size, 2*hidden_dim)\n            x = torch.cat((ht[0], ht[1]), dim=1)\n            x = x[torch.argsort(indices)]\n        else:\n            unused_output, (h, unused_c) = self.lstm(x)\n            h_concat = torch.cat((h[0], h[1]), dim=1)\n            x = h_concat\n\n        x = self.relu(self.fc1(x))\n        x = self.dropout(x)\n        x = self.relu(self.fc2(x))\n        return x\n</code></pre>"},{"location":"reference/models/beyelstm_model/#models.beyelstm_model.LSTMBlock.__init__","title":"<code>__init__(model_args, input_dim=None, num_embed=None)</code>","text":"<p>Initialize LSTMBlock.</p> <p>Parameters:</p> Name Type Description Default <code>model_args</code> <code>BEyeLSTMArgs</code> <p>Model parameters.</p> required <code>input_dim</code> <code>int | None</code> <p>Input dimension. Defaults to None.</p> <code>None</code> <code>num_embed</code> <code>int | None</code> <p>Embedding dimension. Defaults to None.</p> <code>None</code> Source code in <code>src/models/beyelstm_model.py</code> <pre><code>def __init__(\n    self,\n    model_args: BEyeLSTMArgs,\n    input_dim: int | None = None,\n    num_embed: int | None = None,\n) -&gt; None:\n    \"\"\"Initialize LSTMBlock.\n\n    Args:\n        model_args (BEyeLSTMArgs): Model parameters.\n        input_dim (int | None, optional): Input dimension. Defaults to None.\n        num_embed (int | None, optional): Embedding dimension. Defaults to None.\n    \"\"\"\n    super().__init__()\n    assert (input_dim is None) != (num_embed is None), (\n        'input_dim and num_embeddings cannot both be None or not None.'\n    )\n    self.num_embeddings = num_embed  # for universal_pos and Content\n    if num_embed:\n        self.embedding = nn.Embedding(num_embed, model_args.embedding_dim)\n        lstm_input_dim = model_args.embedding_dim\n    else:  # for Fixations\n        lstm_input_dim = input_dim\n\n    self.lstm = nn.LSTM(\n        lstm_input_dim,\n        model_args.hidden_dim,\n        bidirectional=True,\n        batch_first=True,\n    )\n    self.dropout = nn.Dropout(model_args.dropout_rate)\n    self.fc1 = nn.Linear(\n        2 * model_args.hidden_dim, model_args.lstm_block_fc1_out_dim\n    )\n    self.fc2 = nn.Linear(\n        model_args.lstm_block_fc1_out_dim, model_args.lstm_block_fc2_out_dim\n    )\n    self.relu = nn.ReLU()\n</code></pre>"},{"location":"reference/models/beyelstm_model/#models.beyelstm_model.LSTMBlock.forward","title":"<code>forward(x, seq_lengths=None)</code>","text":"<p>Forward pass for LSTMBlock.</p> <p>Parameters:</p> Name Type Description Default <code>seq_lengths</code> <code>Tensor | None</code> <p>Length of scanpath for each trial. Defaults to None.</p> <code>None</code> <code>x</code> <code>Tensor</code> <p>Input tensor.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: Output tensor.</p> Source code in <code>src/models/beyelstm_model.py</code> <pre><code>def forward(\n    self, x: torch.Tensor, seq_lengths: torch.Tensor | None = None\n) -&gt; torch.Tensor:\n    \"\"\"Forward pass for LSTMBlock.\n\n    Args:\n        seq_lengths (torch.Tensor | None): Length of scanpath for each trial. Defaults to None.\n        x (torch.Tensor): Input tensor.\n\n    Returns:\n        torch.Tensor: Output tensor.\n    \"\"\"\n    if self.num_embeddings:\n        x = self.embedding(x)\n\n    if seq_lengths is not None:\n        sorted_lengths, indices = torch.sort(seq_lengths, descending=True)\n        x = x[indices]\n        # Pass the entire sequence through the LSTM layer\n        packed_x = nn.utils.rnn.pack_padded_sequence(\n            input=x,\n            lengths=sorted_lengths.to('cpu'),\n            batch_first=True,\n            enforce_sorted=True,\n        )\n        assert not torch.isnan(packed_x.data).any()\n\n        unused_packed_output, (ht, unused_ct) = self.lstm(packed_x)\n\n        # from dimension (2, batch_size, hidden_dim) to (batch_size, 2*hidden_dim)\n        x = torch.cat((ht[0], ht[1]), dim=1)\n        x = x[torch.argsort(indices)]\n    else:\n        unused_output, (h, unused_c) = self.lstm(x)\n        h_concat = torch.cat((h[0], h[1]), dim=1)\n        x = h_concat\n\n    x = self.relu(self.fc1(x))\n    x = self.dropout(x)\n    x = self.relu(self.fc2(x))\n    return x\n</code></pre>"},{"location":"reference/models/mag_model/","title":"mag_model","text":"<p>This module contains the MAG module and the MAGRobertaModel class.</p>"},{"location":"reference/models/mag_model/#models.mag_model.MAGModel","title":"<code>MAGModel</code>","text":"<p>               Bases: <code>BaseMultiModalRoberta</code></p> <p>Model for Multiple Choice Question Answering and question prediction tasks.</p> Source code in <code>src/models/mag_model.py</code> <pre><code>@register_model\nclass MAGModel(BaseMultiModalRoberta):\n    \"\"\"\n    Model for Multiple Choice Question Answering and question prediction tasks.\n    \"\"\"\n\n    def __init__(\n        self,\n        model_args: MAG,\n        trainer_args: TrainerDL,\n        data_args: DataArgs,\n    ) -&gt; None:\n        super().__init__(\n            model_args=model_args, trainer_args=trainer_args, data_args=data_args\n        )\n\n        eyes_projection_input_dim = model_args.eyes_dim\n\n        self.multimodal_config = MultimodalConfig(\n            dropout_prob=model_args.mag_dropout,\n            beta_shift=model_args.mag_beta_shift,\n            text_dim=model_args.text_dim,\n            eyes_dim=eyes_projection_input_dim,\n            mag_injection_index=model_args.mag_injection_index,\n        )\n        print(\n            f'Injecting MAG at layer index {self.multimodal_config.mag_injection_index}'\n        )\n        if (\n            self.prediction_mode\n            in BINARY_PARAGRAPH_ONLY_TASKS\n            + BINARY_P_AND_Q_TASKS\n            + REGRESSION_PARAGRAPH_ONLY_TASKS\n        ):\n            if data_args.is_english:\n                self.model = MAGRobertaForSequenceClassification.from_pretrained(\n                    model_args.backbone,\n                    num_labels=self.num_classes,\n                    multimodal_config=self.multimodal_config,\n                )\n            else:\n                self.model = XLMMAGRobertaForSequenceClassification.from_pretrained(\n                    model_args.backbone,\n                    num_labels=self.num_classes,\n                    multimodal_config=self.multimodal_config,\n                )\n        else:\n            raise ValueError(f'Untested prediction mode: {self.prediction_mode}.')\n\n        if model_args.freeze:\n            # Freeze all model parameters except specific ones\n            for name, param in self.named_parameters():\n                if name.startswith('model.roberta.encoder.mag') or name.startswith(\n                    'model.classifier'\n                ):\n                    param.requires_grad = True\n                else:\n                    param.requires_grad = False\n\n        self.train()\n        self.save_hyperparameters()\n</code></pre>"},{"location":"reference/models/mag_model/#models.mag_model.MAGModule","title":"<code>MAGModule</code>","text":"<p>               Bases: <code>Module</code></p> <p>This class implements the Multimodal Attention Gate (MAG) module.</p> Source code in <code>src/models/mag_model.py</code> <pre><code>class MAGModule(nn.Module):\n    \"\"\"\n    This class implements the Multimodal Attention Gate (MAG) module.\n    \"\"\"\n\n    # Based on https://github.com/WasifurRahman/BERT_multimodal_transformer/blob/master/modeling.py\n    def __init__(self, hidden_size, beta_shift, dropout_prob, text_dim, eyes_dim):\n        super().__init__()\n        print(\n            f'Initializing MAG with beta_shift:{beta_shift} hidden_prob:{dropout_prob}'\n        )\n        self.w_hv = nn.Linear(eyes_dim + text_dim, text_dim)\n        self.w_v = nn.Linear(eyes_dim, text_dim)\n        self.beta_shift = beta_shift\n\n        self.layer_norm = nn.LayerNorm(hidden_size)\n        self.dropout = nn.Dropout(dropout_prob)\n        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n    def forward(self, text_embedding, gaze_features):\n        # text_embedding: torch.Size([num_classes X batch_size, max_len, embed_dim])\n        # eyes: torch.Size([num_classes X batch_size, max_len, num_features])\n        eps = 1e-6\n\n        # weight_v: torch.Size([num_classes X batch_size, max_len, embed_dim])\n        weight_v = F.relu(self.w_hv(torch.cat((gaze_features, text_embedding), dim=-1)))\n\n        h_m = weight_v * self.w_v(gaze_features)\n        em_norm = text_embedding.norm(2, dim=-1)\n        hm_norm = h_m.norm(2, dim=-1)\n\n        hm_norm_ones = torch.ones(hm_norm.shape, requires_grad=True).type_as(hm_norm)\n        hm_norm = torch.where(hm_norm == 0, hm_norm_ones, hm_norm)\n\n        threshhold = (em_norm / (hm_norm + eps)) * self.beta_shift\n\n        ones = torch.ones(threshhold.shape, requires_grad=True).type_as(threshhold)\n        alpha = torch.min(threshhold, ones)\n        alpha = alpha.unsqueeze(dim=-1)\n\n        acoustic_vis_embedding = alpha * h_m\n\n        zero_entries = (gaze_features == 0).all(dim=2)\n        acoustic_vis_embedding[zero_entries] = 0\n\n        return self.dropout(self.layer_norm(acoustic_vis_embedding + text_embedding))\n</code></pre>"},{"location":"reference/models/mag_model/#models.mag_model.MAGRobertaForSequenceClassification","title":"<code>MAGRobertaForSequenceClassification</code>","text":"<p>               Bases: <code>RobertaForSequenceClassification</code></p> <p>This class is a modified version of the RobertaForSequenceClassification class from the transformers library. It adds the MAG module to the forward pass.</p> <p>Copied from transformers.models.roberta.modeling_roberta.RobertaForSequenceClassification</p> Source code in <code>src/models/mag_model.py</code> <pre><code>class MAGRobertaForSequenceClassification(RobertaForSequenceClassification):\n    \"\"\"\n    This class is a modified version of the RobertaForSequenceClassification class\n    from the transformers library.\n    It adds the MAG module to the forward pass.\n\n    Copied from transformers.models.roberta.modeling_roberta.RobertaForSequenceClassification\n    \"\"\"\n\n    _keys_to_ignore_on_load_missing = [r'position_ids']\n\n    def __init__(self, config, multimodal_config):\n        super().__init__(config)\n        self.num_labels = config.num_labels\n        self.config = config\n\n        self.roberta = MAGRobertaModel(\n            config, multimodal_config, add_pooling_layer=False\n        )\n        self.classifier = RobertaClassificationHead(config)\n\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    def forward(\n        self,\n        input_ids: Optional[torch.Tensor] = None,\n        gaze_features: Optional[torch.Tensor] = None,\n        gaze_positions: Optional[torch.Tensor] = None,\n        attention_mask: torch.Tensor | None = None,\n        token_type_ids: Optional[torch.LongTensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        head_mask: Optional[torch.FloatTensor] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        labels: Optional[torch.Tensor] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        eye_token_type_ids: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n    ) -&gt; Union[tuple[torch.Tensor], SequenceClassifierOutput]:\n        r\"\"\"\n        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n            Labels for computing the sequence classification/regression loss.\n            Indices should be in `[0, ..., config.num_labels - 1]`.\n            If `config.num_labels == 1` a regression loss is computed (Mean-Square loss),\n            If `config.num_labels &gt; 1` a classification loss is computed (Cross-Entropy).\n        \"\"\"\n        return_dict = (\n            return_dict if return_dict is not None else self.config.use_return_dict\n        )\n\n        outputs = self.roberta(\n            input_ids=input_ids,\n            gaze_features=gaze_features,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n            position_ids=position_ids,\n            head_mask=head_mask,\n            inputs_embeds=inputs_embeds,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n        sequence_output = outputs[0]\n        logits = self.classifier(sequence_output)\n\n        loss = None\n        if labels is not None:\n            # move labels to correct device to enable model parallelism\n            labels = labels.to(logits.device)  # type: ignore\n            if self.config.problem_type is None:\n                if self.num_labels == 1:\n                    self.config.problem_type = 'regression'\n                elif self.num_labels &gt; 1 and labels.dtype in (torch.long, torch.int):  # type: ignore\n                    self.config.problem_type = 'single_label_classification'\n                else:\n                    self.config.problem_type = 'multi_label_classification'\n\n            if self.config.problem_type == 'regression':\n                loss_fct = MSELoss()\n                if self.num_labels == 1:\n                    loss = loss_fct(logits.squeeze(), labels.squeeze())  #  type: ignore\n                else:\n                    loss = loss_fct(logits, labels)\n            elif self.config.problem_type == 'single_label_classification':\n                loss_fct = CrossEntropyLoss()\n                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))  # type: ignore\n            elif self.config.problem_type == 'multi_label_classification':\n                loss_fct = BCEWithLogitsLoss()\n                loss = loss_fct(logits, labels)\n\n        if not return_dict:\n            output = (logits,) + outputs[2:]\n            return ((loss,) + output) if loss is not None else output\n\n        return SequenceClassifierOutput(\n            loss=loss,\n            logits=logits,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )\n</code></pre>"},{"location":"reference/models/mag_model/#models.mag_model.MAGRobertaForSequenceClassification.forward","title":"<code>forward(input_ids=None, gaze_features=None, gaze_positions=None, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None, inputs_embeds=None, labels=None, output_attentions=None, output_hidden_states=None, eye_token_type_ids=None, return_dict=None)</code>","text":"<p>labels (<code>torch.LongTensor</code> of shape <code>(batch_size,)</code>, optional):     Labels for computing the sequence classification/regression loss.     Indices should be in <code>[0, ..., config.num_labels - 1]</code>.     If <code>config.num_labels == 1</code> a regression loss is computed (Mean-Square loss),     If <code>config.num_labels &gt; 1</code> a classification loss is computed (Cross-Entropy).</p> Source code in <code>src/models/mag_model.py</code> <pre><code>def forward(\n    self,\n    input_ids: Optional[torch.Tensor] = None,\n    gaze_features: Optional[torch.Tensor] = None,\n    gaze_positions: Optional[torch.Tensor] = None,\n    attention_mask: torch.Tensor | None = None,\n    token_type_ids: Optional[torch.LongTensor] = None,\n    position_ids: Optional[torch.LongTensor] = None,\n    head_mask: Optional[torch.FloatTensor] = None,\n    inputs_embeds: Optional[torch.FloatTensor] = None,\n    labels: Optional[torch.Tensor] = None,\n    output_attentions: Optional[bool] = None,\n    output_hidden_states: Optional[bool] = None,\n    eye_token_type_ids: Optional[bool] = None,\n    return_dict: Optional[bool] = None,\n) -&gt; Union[tuple[torch.Tensor], SequenceClassifierOutput]:\n    r\"\"\"\n    labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n        Labels for computing the sequence classification/regression loss.\n        Indices should be in `[0, ..., config.num_labels - 1]`.\n        If `config.num_labels == 1` a regression loss is computed (Mean-Square loss),\n        If `config.num_labels &gt; 1` a classification loss is computed (Cross-Entropy).\n    \"\"\"\n    return_dict = (\n        return_dict if return_dict is not None else self.config.use_return_dict\n    )\n\n    outputs = self.roberta(\n        input_ids=input_ids,\n        gaze_features=gaze_features,\n        attention_mask=attention_mask,\n        token_type_ids=token_type_ids,\n        position_ids=position_ids,\n        head_mask=head_mask,\n        inputs_embeds=inputs_embeds,\n        output_attentions=output_attentions,\n        output_hidden_states=output_hidden_states,\n        return_dict=return_dict,\n    )\n    sequence_output = outputs[0]\n    logits = self.classifier(sequence_output)\n\n    loss = None\n    if labels is not None:\n        # move labels to correct device to enable model parallelism\n        labels = labels.to(logits.device)  # type: ignore\n        if self.config.problem_type is None:\n            if self.num_labels == 1:\n                self.config.problem_type = 'regression'\n            elif self.num_labels &gt; 1 and labels.dtype in (torch.long, torch.int):  # type: ignore\n                self.config.problem_type = 'single_label_classification'\n            else:\n                self.config.problem_type = 'multi_label_classification'\n\n        if self.config.problem_type == 'regression':\n            loss_fct = MSELoss()\n            if self.num_labels == 1:\n                loss = loss_fct(logits.squeeze(), labels.squeeze())  #  type: ignore\n            else:\n                loss = loss_fct(logits, labels)\n        elif self.config.problem_type == 'single_label_classification':\n            loss_fct = CrossEntropyLoss()\n            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))  # type: ignore\n        elif self.config.problem_type == 'multi_label_classification':\n            loss_fct = BCEWithLogitsLoss()\n            loss = loss_fct(logits, labels)\n\n    if not return_dict:\n        output = (logits,) + outputs[2:]\n        return ((loss,) + output) if loss is not None else output\n\n    return SequenceClassifierOutput(\n        loss=loss,\n        logits=logits,\n        hidden_states=outputs.hidden_states,\n        attentions=outputs.attentions,\n    )\n</code></pre>"},{"location":"reference/models/mag_model/#models.mag_model.MAGRobertaModel","title":"<code>MAGRobertaModel</code>","text":"<p>               Bases: <code>RobertaModel</code></p> <p>This class is a modified version of the RobertaModel class from the transformers library. It adds the MAG module to the forward pass.</p> Source code in <code>src/models/mag_model.py</code> <pre><code>class MAGRobertaModel(RobertaModel):\n    \"\"\"\n    This class is a modified version of the RobertaModel class from the transformers library.\n    It adds the MAG module to the forward pass.\n    \"\"\"\n\n    def __init__(self, config, multimodal_config, add_pooling_layer=True):\n        super().__init__(config, add_pooling_layer)\n        self.config = config\n\n        self.embeddings = RobertaEmbeddings(config)\n        self.encoder = RobertaEncoder(config, multimodal_config=multimodal_config)\n\n        self.pooler = RobertaPooler(config) if add_pooling_layer else None\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    # Copied from transformers.models.roberta.modeling_roberta.RobertaModel.forward\n    def forward(\n        self,\n        input_ids: Optional[torch.Tensor] = None,\n        gaze_features: Optional[torch.Tensor] = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        token_type_ids: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.Tensor] = None,\n        head_mask: Optional[torch.Tensor] = None,\n        inputs_embeds: Optional[torch.Tensor] = None,\n        encoder_hidden_states: Optional[torch.Tensor] = None,\n        encoder_attention_mask: Optional[torch.Tensor] = None,\n        past_key_values: Optional[list[torch.FloatTensor]] = None,\n        use_cache: Optional[bool] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n    ) -&gt; Union[tuple[torch.Tensor], BaseModelOutputWithPoolingAndCrossAttentions]:\n        r\"\"\"\n        encoder_hidden_states  (`torch.FloatTensor` of shape\n        `(batch_size, sequence_length, hidden_size)`, *optional*):\n            Sequence of hidden-states at the output of the last layer of the encoder.\n            Used in the cross-attention if the model is configured as a decoder.\n        encoder_attention_mask (`torch.FloatTensor` of shape\n        `(batch_size, sequence_length)`, *optional*):\n            Mask to avoid performing attention on the padding token indices of the encoder input.\n            This mask is used in the cross-attention if the model is configured as a decoder.\n            Mask values selected in `[0, 1]`:\n\n            - 1 for tokens that are **not masked**,\n            - 0 for tokens that are **masked**.\n        past_key_values (`tuple(tuple(torch.FloatTensor))` of length `config.n_layers` with\n        each tuple having 4 tensors of shape\n        `(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`):\n            Contains precomputed key and value hidden states of the attention blocks.\n            Can be used to speed up decoding.\n\n            If `past_key_values` are used, can optionally input only the last `decoder_input_ids`\n            (those that don't have their past key value states given to this model) of shape\n            `(batch_size, 1)` instead of all `decoder_input_ids` of\n            shape `(batch_size, sequence_length)`.\n        use_cache (`bool`, *optional*):\n            If set to `True`, `past_key_values` key value states are returned\n            and can be used to speed up decoding (see `past_key_values`).\n        \"\"\"\n        output_attentions = (\n            output_attentions\n            if output_attentions is not None\n            else self.config.output_attentions\n        )\n        output_hidden_states = (\n            output_hidden_states\n            if output_hidden_states is not None\n            else self.config.output_hidden_states\n        )\n        return_dict = (\n            return_dict if return_dict is not None else self.config.use_return_dict\n        )\n\n        if self.config.is_decoder:\n            use_cache = use_cache if use_cache is not None else self.config.use_cache\n        else:\n            use_cache = False\n\n        if input_ids is not None and inputs_embeds is not None:\n            raise ValueError(\n                'You cannot specify both input_ids and inputs_embeds at the same time'\n            )\n        elif input_ids is not None:\n            input_shape = input_ids.size()\n        elif inputs_embeds is not None:\n            input_shape = inputs_embeds.size()[:-1]\n        else:\n            raise ValueError('You have to specify either input_ids or inputs_embeds')\n\n        batch_size, seq_length = input_shape\n        device = input_ids.device if input_ids is not None else inputs_embeds.device  # type: ignore\n\n        # past_key_values_length\n        past_key_values_length = (\n            past_key_values[0][0].shape[2] if past_key_values is not None else 0\n        )\n\n        if attention_mask is None:\n            attention_mask = torch.ones(\n                ((batch_size, seq_length + past_key_values_length)), device=device\n            )\n\n        if token_type_ids is None:\n            if hasattr(self.embeddings, 'token_type_ids'):\n                buffered_token_type_ids = self.embeddings.token_type_ids[:, :seq_length]  # type: ignore\n                buffered_token_type_ids_expanded = buffered_token_type_ids.expand(\n                    batch_size, seq_length\n                )\n                token_type_ids = buffered_token_type_ids_expanded\n            else:\n                token_type_ids = torch.zeros(\n                    input_shape, dtype=torch.long, device=device\n                )\n\n        # We can provide a self-attention mask of dimensions\n        # [batch_size, from_seq_length, to_seq_length]\n        # ourselves in which case we just need to make it broadcastable to all heads.\n        extended_attention_mask: torch.Tensor = self.get_extended_attention_mask(\n            attention_mask,\n            input_shape,  # type: ignore\n        )\n\n        # If a 2D or 3D attention mask is provided for the cross-attention\n        # we need to make broadcastable to [batch_size, num_heads, seq_length, seq_length]\n        if self.config.is_decoder and encoder_hidden_states is not None:\n            (\n                encoder_batch_size,\n                encoder_sequence_length,\n                _,\n            ) = encoder_hidden_states.size()\n            encoder_hidden_shape = (encoder_batch_size, encoder_sequence_length)\n            if encoder_attention_mask is None:\n                encoder_attention_mask = torch.ones(encoder_hidden_shape, device=device)\n            encoder_extended_attention_mask = self.invert_attention_mask(\n                encoder_attention_mask\n            )\n        else:\n            encoder_extended_attention_mask = None\n\n        # Prepare head mask if needed\n        # 1.0 in head_mask indicate we keep the head\n        # attention_probs has shape bsz x n_heads x N x N\n        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\n        # and head_mask is converted to shape\n        # [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n        head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n\n        embedding_output = self.embeddings(\n            input_ids=input_ids,\n            position_ids=position_ids,\n            token_type_ids=token_type_ids,\n            inputs_embeds=inputs_embeds,\n            past_key_values_length=past_key_values_length,\n        )\n\n        encoder_outputs = self.encoder(\n            embedding_output,\n            gaze_features,\n            attention_mask=extended_attention_mask,\n            head_mask=head_mask,\n            encoder_hidden_states=encoder_hidden_states,\n            encoder_attention_mask=encoder_extended_attention_mask,\n            past_key_values=past_key_values,\n            use_cache=use_cache,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n        sequence_output = encoder_outputs[0]\n        pooled_output = (\n            self.pooler(sequence_output) if self.pooler is not None else None\n        )\n\n        if not return_dict:\n            return (sequence_output, pooled_output) + encoder_outputs[1:]\n\n        return BaseModelOutputWithPoolingAndCrossAttentions(\n            last_hidden_state=sequence_output,\n            pooler_output=pooled_output,  # type: ignore\n            past_key_values=encoder_outputs.past_key_values,\n            hidden_states=encoder_outputs.hidden_states,\n            attentions=encoder_outputs.attentions,\n            cross_attentions=encoder_outputs.cross_attentions,\n        )\n</code></pre>"},{"location":"reference/models/mag_model/#models.mag_model.MAGRobertaModel.forward","title":"<code>forward(input_ids=None, gaze_features=None, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None, inputs_embeds=None, encoder_hidden_states=None, encoder_attention_mask=None, past_key_values=None, use_cache=None, output_attentions=None, output_hidden_states=None, return_dict=None)</code>","text":"<p>encoder_hidden_states  (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, optional):     Sequence of hidden-states at the output of the last layer of the encoder.     Used in the cross-attention if the model is configured as a decoder. encoder_attention_mask (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>, optional):     Mask to avoid performing attention on the padding token indices of the encoder input.     This mask is used in the cross-attention if the model is configured as a decoder.     Mask values selected in <code>[0, 1]</code>:</p> <pre><code>- 1 for tokens that are **not masked**,\n- 0 for tokens that are **masked**.\n</code></pre> <p>past_key_values (<code>tuple(tuple(torch.FloatTensor))</code> of length <code>config.n_layers</code> with each tuple having 4 tensors of shape <code>(batch_size, num_heads, sequence_length - 1, embed_size_per_head)</code>):     Contains precomputed key and value hidden states of the attention blocks.     Can be used to speed up decoding.</p> <pre><code>If `past_key_values` are used, can optionally input only the last `decoder_input_ids`\n(those that don't have their past key value states given to this model) of shape\n`(batch_size, 1)` instead of all `decoder_input_ids` of\nshape `(batch_size, sequence_length)`.\n</code></pre> <p>use_cache (<code>bool</code>, optional):     If set to <code>True</code>, <code>past_key_values</code> key value states are returned     and can be used to speed up decoding (see <code>past_key_values</code>).</p> Source code in <code>src/models/mag_model.py</code> <pre><code>def forward(\n    self,\n    input_ids: Optional[torch.Tensor] = None,\n    gaze_features: Optional[torch.Tensor] = None,\n    attention_mask: Optional[torch.Tensor] = None,\n    token_type_ids: Optional[torch.Tensor] = None,\n    position_ids: Optional[torch.Tensor] = None,\n    head_mask: Optional[torch.Tensor] = None,\n    inputs_embeds: Optional[torch.Tensor] = None,\n    encoder_hidden_states: Optional[torch.Tensor] = None,\n    encoder_attention_mask: Optional[torch.Tensor] = None,\n    past_key_values: Optional[list[torch.FloatTensor]] = None,\n    use_cache: Optional[bool] = None,\n    output_attentions: Optional[bool] = None,\n    output_hidden_states: Optional[bool] = None,\n    return_dict: Optional[bool] = None,\n) -&gt; Union[tuple[torch.Tensor], BaseModelOutputWithPoolingAndCrossAttentions]:\n    r\"\"\"\n    encoder_hidden_states  (`torch.FloatTensor` of shape\n    `(batch_size, sequence_length, hidden_size)`, *optional*):\n        Sequence of hidden-states at the output of the last layer of the encoder.\n        Used in the cross-attention if the model is configured as a decoder.\n    encoder_attention_mask (`torch.FloatTensor` of shape\n    `(batch_size, sequence_length)`, *optional*):\n        Mask to avoid performing attention on the padding token indices of the encoder input.\n        This mask is used in the cross-attention if the model is configured as a decoder.\n        Mask values selected in `[0, 1]`:\n\n        - 1 for tokens that are **not masked**,\n        - 0 for tokens that are **masked**.\n    past_key_values (`tuple(tuple(torch.FloatTensor))` of length `config.n_layers` with\n    each tuple having 4 tensors of shape\n    `(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`):\n        Contains precomputed key and value hidden states of the attention blocks.\n        Can be used to speed up decoding.\n\n        If `past_key_values` are used, can optionally input only the last `decoder_input_ids`\n        (those that don't have their past key value states given to this model) of shape\n        `(batch_size, 1)` instead of all `decoder_input_ids` of\n        shape `(batch_size, sequence_length)`.\n    use_cache (`bool`, *optional*):\n        If set to `True`, `past_key_values` key value states are returned\n        and can be used to speed up decoding (see `past_key_values`).\n    \"\"\"\n    output_attentions = (\n        output_attentions\n        if output_attentions is not None\n        else self.config.output_attentions\n    )\n    output_hidden_states = (\n        output_hidden_states\n        if output_hidden_states is not None\n        else self.config.output_hidden_states\n    )\n    return_dict = (\n        return_dict if return_dict is not None else self.config.use_return_dict\n    )\n\n    if self.config.is_decoder:\n        use_cache = use_cache if use_cache is not None else self.config.use_cache\n    else:\n        use_cache = False\n\n    if input_ids is not None and inputs_embeds is not None:\n        raise ValueError(\n            'You cannot specify both input_ids and inputs_embeds at the same time'\n        )\n    elif input_ids is not None:\n        input_shape = input_ids.size()\n    elif inputs_embeds is not None:\n        input_shape = inputs_embeds.size()[:-1]\n    else:\n        raise ValueError('You have to specify either input_ids or inputs_embeds')\n\n    batch_size, seq_length = input_shape\n    device = input_ids.device if input_ids is not None else inputs_embeds.device  # type: ignore\n\n    # past_key_values_length\n    past_key_values_length = (\n        past_key_values[0][0].shape[2] if past_key_values is not None else 0\n    )\n\n    if attention_mask is None:\n        attention_mask = torch.ones(\n            ((batch_size, seq_length + past_key_values_length)), device=device\n        )\n\n    if token_type_ids is None:\n        if hasattr(self.embeddings, 'token_type_ids'):\n            buffered_token_type_ids = self.embeddings.token_type_ids[:, :seq_length]  # type: ignore\n            buffered_token_type_ids_expanded = buffered_token_type_ids.expand(\n                batch_size, seq_length\n            )\n            token_type_ids = buffered_token_type_ids_expanded\n        else:\n            token_type_ids = torch.zeros(\n                input_shape, dtype=torch.long, device=device\n            )\n\n    # We can provide a self-attention mask of dimensions\n    # [batch_size, from_seq_length, to_seq_length]\n    # ourselves in which case we just need to make it broadcastable to all heads.\n    extended_attention_mask: torch.Tensor = self.get_extended_attention_mask(\n        attention_mask,\n        input_shape,  # type: ignore\n    )\n\n    # If a 2D or 3D attention mask is provided for the cross-attention\n    # we need to make broadcastable to [batch_size, num_heads, seq_length, seq_length]\n    if self.config.is_decoder and encoder_hidden_states is not None:\n        (\n            encoder_batch_size,\n            encoder_sequence_length,\n            _,\n        ) = encoder_hidden_states.size()\n        encoder_hidden_shape = (encoder_batch_size, encoder_sequence_length)\n        if encoder_attention_mask is None:\n            encoder_attention_mask = torch.ones(encoder_hidden_shape, device=device)\n        encoder_extended_attention_mask = self.invert_attention_mask(\n            encoder_attention_mask\n        )\n    else:\n        encoder_extended_attention_mask = None\n\n    # Prepare head mask if needed\n    # 1.0 in head_mask indicate we keep the head\n    # attention_probs has shape bsz x n_heads x N x N\n    # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\n    # and head_mask is converted to shape\n    # [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n    head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n\n    embedding_output = self.embeddings(\n        input_ids=input_ids,\n        position_ids=position_ids,\n        token_type_ids=token_type_ids,\n        inputs_embeds=inputs_embeds,\n        past_key_values_length=past_key_values_length,\n    )\n\n    encoder_outputs = self.encoder(\n        embedding_output,\n        gaze_features,\n        attention_mask=extended_attention_mask,\n        head_mask=head_mask,\n        encoder_hidden_states=encoder_hidden_states,\n        encoder_attention_mask=encoder_extended_attention_mask,\n        past_key_values=past_key_values,\n        use_cache=use_cache,\n        output_attentions=output_attentions,\n        output_hidden_states=output_hidden_states,\n        return_dict=return_dict,\n    )\n    sequence_output = encoder_outputs[0]\n    pooled_output = (\n        self.pooler(sequence_output) if self.pooler is not None else None\n    )\n\n    if not return_dict:\n        return (sequence_output, pooled_output) + encoder_outputs[1:]\n\n    return BaseModelOutputWithPoolingAndCrossAttentions(\n        last_hidden_state=sequence_output,\n        pooler_output=pooled_output,  # type: ignore\n        past_key_values=encoder_outputs.past_key_values,\n        hidden_states=encoder_outputs.hidden_states,\n        attentions=encoder_outputs.attentions,\n        cross_attentions=encoder_outputs.cross_attentions,\n    )\n</code></pre>"},{"location":"reference/models/mag_model/#models.mag_model.RobertaEncoder","title":"<code>RobertaEncoder</code>","text":"<p>               Bases: <code>Module</code></p> <p>This class is a modified version of the RobertaEncoder class from the transformers library.</p> Source code in <code>src/models/mag_model.py</code> <pre><code>class RobertaEncoder(nn.Module):\n    \"\"\"\n    This class is a modified version of the RobertaEncoder class from the transformers library.\n    \"\"\"\n\n    def __init__(self, config, multimodal_config):\n        super().__init__()\n        self.config = config\n        self.layer = nn.ModuleList(\n            [RobertaLayer(config) for _ in range(config.num_hidden_layers)]\n        )\n        self.gradient_checkpointing = False\n        self.mag_injection_index = multimodal_config.mag_injection_index\n        self.mag = MAGModule(\n            hidden_size=config.hidden_size,\n            beta_shift=multimodal_config.beta_shift,\n            dropout_prob=multimodal_config.dropout_prob,\n            text_dim=multimodal_config.text_dim,\n            eyes_dim=multimodal_config.eyes_dim,\n        )\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        gaze_features: torch.Tensor | None,\n        attention_mask: torch.Tensor | None = None,\n        head_mask: torch.Tensor | None | list = None,\n        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n        past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n        use_cache: Optional[bool] = None,\n        output_attentions: Optional[bool] = False,\n        output_hidden_states: Optional[bool] = False,\n        return_dict: Optional[bool] = True,\n    ) -&gt; Union[tuple[torch.Tensor], BaseModelOutputWithPastAndCrossAttentions]:\n        all_hidden_states = () if output_hidden_states else None\n        all_self_attentions = () if output_attentions else None\n        all_cross_attentions = (\n            () if output_attentions and self.config.add_cross_attention else None\n        )\n\n        if self.gradient_checkpointing and self.training:\n            if use_cache:\n                print(\n                    '`use_cache=True` is incompatible with gradient checkpointing. '\n                    'Setting `use_cache=False`...'\n                )\n                use_cache = False\n\n        next_decoder_cache = () if use_cache else None\n        for i, layer_module in enumerate(self.layer):\n            if output_hidden_states:\n                all_hidden_states = all_hidden_states + (hidden_states,)  # type: ignore\n\n            layer_head_mask = head_mask[i] if head_mask is not None else None\n            past_key_value = past_key_values[i] if past_key_values is not None else None\n\n            if i == self.mag_injection_index and gaze_features is not None:\n                hidden_states = self.mag(hidden_states, gaze_features)\n\n            if self.gradient_checkpointing and self.training:\n\n                def create_custom_forward(module):\n                    def custom_forward(*inputs):\n                        return module(\n                            *inputs,\n                            past_key_value,  # pylint: disable=cell-var-from-loop\n                            output_attentions,\n                        )\n\n                    return custom_forward\n\n                layer_outputs = torch.utils.checkpoint.checkpoint(  # type: ignore\n                    create_custom_forward(layer_module),\n                    hidden_states,\n                    attention_mask,\n                    layer_head_mask,\n                    encoder_hidden_states,\n                    encoder_attention_mask,\n                )\n            else:\n                layer_outputs = layer_module(\n                    hidden_states,\n                    attention_mask,\n                    layer_head_mask,\n                    encoder_hidden_states,\n                    encoder_attention_mask,\n                    past_key_value,\n                    output_attentions,\n                )\n\n            hidden_states = layer_outputs[0]\n            if use_cache:\n                next_decoder_cache += (layer_outputs[-1],)  # type: ignore\n            if output_attentions:\n                all_self_attentions = all_self_attentions + (layer_outputs[1],)  # type: ignore\n                if self.config.add_cross_attention:\n                    all_cross_attentions = all_cross_attentions + (layer_outputs[2],)  # type: ignore\n\n        if output_hidden_states:\n            all_hidden_states = all_hidden_states + (hidden_states,)  # type: ignore\n\n        if not return_dict:\n            return tuple(\n                v\n                for v in [\n                    hidden_states,\n                    next_decoder_cache,\n                    all_hidden_states,\n                    all_self_attentions,\n                    all_cross_attentions,\n                ]\n                if v is not None\n            )\n        return BaseModelOutputWithPastAndCrossAttentions(\n            last_hidden_state=hidden_states,  # type: ignore\n            past_key_values=next_decoder_cache,  # type: ignore\n            hidden_states=all_hidden_states,  # type: ignore\n            attentions=all_self_attentions,  # type: ignore\n            cross_attentions=all_cross_attentions,  # type: ignore\n        )\n</code></pre>"},{"location":"reference/models/mag_model/#models.mag_model.XLMMAGRobertaForSequenceClassification","title":"<code>XLMMAGRobertaForSequenceClassification</code>","text":"<p>               Bases: <code>XLMRobertaForSequenceClassification</code></p> <p>This class is a modified version of the RobertaForSequenceClassification class from the transformers library. It adds the MAG module to the forward pass.</p> <p>Copied from transformers.models.roberta.modeling_roberta.RobertaForSequenceClassification</p> Source code in <code>src/models/mag_model.py</code> <pre><code>class XLMMAGRobertaForSequenceClassification(XLMRobertaForSequenceClassification):\n    \"\"\"\n    This class is a modified version of the RobertaForSequenceClassification class\n    from the transformers library.\n    It adds the MAG module to the forward pass.\n\n    Copied from transformers.models.roberta.modeling_roberta.RobertaForSequenceClassification\n    \"\"\"\n\n    _keys_to_ignore_on_load_missing = [r'position_ids']\n\n    def __init__(self, config, multimodal_config):\n        super().__init__(config)\n        self.num_labels = config.num_labels\n        self.config = config\n\n        self.roberta = XLMMAGRobertaModel(\n            config, multimodal_config, add_pooling_layer=False\n        )\n        self.classifier = XLMRobertaClassificationHead(config)\n\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    def forward(\n        self,\n        input_ids: Optional[torch.Tensor] = None,\n        gaze_features: Optional[torch.Tensor] = None,\n        gaze_positions: Optional[torch.Tensor] = None,\n        attention_mask: torch.Tensor | None = None,\n        token_type_ids: Optional[torch.LongTensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        head_mask: Optional[torch.FloatTensor] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        labels: Optional[torch.Tensor] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        eye_token_type_ids: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n    ) -&gt; Union[tuple[torch.Tensor], SequenceClassifierOutput]:\n        r\"\"\"\n        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n            Labels for computing the sequence classification/regression loss.\n            Indices should be in `[0, ..., config.num_labels - 1]`.\n            If `config.num_labels == 1` a regression loss is computed (Mean-Square loss),\n            If `config.num_labels &gt; 1` a classification loss is computed (Cross-Entropy).\n        \"\"\"\n        return_dict = (\n            return_dict if return_dict is not None else self.config.use_return_dict\n        )\n\n        outputs = self.roberta(\n            input_ids=input_ids,\n            gaze_features=gaze_features,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n            position_ids=position_ids,\n            head_mask=head_mask,\n            inputs_embeds=inputs_embeds,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n        sequence_output = outputs[0]\n        logits = self.classifier(sequence_output)\n\n        loss = None\n        if labels is not None:\n            # move labels to correct device to enable model parallelism\n            labels = labels.to(logits.device)  # type: ignore\n            if self.config.problem_type is None:\n                if self.num_labels == 1:\n                    self.config.problem_type = 'regression'\n                elif self.num_labels &gt; 1 and labels.dtype in (torch.long, torch.int):  # type: ignore\n                    self.config.problem_type = 'single_label_classification'\n                else:\n                    self.config.problem_type = 'multi_label_classification'\n\n            if self.config.problem_type == 'regression':\n                loss_fct = MSELoss()\n                if self.num_labels == 1:\n                    loss = loss_fct(logits.squeeze(), labels.squeeze())  #  type: ignore\n                else:\n                    loss = loss_fct(logits, labels)\n            elif self.config.problem_type == 'single_label_classification':\n                loss_fct = CrossEntropyLoss()\n                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))  # type: ignore\n            elif self.config.problem_type == 'multi_label_classification':\n                loss_fct = BCEWithLogitsLoss()\n                loss = loss_fct(logits, labels)\n\n        if not return_dict:\n            output = (logits,) + outputs[2:]\n            return ((loss,) + output) if loss is not None else output\n\n        return SequenceClassifierOutput(\n            loss=loss,\n            logits=logits,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )\n</code></pre>"},{"location":"reference/models/mag_model/#models.mag_model.XLMMAGRobertaForSequenceClassification.forward","title":"<code>forward(input_ids=None, gaze_features=None, gaze_positions=None, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None, inputs_embeds=None, labels=None, output_attentions=None, output_hidden_states=None, eye_token_type_ids=None, return_dict=None)</code>","text":"<p>labels (<code>torch.LongTensor</code> of shape <code>(batch_size,)</code>, optional):     Labels for computing the sequence classification/regression loss.     Indices should be in <code>[0, ..., config.num_labels - 1]</code>.     If <code>config.num_labels == 1</code> a regression loss is computed (Mean-Square loss),     If <code>config.num_labels &gt; 1</code> a classification loss is computed (Cross-Entropy).</p> Source code in <code>src/models/mag_model.py</code> <pre><code>def forward(\n    self,\n    input_ids: Optional[torch.Tensor] = None,\n    gaze_features: Optional[torch.Tensor] = None,\n    gaze_positions: Optional[torch.Tensor] = None,\n    attention_mask: torch.Tensor | None = None,\n    token_type_ids: Optional[torch.LongTensor] = None,\n    position_ids: Optional[torch.LongTensor] = None,\n    head_mask: Optional[torch.FloatTensor] = None,\n    inputs_embeds: Optional[torch.FloatTensor] = None,\n    labels: Optional[torch.Tensor] = None,\n    output_attentions: Optional[bool] = None,\n    output_hidden_states: Optional[bool] = None,\n    eye_token_type_ids: Optional[bool] = None,\n    return_dict: Optional[bool] = None,\n) -&gt; Union[tuple[torch.Tensor], SequenceClassifierOutput]:\n    r\"\"\"\n    labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n        Labels for computing the sequence classification/regression loss.\n        Indices should be in `[0, ..., config.num_labels - 1]`.\n        If `config.num_labels == 1` a regression loss is computed (Mean-Square loss),\n        If `config.num_labels &gt; 1` a classification loss is computed (Cross-Entropy).\n    \"\"\"\n    return_dict = (\n        return_dict if return_dict is not None else self.config.use_return_dict\n    )\n\n    outputs = self.roberta(\n        input_ids=input_ids,\n        gaze_features=gaze_features,\n        attention_mask=attention_mask,\n        token_type_ids=token_type_ids,\n        position_ids=position_ids,\n        head_mask=head_mask,\n        inputs_embeds=inputs_embeds,\n        output_attentions=output_attentions,\n        output_hidden_states=output_hidden_states,\n        return_dict=return_dict,\n    )\n    sequence_output = outputs[0]\n    logits = self.classifier(sequence_output)\n\n    loss = None\n    if labels is not None:\n        # move labels to correct device to enable model parallelism\n        labels = labels.to(logits.device)  # type: ignore\n        if self.config.problem_type is None:\n            if self.num_labels == 1:\n                self.config.problem_type = 'regression'\n            elif self.num_labels &gt; 1 and labels.dtype in (torch.long, torch.int):  # type: ignore\n                self.config.problem_type = 'single_label_classification'\n            else:\n                self.config.problem_type = 'multi_label_classification'\n\n        if self.config.problem_type == 'regression':\n            loss_fct = MSELoss()\n            if self.num_labels == 1:\n                loss = loss_fct(logits.squeeze(), labels.squeeze())  #  type: ignore\n            else:\n                loss = loss_fct(logits, labels)\n        elif self.config.problem_type == 'single_label_classification':\n            loss_fct = CrossEntropyLoss()\n            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))  # type: ignore\n        elif self.config.problem_type == 'multi_label_classification':\n            loss_fct = BCEWithLogitsLoss()\n            loss = loss_fct(logits, labels)\n\n    if not return_dict:\n        output = (logits,) + outputs[2:]\n        return ((loss,) + output) if loss is not None else output\n\n    return SequenceClassifierOutput(\n        loss=loss,\n        logits=logits,\n        hidden_states=outputs.hidden_states,\n        attentions=outputs.attentions,\n    )\n</code></pre>"},{"location":"reference/models/mag_model/#models.mag_model.XLMMAGRobertaModel","title":"<code>XLMMAGRobertaModel</code>","text":"<p>               Bases: <code>XLMRobertaModel</code></p> <p>This class is a modified version of the RobertaModel class from the transformers library. It adds the MAG module to the forward pass.</p> Source code in <code>src/models/mag_model.py</code> <pre><code>class XLMMAGRobertaModel(XLMRobertaModel):\n    \"\"\"\n    This class is a modified version of the RobertaModel class from the transformers library.\n    It adds the MAG module to the forward pass.\n    \"\"\"\n\n    def __init__(self, config, multimodal_config, add_pooling_layer=True):\n        super().__init__(config, add_pooling_layer)\n        self.config = config\n\n        self.embeddings = XLMRobertaEmbeddings(config)\n        self.encoder = XLMRobertaEncoder(config, multimodal_config=multimodal_config)\n\n        self.pooler = XLMRobertaPooler(config) if add_pooling_layer else None\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    # Copied from transformers.models.roberta.modeling_roberta.RobertaModel.forward\n    def forward(\n        self,\n        input_ids: Optional[torch.Tensor] = None,\n        gaze_features: Optional[torch.Tensor] = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        token_type_ids: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.Tensor] = None,\n        head_mask: Optional[torch.Tensor] = None,\n        inputs_embeds: Optional[torch.Tensor] = None,\n        encoder_hidden_states: Optional[torch.Tensor] = None,\n        encoder_attention_mask: Optional[torch.Tensor] = None,\n        past_key_values: Optional[list[torch.FloatTensor]] = None,\n        use_cache: Optional[bool] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n    ) -&gt; Union[tuple[torch.Tensor], BaseModelOutputWithPoolingAndCrossAttentions]:\n        r\"\"\"\n        encoder_hidden_states  (`torch.FloatTensor` of shape\n        `(batch_size, sequence_length, hidden_size)`, *optional*):\n            Sequence of hidden-states at the output of the last layer of the encoder.\n            Used in the cross-attention if the model is configured as a decoder.\n        encoder_attention_mask (`torch.FloatTensor` of shape\n        `(batch_size, sequence_length)`, *optional*):\n            Mask to avoid performing attention on the padding token indices of the encoder input.\n            This mask is used in the cross-attention if the model is configured as a decoder.\n            Mask values selected in `[0, 1]`:\n\n            - 1 for tokens that are **not masked**,\n            - 0 for tokens that are **masked**.\n        past_key_values (`tuple(tuple(torch.FloatTensor))` of length `config.n_layers` with\n        each tuple having 4 tensors of shape\n        `(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`):\n            Contains precomputed key and value hidden states of the attention blocks.\n            Can be used to speed up decoding.\n\n            If `past_key_values` are used, can optionally input only the last `decoder_input_ids`\n            (those that don't have their past key value states given to this model) of shape\n            `(batch_size, 1)` instead of all `decoder_input_ids` of\n            shape `(batch_size, sequence_length)`.\n        use_cache (`bool`, *optional*):\n            If set to `True`, `past_key_values` key value states are returned\n            and can be used to speed up decoding (see `past_key_values`).\n        \"\"\"\n        output_attentions = (\n            output_attentions\n            if output_attentions is not None\n            else self.config.output_attentions\n        )\n        output_hidden_states = (\n            output_hidden_states\n            if output_hidden_states is not None\n            else self.config.output_hidden_states\n        )\n        return_dict = (\n            return_dict if return_dict is not None else self.config.use_return_dict\n        )\n\n        if self.config.is_decoder:\n            use_cache = use_cache if use_cache is not None else self.config.use_cache\n        else:\n            use_cache = False\n\n        if input_ids is not None and inputs_embeds is not None:\n            raise ValueError(\n                'You cannot specify both input_ids and inputs_embeds at the same time'\n            )\n        elif input_ids is not None:\n            input_shape = input_ids.size()\n        elif inputs_embeds is not None:\n            input_shape = inputs_embeds.size()[:-1]\n        else:\n            raise ValueError('You have to specify either input_ids or inputs_embeds')\n\n        batch_size, seq_length = input_shape\n        device = input_ids.device if input_ids is not None else inputs_embeds.device  # type: ignore\n\n        # past_key_values_length\n        past_key_values_length = (\n            past_key_values[0][0].shape[2] if past_key_values is not None else 0\n        )\n\n        if attention_mask is None:\n            attention_mask = torch.ones(\n                ((batch_size, seq_length + past_key_values_length)), device=device\n            )\n\n        if token_type_ids is None:\n            if hasattr(self.embeddings, 'token_type_ids'):\n                buffered_token_type_ids = self.embeddings.token_type_ids[:, :seq_length]  # type: ignore\n                buffered_token_type_ids_expanded = buffered_token_type_ids.expand(\n                    batch_size, seq_length\n                )\n                token_type_ids = buffered_token_type_ids_expanded\n            else:\n                token_type_ids = torch.zeros(\n                    input_shape, dtype=torch.long, device=device\n                )\n\n        # We can provide a self-attention mask of dimensions\n        # [batch_size, from_seq_length, to_seq_length]\n        # ourselves in which case we just need to make it broadcastable to all heads.\n        extended_attention_mask: torch.Tensor = self.get_extended_attention_mask(\n            attention_mask,\n            input_shape,  # type: ignore\n        )\n\n        # If a 2D or 3D attention mask is provided for the cross-attention\n        # we need to make broadcastable to [batch_size, num_heads, seq_length, seq_length]\n        if self.config.is_decoder and encoder_hidden_states is not None:\n            (\n                encoder_batch_size,\n                encoder_sequence_length,\n                _,\n            ) = encoder_hidden_states.size()\n            encoder_hidden_shape = (encoder_batch_size, encoder_sequence_length)\n            if encoder_attention_mask is None:\n                encoder_attention_mask = torch.ones(encoder_hidden_shape, device=device)\n            encoder_extended_attention_mask = self.invert_attention_mask(\n                encoder_attention_mask\n            )\n        else:\n            encoder_extended_attention_mask = None\n\n        # Prepare head mask if needed\n        # 1.0 in head_mask indicate we keep the head\n        # attention_probs has shape bsz x n_heads x N x N\n        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\n        # and head_mask is converted to shape\n        # [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n        head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n\n        embedding_output = self.embeddings(\n            input_ids=input_ids,\n            position_ids=position_ids,\n            token_type_ids=token_type_ids,\n            inputs_embeds=inputs_embeds,\n            past_key_values_length=past_key_values_length,\n        )\n\n        encoder_outputs = self.encoder(\n            embedding_output,\n            gaze_features,\n            attention_mask=extended_attention_mask,\n            head_mask=head_mask,\n            encoder_hidden_states=encoder_hidden_states,\n            encoder_attention_mask=encoder_extended_attention_mask,\n            past_key_values=past_key_values,\n            use_cache=use_cache,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n        sequence_output = encoder_outputs[0]\n        pooled_output = (\n            self.pooler(sequence_output) if self.pooler is not None else None\n        )\n\n        if not return_dict:\n            return (sequence_output, pooled_output) + encoder_outputs[1:]\n\n        return BaseModelOutputWithPoolingAndCrossAttentions(\n            last_hidden_state=sequence_output,\n            pooler_output=pooled_output,  # type: ignore\n            past_key_values=encoder_outputs.past_key_values,\n            hidden_states=encoder_outputs.hidden_states,\n            attentions=encoder_outputs.attentions,\n            cross_attentions=encoder_outputs.cross_attentions,\n        )\n</code></pre>"},{"location":"reference/models/mag_model/#models.mag_model.XLMMAGRobertaModel.forward","title":"<code>forward(input_ids=None, gaze_features=None, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None, inputs_embeds=None, encoder_hidden_states=None, encoder_attention_mask=None, past_key_values=None, use_cache=None, output_attentions=None, output_hidden_states=None, return_dict=None)</code>","text":"<p>encoder_hidden_states  (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, optional):     Sequence of hidden-states at the output of the last layer of the encoder.     Used in the cross-attention if the model is configured as a decoder. encoder_attention_mask (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>, optional):     Mask to avoid performing attention on the padding token indices of the encoder input.     This mask is used in the cross-attention if the model is configured as a decoder.     Mask values selected in <code>[0, 1]</code>:</p> <pre><code>- 1 for tokens that are **not masked**,\n- 0 for tokens that are **masked**.\n</code></pre> <p>past_key_values (<code>tuple(tuple(torch.FloatTensor))</code> of length <code>config.n_layers</code> with each tuple having 4 tensors of shape <code>(batch_size, num_heads, sequence_length - 1, embed_size_per_head)</code>):     Contains precomputed key and value hidden states of the attention blocks.     Can be used to speed up decoding.</p> <pre><code>If `past_key_values` are used, can optionally input only the last `decoder_input_ids`\n(those that don't have their past key value states given to this model) of shape\n`(batch_size, 1)` instead of all `decoder_input_ids` of\nshape `(batch_size, sequence_length)`.\n</code></pre> <p>use_cache (<code>bool</code>, optional):     If set to <code>True</code>, <code>past_key_values</code> key value states are returned     and can be used to speed up decoding (see <code>past_key_values</code>).</p> Source code in <code>src/models/mag_model.py</code> <pre><code>def forward(\n    self,\n    input_ids: Optional[torch.Tensor] = None,\n    gaze_features: Optional[torch.Tensor] = None,\n    attention_mask: Optional[torch.Tensor] = None,\n    token_type_ids: Optional[torch.Tensor] = None,\n    position_ids: Optional[torch.Tensor] = None,\n    head_mask: Optional[torch.Tensor] = None,\n    inputs_embeds: Optional[torch.Tensor] = None,\n    encoder_hidden_states: Optional[torch.Tensor] = None,\n    encoder_attention_mask: Optional[torch.Tensor] = None,\n    past_key_values: Optional[list[torch.FloatTensor]] = None,\n    use_cache: Optional[bool] = None,\n    output_attentions: Optional[bool] = None,\n    output_hidden_states: Optional[bool] = None,\n    return_dict: Optional[bool] = None,\n) -&gt; Union[tuple[torch.Tensor], BaseModelOutputWithPoolingAndCrossAttentions]:\n    r\"\"\"\n    encoder_hidden_states  (`torch.FloatTensor` of shape\n    `(batch_size, sequence_length, hidden_size)`, *optional*):\n        Sequence of hidden-states at the output of the last layer of the encoder.\n        Used in the cross-attention if the model is configured as a decoder.\n    encoder_attention_mask (`torch.FloatTensor` of shape\n    `(batch_size, sequence_length)`, *optional*):\n        Mask to avoid performing attention on the padding token indices of the encoder input.\n        This mask is used in the cross-attention if the model is configured as a decoder.\n        Mask values selected in `[0, 1]`:\n\n        - 1 for tokens that are **not masked**,\n        - 0 for tokens that are **masked**.\n    past_key_values (`tuple(tuple(torch.FloatTensor))` of length `config.n_layers` with\n    each tuple having 4 tensors of shape\n    `(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`):\n        Contains precomputed key and value hidden states of the attention blocks.\n        Can be used to speed up decoding.\n\n        If `past_key_values` are used, can optionally input only the last `decoder_input_ids`\n        (those that don't have their past key value states given to this model) of shape\n        `(batch_size, 1)` instead of all `decoder_input_ids` of\n        shape `(batch_size, sequence_length)`.\n    use_cache (`bool`, *optional*):\n        If set to `True`, `past_key_values` key value states are returned\n        and can be used to speed up decoding (see `past_key_values`).\n    \"\"\"\n    output_attentions = (\n        output_attentions\n        if output_attentions is not None\n        else self.config.output_attentions\n    )\n    output_hidden_states = (\n        output_hidden_states\n        if output_hidden_states is not None\n        else self.config.output_hidden_states\n    )\n    return_dict = (\n        return_dict if return_dict is not None else self.config.use_return_dict\n    )\n\n    if self.config.is_decoder:\n        use_cache = use_cache if use_cache is not None else self.config.use_cache\n    else:\n        use_cache = False\n\n    if input_ids is not None and inputs_embeds is not None:\n        raise ValueError(\n            'You cannot specify both input_ids and inputs_embeds at the same time'\n        )\n    elif input_ids is not None:\n        input_shape = input_ids.size()\n    elif inputs_embeds is not None:\n        input_shape = inputs_embeds.size()[:-1]\n    else:\n        raise ValueError('You have to specify either input_ids or inputs_embeds')\n\n    batch_size, seq_length = input_shape\n    device = input_ids.device if input_ids is not None else inputs_embeds.device  # type: ignore\n\n    # past_key_values_length\n    past_key_values_length = (\n        past_key_values[0][0].shape[2] if past_key_values is not None else 0\n    )\n\n    if attention_mask is None:\n        attention_mask = torch.ones(\n            ((batch_size, seq_length + past_key_values_length)), device=device\n        )\n\n    if token_type_ids is None:\n        if hasattr(self.embeddings, 'token_type_ids'):\n            buffered_token_type_ids = self.embeddings.token_type_ids[:, :seq_length]  # type: ignore\n            buffered_token_type_ids_expanded = buffered_token_type_ids.expand(\n                batch_size, seq_length\n            )\n            token_type_ids = buffered_token_type_ids_expanded\n        else:\n            token_type_ids = torch.zeros(\n                input_shape, dtype=torch.long, device=device\n            )\n\n    # We can provide a self-attention mask of dimensions\n    # [batch_size, from_seq_length, to_seq_length]\n    # ourselves in which case we just need to make it broadcastable to all heads.\n    extended_attention_mask: torch.Tensor = self.get_extended_attention_mask(\n        attention_mask,\n        input_shape,  # type: ignore\n    )\n\n    # If a 2D or 3D attention mask is provided for the cross-attention\n    # we need to make broadcastable to [batch_size, num_heads, seq_length, seq_length]\n    if self.config.is_decoder and encoder_hidden_states is not None:\n        (\n            encoder_batch_size,\n            encoder_sequence_length,\n            _,\n        ) = encoder_hidden_states.size()\n        encoder_hidden_shape = (encoder_batch_size, encoder_sequence_length)\n        if encoder_attention_mask is None:\n            encoder_attention_mask = torch.ones(encoder_hidden_shape, device=device)\n        encoder_extended_attention_mask = self.invert_attention_mask(\n            encoder_attention_mask\n        )\n    else:\n        encoder_extended_attention_mask = None\n\n    # Prepare head mask if needed\n    # 1.0 in head_mask indicate we keep the head\n    # attention_probs has shape bsz x n_heads x N x N\n    # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\n    # and head_mask is converted to shape\n    # [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n    head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n\n    embedding_output = self.embeddings(\n        input_ids=input_ids,\n        position_ids=position_ids,\n        token_type_ids=token_type_ids,\n        inputs_embeds=inputs_embeds,\n        past_key_values_length=past_key_values_length,\n    )\n\n    encoder_outputs = self.encoder(\n        embedding_output,\n        gaze_features,\n        attention_mask=extended_attention_mask,\n        head_mask=head_mask,\n        encoder_hidden_states=encoder_hidden_states,\n        encoder_attention_mask=encoder_extended_attention_mask,\n        past_key_values=past_key_values,\n        use_cache=use_cache,\n        output_attentions=output_attentions,\n        output_hidden_states=output_hidden_states,\n        return_dict=return_dict,\n    )\n    sequence_output = encoder_outputs[0]\n    pooled_output = (\n        self.pooler(sequence_output) if self.pooler is not None else None\n    )\n\n    if not return_dict:\n        return (sequence_output, pooled_output) + encoder_outputs[1:]\n\n    return BaseModelOutputWithPoolingAndCrossAttentions(\n        last_hidden_state=sequence_output,\n        pooler_output=pooled_output,  # type: ignore\n        past_key_values=encoder_outputs.past_key_values,\n        hidden_states=encoder_outputs.hidden_states,\n        attentions=encoder_outputs.attentions,\n        cross_attentions=encoder_outputs.cross_attentions,\n    )\n</code></pre>"},{"location":"reference/models/mag_model/#models.mag_model.XLMRobertaEncoder","title":"<code>XLMRobertaEncoder</code>","text":"<p>               Bases: <code>Module</code></p> <p>This class is a modified version of the RobertaEncoder class from the transformers library.</p> Source code in <code>src/models/mag_model.py</code> <pre><code>class XLMRobertaEncoder(nn.Module):\n    \"\"\"\n    This class is a modified version of the RobertaEncoder class from the transformers library.\n    \"\"\"\n\n    def __init__(self, config, multimodal_config):\n        super().__init__()\n        self.config = config\n        self.layer = nn.ModuleList(\n            [XLMRobertaLayer(config) for _ in range(config.num_hidden_layers)]\n        )\n        self.gradient_checkpointing = False\n        self.mag_injection_index = multimodal_config.mag_injection_index\n        self.mag = MAGModule(\n            hidden_size=config.hidden_size,\n            beta_shift=multimodal_config.beta_shift,\n            dropout_prob=multimodal_config.dropout_prob,\n            text_dim=multimodal_config.text_dim,\n            eyes_dim=multimodal_config.eyes_dim,\n        )\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        gaze_features: torch.Tensor | None,\n        attention_mask: torch.Tensor | None = None,\n        head_mask: torch.Tensor | None | list = None,\n        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n        past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n        use_cache: Optional[bool] = None,\n        output_attentions: Optional[bool] = False,\n        output_hidden_states: Optional[bool] = False,\n        return_dict: Optional[bool] = True,\n    ) -&gt; Union[tuple[torch.Tensor], BaseModelOutputWithPastAndCrossAttentions]:\n        all_hidden_states = () if output_hidden_states else None\n        all_self_attentions = () if output_attentions else None\n        all_cross_attentions = (\n            () if output_attentions and self.config.add_cross_attention else None\n        )\n\n        if self.gradient_checkpointing and self.training:\n            if use_cache:\n                print(\n                    '`use_cache=True` is incompatible with gradient checkpointing. '\n                    'Setting `use_cache=False`...'\n                )\n                use_cache = False\n\n        next_decoder_cache = () if use_cache else None\n        for i, layer_module in enumerate(self.layer):\n            if output_hidden_states:\n                all_hidden_states = all_hidden_states + (hidden_states,)  # type: ignore\n\n            layer_head_mask = head_mask[i] if head_mask is not None else None\n            past_key_value = past_key_values[i] if past_key_values is not None else None\n\n            if i == self.mag_injection_index and gaze_features is not None:\n                hidden_states = self.mag(hidden_states, gaze_features)\n\n            if self.gradient_checkpointing and self.training:\n\n                def create_custom_forward(module):\n                    def custom_forward(*inputs):\n                        return module(\n                            *inputs,\n                            past_key_value,  # pylint: disable=cell-var-from-loop\n                            output_attentions,\n                        )\n\n                    return custom_forward\n\n                layer_outputs = torch.utils.checkpoint.checkpoint(  # type: ignore\n                    create_custom_forward(layer_module),\n                    hidden_states,\n                    attention_mask,\n                    layer_head_mask,\n                    encoder_hidden_states,\n                    encoder_attention_mask,\n                )\n            else:\n                layer_outputs = layer_module(\n                    hidden_states,\n                    attention_mask,\n                    layer_head_mask,\n                    encoder_hidden_states,\n                    encoder_attention_mask,\n                    past_key_value,\n                    output_attentions,\n                )\n\n            hidden_states = layer_outputs[0]\n            if use_cache:\n                next_decoder_cache += (layer_outputs[-1],)  # type: ignore\n            if output_attentions:\n                all_self_attentions = all_self_attentions + (layer_outputs[1],)  # type: ignore\n                if self.config.add_cross_attention:\n                    all_cross_attentions = all_cross_attentions + (layer_outputs[2],)  # type: ignore\n\n        if output_hidden_states:\n            all_hidden_states = all_hidden_states + (hidden_states,)  # type: ignore\n\n        if not return_dict:\n            return tuple(\n                v\n                for v in [\n                    hidden_states,\n                    next_decoder_cache,\n                    all_hidden_states,\n                    all_self_attentions,\n                    all_cross_attentions,\n                ]\n                if v is not None\n            )\n        return BaseModelOutputWithPastAndCrossAttentions(\n            last_hidden_state=hidden_states,  # type: ignore\n            past_key_values=next_decoder_cache,  # type: ignore\n            hidden_states=all_hidden_states,  # type: ignore\n            attentions=all_self_attentions,  # type: ignore\n            cross_attentions=all_cross_attentions,  # type: ignore\n        )\n</code></pre>"},{"location":"reference/models/models_ml/","title":"models_ml","text":""},{"location":"reference/models/models_ml/#models.models_ml.DummyClassifierMLModel","title":"<code>DummyClassifierMLModel</code>","text":"<p>               Bases: <code>BaseMLModel</code></p> <p>Dummy classifier that uses null features and a dummy classifier from sklearn.</p> Source code in <code>src/models/models_ml.py</code> <pre><code>@register_model\nclass DummyClassifierMLModel(BaseMLModel):\n    \"\"\"\n    Dummy classifier that uses null features and a dummy classifier from sklearn.\n    \"\"\"\n\n    def __init__(\n        self,\n        model_args: DummyClassifierMLArgs,\n        trainer_args: TrainerML,\n        data_args=DataArgs,\n    ):\n        super().__init__(\n            model_args=model_args, trainer_args=trainer_args, data_args=data_args\n        )\n\n    def fit(self, dm) -&gt; None:\n        # Dummy classifier: generate zero features just for demonstration\n        train_batches = self.shared_fit(dm)\n        features_list = []\n        y_true_list = []\n        for train_batch in train_batches:\n            features = torch.zeros((train_batch.labels.shape[0], 1)).to('cpu')\n            features_list.append(features)\n            y_true_list.append(train_batch.labels)\n\n        features = torch.cat(features_list, dim=0).numpy()\n        y_true = torch.cat(y_true_list, dim=0).numpy()\n        self.classifier.fit(features, y_true)\n\n    def model_specific_predict(\n        self, dev_batches: list\n    ) -&gt; tuple[list[torch.Tensor], list[torch.Tensor]]:\n        preds_list: list[torch.Tensor] = []\n        probs_list: list[torch.Tensor] = []\n        for dev_batch in dev_batches:\n            features = torch.zeros((dev_batch.labels.shape[0], 1)).to('cpu').numpy()\n            preds, probs = self._predict_with_fallback(features)\n            preds_list.append(preds)\n            probs_list.append(probs)\n        return preds_list, probs_list\n</code></pre>"},{"location":"reference/models/models_ml/#models.models_ml.DummyRegressorMLModel","title":"<code>DummyRegressorMLModel</code>","text":"<p>               Bases: <code>BaseMLModel</code></p> <p>Dummy regressor that uses null features and a dummy regressor from sklearn.</p> Source code in <code>src/models/models_ml.py</code> <pre><code>@register_model\nclass DummyRegressorMLModel(BaseMLModel):\n    \"\"\"\n    Dummy regressor that uses null features and a dummy regressor from sklearn.\n    \"\"\"\n\n    def __init__(\n        self,\n        model_args: DummyRegressorMLArgs,\n        trainer_args: TrainerML,\n        data_args=DataArgs,\n    ):\n        super().__init__(\n            model_args=model_args, trainer_args=trainer_args, data_args=data_args\n        )\n\n    def fit(self, dm) -&gt; None:\n        # Dummy classifier: generate zero features just for demonstration\n        train_batches = self.shared_fit(dm)\n        features_list = []\n        y_true_list = []\n        for train_batch in train_batches:\n            features = torch.zeros((train_batch.labels.shape[0], 1)).to('cpu')\n            features_list.append(features)\n            y_true_list.append(train_batch.labels)\n\n        features = torch.cat(features_list, dim=0).numpy()\n        y_true = torch.cat(y_true_list, dim=0).numpy()\n        self.classifier.fit(features, y_true)\n\n    def model_specific_predict(\n        self, dev_batches: list\n    ) -&gt; tuple[list[torch.Tensor], list[None]]:\n        preds_list: list[torch.Tensor] = []\n        probs_list: list[None] = []\n        for dev_batch in dev_batches:\n            features = torch.zeros((dev_batch.labels.shape[0], 1)).to('cpu').numpy()\n            preds = torch.tensor(self.classifier.predict(features))\n            probs = None\n            preds_list.append(preds)\n            probs_list.append(probs)\n        return preds_list, probs_list\n</code></pre>"},{"location":"reference/models/models_ml/#models.models_ml.LinearRegressionRegressorMLModel","title":"<code>LinearRegressionRegressorMLModel</code>","text":"<p>               Bases: <code>BaseMLModel</code></p> <p>Logistic Regression classifier.</p> Source code in <code>src/models/models_ml.py</code> <pre><code>@register_model\nclass LinearRegressionRegressorMLModel(BaseMLModel):\n    \"\"\"\n    Logistic Regression classifier.\n    \"\"\"\n\n    def __init__(\n        self,\n        model_args: LinearRegressionArgs,\n        trainer_args: TrainerML,\n        data_args=DataArgs,\n    ):\n        super().__init__(\n            model_args=model_args, trainer_args=trainer_args, data_args=data_args\n        )\n</code></pre>"},{"location":"reference/models/models_ml/#models.models_ml.LogisticRegressionMLModel","title":"<code>LogisticRegressionMLModel</code>","text":"<p>               Bases: <code>BaseMLModel</code></p> <p>Logistic Regression classifier.</p> Source code in <code>src/models/models_ml.py</code> <pre><code>@register_model\nclass LogisticRegressionMLModel(BaseMLModel):\n    \"\"\"\n    Logistic Regression classifier.\n    \"\"\"\n\n    def __init__(\n        self,\n        model_args: LogisticRegressionMLArgs,\n        trainer_args: TrainerML,\n        data_args=DataArgs,\n    ):\n        super().__init__(\n            model_args=model_args, trainer_args=trainer_args, data_args=data_args\n        )\n</code></pre>"},{"location":"reference/models/models_ml/#models.models_ml.RandomForestMLModel","title":"<code>RandomForestMLModel</code>","text":"<p>               Bases: <code>BaseMLModel</code></p> <p>RandomForest classifier.</p> Source code in <code>src/models/models_ml.py</code> <pre><code>@register_model\nclass RandomForestMLModel(BaseMLModel):\n    \"\"\"\n    RandomForest classifier.\n    \"\"\"\n\n    def __init__(\n        self,\n        model_args: RandomForestMLArgs,\n        trainer_args: TrainerML,\n        data_args=DataArgs,\n    ):\n        super().__init__(\n            model_args=model_args, trainer_args=trainer_args, data_args=data_args\n        )\n</code></pre>"},{"location":"reference/models/models_ml/#models.models_ml.RandomForestRegressorMLModel","title":"<code>RandomForestRegressorMLModel</code>","text":"<p>               Bases: <code>BaseMLModel</code></p> <p>RandomForest Regressor.</p> Source code in <code>src/models/models_ml.py</code> <pre><code>@register_model\nclass RandomForestRegressorMLModel(BaseMLModel):\n    \"\"\"\n    RandomForest Regressor.\n    \"\"\"\n\n    def __init__(\n        self,\n        model_args: RandomForestRegressorMLArgs,\n        trainer_args: TrainerML,\n        data_args=DataArgs,\n    ):\n        super().__init__(\n            model_args=model_args, trainer_args=trainer_args, data_args=data_args\n        )\n</code></pre>"},{"location":"reference/models/models_ml/#models.models_ml.SupportVectorMachineMLModel","title":"<code>SupportVectorMachineMLModel</code>","text":"<p>               Bases: <code>BaseMLModel</code></p> <p>Support Vector Machine classifier.</p> Source code in <code>src/models/models_ml.py</code> <pre><code>@register_model\nclass SupportVectorMachineMLModel(BaseMLModel):\n    \"\"\"\n    Support Vector Machine classifier.\n    \"\"\"\n\n    def __init__(\n        self,\n        model_args: SupportVectorMachineMLArgs,\n        trainer_args: TrainerML,\n        data_args=DataArgs,\n    ):\n        super().__init__(\n            model_args=model_args, trainer_args=trainer_args, data_args=data_args\n        )\n</code></pre>"},{"location":"reference/models/models_ml/#models.models_ml.SupportVectorRegressorMLModel","title":"<code>SupportVectorRegressorMLModel</code>","text":"<p>               Bases: <code>BaseMLModel</code></p> <p>Support Vector Machine regressor.</p> Source code in <code>src/models/models_ml.py</code> <pre><code>@register_model\nclass SupportVectorRegressorMLModel(BaseMLModel):\n    \"\"\"\n    Support Vector Machine regressor.\n    \"\"\"\n\n    def __init__(\n        self,\n        model_args: SupportVectorRegressorMLArgs,\n        trainer_args: TrainerML,\n        data_args=DataArgs,\n    ):\n        super().__init__(\n            model_args=model_args, trainer_args=trainer_args, data_args=data_args\n        )\n</code></pre>"},{"location":"reference/models/models_ml/#models.models_ml.XGBoostMLModel","title":"<code>XGBoostMLModel</code>","text":"<p>               Bases: <code>BaseMLModel</code></p> <p>XGBoost classifier.</p> Source code in <code>src/models/models_ml.py</code> <pre><code>@register_model\nclass XGBoostMLModel(BaseMLModel):\n    \"\"\"\n    XGBoost classifier.\n    \"\"\"\n\n    def __init__(\n        self, model_args: XGBoostMLArgs, trainer_args: TrainerML, data_args=DataArgs\n    ):\n        super().__init__(\n            model_args=model_args, trainer_args=trainer_args, data_args=data_args\n        )\n</code></pre>"},{"location":"reference/models/models_ml/#models.models_ml.XGBoostRegressorMLModel","title":"<code>XGBoostRegressorMLModel</code>","text":"<p>               Bases: <code>BaseMLModel</code></p> <p>XGBoost regressor.</p> Source code in <code>src/models/models_ml.py</code> <pre><code>@register_model\nclass XGBoostRegressorMLModel(BaseMLModel):\n    \"\"\"\n    XGBoost regressor.\n    \"\"\"\n\n    def __init__(\n        self,\n        model_args: XGBoostRegressorMLArgs,\n        trainer_args: TrainerML,\n        data_args=DataArgs,\n    ):\n        super().__init__(\n            model_args=model_args, trainer_args=trainer_args, data_args=data_args\n        )\n</code></pre>"},{"location":"reference/models/plm_as_f_model/","title":"plm_as_f_model","text":""},{"location":"reference/models/plm_as_f_model/#models.plm_as_f_model.PLMASFModel","title":"<code>PLMASFModel</code>","text":"<p>               Bases: <code>BaseModel</code></p> Source code in <code>src/models/plm_as_f_model.py</code> <pre><code>@register_model\nclass PLMASFModel(BaseModel):\n    def __init__(\n        self,\n        model_args: PLMASfArgs,\n        trainer_args: TrainerDL,\n        data_args: DataArgs,\n    ):\n        super().__init__(\n            model_args=model_args, trainer_args=trainer_args, data_args=data_args\n        )\n\n        self.model_args = model_args\n        self.backbone = model_args.backbone\n        self.freeze_bert = model_args.freeze\n\n        self.add_question = data_args.task == PredMode.RC\n\n        self.fast_tokenizer = AutoTokenizer.from_pretrained(self.backbone)\n        self.pad_token_id = self.fast_tokenizer.pad_token_id\n        self.sep_token_id = self.fast_tokenizer.sep_token_id\n        # Cache tokenizer with add_prefix_space for scanpath processing\n        self.fast_tokenizer_prefix = AutoTokenizer.from_pretrained(\n            self.backbone, add_prefix_space=True\n        )\n\n        self.classifier_head = nn.Linear(\n            self.model_args.lstm_hidden_size * 2, self.num_classes\n        )  # *2 for bidirectional\n        self.bert_dim = model_args.text_dim\n\n        encoder_config = AutoConfig.from_pretrained(self.backbone)\n        encoder_config.output_hidden_states = True\n        # initiate Bert with pre-trained weights\n        print('keeping Bert with pre-trained weights')\n        self.bert_encoder = AutoModel.from_pretrained(\n            self.backbone, config=encoder_config\n        )  # type: ignore\n\n        # freeze the parameters in Bert model\n        if self.freeze_bert:\n            for param in self.bert_encoder.parameters():  # type: ignore\n                param.requires_grad = False\n\n        # Create feature index mappings for safe access\n        self.fixation_feature_indices = {\n            name: idx for idx, name in enumerate(self.model_args.fixation_features)\n        }\n        self.eye_feature_indices = {\n            name: idx for idx, name in enumerate(self.model_args.eye_features)\n        }\n\n        # Calculate number of additional features dynamically\n        # Based on organise_sp_fixation_features:\n        # features_by_sp_idx: 8 features (from fixation_features)\n        # features_by_word_idx: 2 features (from eye_features)\n        self.num_features_by_sp_idx = 8\n        self.num_features_by_word_idx = 2\n        self.num_additional_fixations_features = (\n            self.num_features_by_sp_idx + self.num_features_by_word_idx\n        )\n\n        # project bert_dim to bert_dim+num_additional_fixations_features\n        self.projection_layer = nn.Sequential(\n            nn.Linear(\n                self.bert_dim, self.bert_dim + self.num_additional_fixations_features\n            )\n        )\n        # create fse_lstm\n        self.fse_lstm = nn.LSTM(\n            input_size=self.bert_dim + self.num_additional_fixations_features,\n            hidden_size=self.model_args.lstm_hidden_size,\n            num_layers=self.model_args.lstm_num_layers,\n            batch_first=True,\n            bidirectional=True,\n            dropout=self.model_args.lstm_dropout,\n        )\n\n        self.train()\n        self.save_hyperparameters()\n\n    def organise_sp_fixation_features(\n        self, fixation_features: torch.Tensor, ia_features: torch.Tensor\n    ) -&gt; tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"\n        Organize scanpath and fixation features for the model.\n\n        Args:\n            fixation_features: Tensor of fixation features indexed by scanpath position\n            ia_features: Tensor of interest area features indexed by word position\n\n        Returns:\n            features_by_sp_idx: Features indexed by scanpath position (8 features)\n            features_by_word_idx: Features indexed by word position (2 features)\n        \"\"\"\n\n        # Create safe feature getters\n        def get_fix_feat(name: str) -&gt; torch.Tensor:\n            if name not in self.fixation_feature_indices:\n                raise ValueError(\n                    f\"Feature '{name}' not found in fixation_features. \"\n                    f'Available: {list(self.fixation_feature_indices.keys())}'\n                )\n            return fixation_features[..., self.fixation_feature_indices[name]]\n\n        def get_eye_feat(name: str) -&gt; torch.Tensor:\n            if name not in self.eye_feature_indices:\n                raise ValueError(\n                    f\"Feature '{name}' not found in eye_features. \"\n                    f'Available: {list(self.eye_feature_indices.keys())}'\n                )\n            return ia_features[..., self.eye_feature_indices[name]]\n\n        features_by_sp_idx = []\n        features_by_word_idx = []\n\n        # Following the PLMAS-F paper:\n        # 1. Horizontal position of the fixation (CURRENT_FIX_Y) - fixation_features\n        features_by_sp_idx.append(get_fix_feat('CURRENT_FIX_Y'))\n\n        # 2. Total gaze duration (sum of all fixations on the word) (IA_DWELL_TIME) - ia_features\n        features_by_word_idx.append(get_eye_feat('IA_DWELL_TIME'))\n\n        # 3. Landing position of first fixation within the word (IA_FIRST_RUN_LANDING_POSITION) - ia_features\n        # features_by_word_idx.append(ia_features[..., 1])  #! removed because of nans\n\n        # 4. Landing position of last fixation within the word (IA_LAST_RUN_LANDING_POSITION) - ia_features\n        # features_by_word_idx.append(ia_features[..., 2])  #! removed because of nans\n\n        # 5. Duration of first fixation (IA_FIRST_FIXATION_DURATION) - ia_features\n        features_by_word_idx.append(get_eye_feat('IA_FIRST_FIXATION_DURATION'))\n\n        # 6. Duration of outgoing saccade (NEXT_SAC_DURATION) - fixation_features\n        features_by_sp_idx.append(get_fix_feat('NEXT_SAC_DURATION'))\n\n        # 7. Horizontal distance of outgoing saccade (NEXT_SAC_END_X - NEXT_SAC_START_X) - fixation_features\n        features_by_sp_idx.append(\n            get_fix_feat('NEXT_SAC_END_X') - get_fix_feat('NEXT_SAC_START_X')\n        )\n\n        # 8. Vertical distance of outgoing saccade (NEXT_SAC_END_Y - NEXT_SAC_START_Y) - fixation_features\n        features_by_sp_idx.append(\n            get_fix_feat('NEXT_SAC_END_Y') - get_fix_feat('NEXT_SAC_START_Y')\n        )\n\n        # 9. Total distance of outgoing saccade (pitagoras of 7 and 8) - fixation_features\n        features_by_sp_idx.append(\n            torch.sqrt(\n                torch.pow(\n                    get_fix_feat('NEXT_SAC_END_X') - get_fix_feat('NEXT_SAC_START_X'), 2\n                )\n                + torch.pow(\n                    get_fix_feat('NEXT_SAC_END_Y') - get_fix_feat('NEXT_SAC_START_Y'), 2\n                )\n            )\n        )\n\n        # 10. Duration of incoming saccade (NEXT_SAC_DURATION of previous fixation) - fixation_features\n        ## v[i] = v[i-1], v[0] = 0\n        tmp = torch.zeros_like(get_fix_feat('NEXT_SAC_DURATION')).to(\n            fixation_features.device\n        )\n        tmp[..., 1:] = get_fix_feat('NEXT_SAC_DURATION')[..., :-1]\n        features_by_sp_idx.append(tmp)\n\n        # 11. Horizontal distance of incoming saccade (NEXT_SAC_END_X - NEXT_SAC_START_X of previous) - fixation_features\n        prev_sac_end_x = torch.zeros_like(get_fix_feat('NEXT_SAC_END_X')).to(\n            fixation_features.device\n        )\n        prev_sac_end_x[..., 1:] = get_fix_feat('NEXT_SAC_END_X')[..., :-1]\n        prev_sac_start_x = torch.zeros_like(get_fix_feat('NEXT_SAC_START_X')).to(\n            fixation_features.device\n        )\n        prev_sac_start_x[..., 1:] = get_fix_feat('NEXT_SAC_START_X')[..., :-1]\n        features_by_sp_idx.append(prev_sac_end_x - prev_sac_start_x)\n\n        # 12. Vertical distance of incoming saccade (NEXT_SAC_END_Y - NEXT_SAC_START_Y of previous) - fixation_features\n        prev_sac_end_y = torch.zeros_like(get_fix_feat('NEXT_SAC_END_Y')).to(\n            fixation_features.device\n        )\n        prev_sac_end_y[..., 1:] = get_fix_feat('NEXT_SAC_END_Y')[..., :-1]\n        prev_sac_start_y = torch.zeros_like(get_fix_feat('NEXT_SAC_START_Y')).to(\n            fixation_features.device\n        )\n        prev_sac_start_y[..., 1:] = get_fix_feat('NEXT_SAC_START_Y')[..., :-1]\n        features_by_sp_idx.append(prev_sac_end_y - prev_sac_start_y)\n\n        features_by_sp_idx_tensor = torch.stack(features_by_sp_idx, dim=2)\n        features_by_word_idx_tensor = torch.stack(features_by_word_idx, dim=2)\n\n        return features_by_sp_idx_tensor, features_by_word_idx_tensor\n\n    def split_context_embeds(\n        self,\n        encoded_word_seq: torch.Tensor,\n        input_ids: torch.Tensor,\n    ) -&gt; tuple[\n        Union[torch.Tensor, None], Union[torch.Tensor, None], Union[torch.Tensor, None]\n    ]:\n        # Find the positions of the separator tokens\n        sep_positions = (\n            (input_ids == self.sep_token_id).nonzero(as_tuple=True)[0].cpu().numpy()\n        )\n\n        # Calculate the sizes of the splits\n        split_sizes = np.diff(a=sep_positions, prepend=0, append=input_ids.size(dim=0))\n        assert split_sizes.sum() == input_ids.size(dim=0), (\n            f'split_sizes.sum(): {split_sizes.sum()}'\n        )\n        if self.add_question:\n            assert split_sizes.size == 4, (\n                f'split_sizes.size: {split_sizes.size} but expected 4'\n            )\n            # Split the encoded_word_seq tensor at the separator positions\n            p, _, q, _ = torch.split(\n                tensor=encoded_word_seq,\n                split_size_or_sections=split_sizes.tolist(),\n                dim=0,\n            )\n            a = torch.zeros_like(q)\n\n        else:\n            # only paragraph\n            assert split_sizes.size == 4, (\n                f'split_sizes.size: {split_sizes.size} but expected 4'\n            )\n            # Split the encoded_word_seq tensor at the separator positions\n            p, _, _, _ = torch.split(\n                tensor=encoded_word_seq,\n                split_size_or_sections=split_sizes.tolist(),\n                dim=0,\n            )\n            # q = torch.zeros_like(p)\n            # q = q[1:, :].mean(dim=0)\n            # a = torch.zeros_like(p)\n            q = None\n            a = None\n\n        return p, q, a\n\n    def split_context_embds_batched(self, encoded_word_seq, input_ids):\n        p_embds, q_embds, a_embeds, p_masks, q_masks, a_masks = (\n            None,\n            None,\n            None,\n            None,\n            None,\n            None,\n        )  # initialize so that the return variables are defined\n        p_embds_batches, q_embds_batches, a_embeds_batches = [], [], []\n        # Process each batch separately\n        for ewsb, iib in zip(encoded_word_seq, input_ids):\n            p_embds, q_embds, a_embeds = self.split_context_embeds(\n                encoded_word_seq=ewsb, input_ids=iib\n            )\n\n            p_embds_batches.append(p_embds)\n            q_embds_batches.append(q_embds)\n            a_embeds_batches.append(a_embeds)\n\n        # pad the embeddings to the maximum sequence length of each list\n        p_max_len = self.actual_max_needed_len\n        p_masks = torch.stack(\n            [\n                torch.cat([torch.ones(p.shape[0]), torch.zeros(p_max_len - p.shape[0])])\n                for p in p_embds_batches\n            ],\n            dim=0,\n        )\n        p_embds_batches = [\n            F.pad(p, (0, 0, 0, p_max_len - p.shape[0])) for p in p_embds_batches\n        ]\n        p_embds = torch.stack(p_embds_batches, dim=0)\n\n        if self.add_question:\n            q_max_len = max([q.shape[0] for q in q_embds_batches])\n            q_masks = torch.stack(\n                [\n                    torch.cat(\n                        [torch.ones(q.shape[0]), torch.zeros(q_max_len - q.shape[0])]\n                    )\n                    for q in q_embds_batches\n                ],\n                dim=0,\n            )\n            q_embds_batches = [\n                F.pad(q, (0, 0, 0, q_max_len - q.shape[0])) for q in q_embds_batches\n            ]\n            q_embds = torch.stack(q_embds_batches, dim=0)\n\n        return p_embds, q_embds, q_embds, p_masks, q_masks, a_masks\n\n    def forward(\n        self,\n        input_ids,\n        input_masks,\n        p_input_ids,\n        scanpath,\n        fixation_features,\n        scanpath_pads,\n        eyes,\n    ):\n        assert (\n            input_ids[:, 0].sum().item() == 0\n        )  # The CLS token is always present first (and 0 in roberta)\n\n        scanpath, fixation_features = trim_scanpath_and_fixation_features(\n            scanpath=scanpath,\n            fixation_features=fixation_features,\n            scanpath_pads=scanpath_pads,\n            max_scanpath_length=self.max_scanpath_length,\n        )\n\n        features_by_sp_idx, features_by_word_idx = self.organise_sp_fixation_features(\n            fixation_features=fixation_features,\n            ia_features=eyes,\n        )\n\n        # scanpath masks --------------------------\n        scanpath_masks = torch.ones_like(scanpath)\n        scanpath_masks[scanpath == self.pad_token_id] = 0\n        # -----------------------------------------\n\n        # get the decoded texts from the input_ids\n        p_input_ids_decoded_txts = self.fast_tokenizer.batch_decode(\n            p_input_ids, return_tensors='pt'\n        )  # previously decoded_to_txt_input_ids\n        # -----------------------------------------\n\n        paragraph_token_to_word_idxs_in_p_text = (\n            plm_as_model.align_word_ids_with_input_ids(\n                tokenizer=self.fast_tokenizer,\n                input_ids=p_input_ids,\n                decoded_to_txt_input_ids=p_input_ids_decoded_txts,\n            )\n        )  # previously word_ids_sn\n\n        # in the decoded texts, space between &lt;pad&gt;&lt;pad&gt;, &lt;pad&gt;&lt;s&gt;, etc.\n        p_input_ids_decoded_txts = list(\n            map(\n                lambda x: x.replace('&lt;', ' &lt;').split(' ')[\n                    1:\n                ],  # &lt;s&gt; is the first token so, when doing the replace, we get \" &lt;s&gt;\", so we split by space and take the second element\n                p_input_ids_decoded_txts,\n            )\n        )\n\n        scanpath_token_to_word_idxs_in_sp_text, scanpath_input_ids = (\n            plm_as_model.calc_sp_word_input_ids(\n                input_ids=p_input_ids,\n                decoded_to_txt_input_ids=p_input_ids_decoded_txts,\n                roberta_tokenizer_prefix_space=self.fast_tokenizer_prefix,\n                scanpath=scanpath,\n            )\n        )  # previously word_ids_sp, sp_input_ids\n\n        # 1. encode the whole sequence (input_ids)\n        # 2. then split the embeddings into paragraph, question, and answer embeddings\n        # 3. reorder the paragraph embeddings according to the scanpath\n        # 4. combine the paragraph embeddings with features_by_sp_idx, features_by_word_idx\n        # 5. combine parts into a sequence of embeddings to feed the lstm\n\n        # (1) encode the whole sequence\n        with torch.no_grad():\n            outputs = self.bert_encoder(input_ids=input_ids, attention_mask=input_masks)\n            #  Make the embedding of the &lt;pad&gt; token to be zeros\n        outputs.last_hidden_state[input_ids == self.pad_token_id] = 0\n        encoded_input_ids = outputs.last_hidden_state\n\n        # (2) split the embeddings into paragraph, question, and answer embeddings\n        p_embds, q_embds, a_embeds, p_masks, q_masks, a_masks = (\n            self.split_context_embds_batched(encoded_input_ids, input_ids)\n        )  # embds shape: [batch, seq_len, emb_dim]\n\n        # (3) reorder the paragraph embeddings according to the scanpath\n        # Pool bert subword to word level for english corpus\n        merged_p_emb, p_mask = plm_as_model.pool_subword_to_word(\n            p_embds,\n            paragraph_token_to_word_idxs_in_p_text,\n            target='sn',\n            max_seq_len=self.actual_max_needed_len,\n            bert_dim=self.bert_dim,\n            pool_method='sum',\n        )\n        batch_index = torch.arange(scanpath.shape[0]).unsqueeze(1).expand_as(scanpath)\n        scanpath_add1 = scanpath.clone()\n        scanpath_add1[scanpath != SCANPATH_PADDING_VAL] += SCANPATH_PADDING_VAL\n        p_embds_sp_order = merged_p_emb[\n            batch_index, scanpath_add1\n        ]  # [batch, max_sp_length, emb_dim]\n\n        # note that p_embds_sp_order should never contain the embeddings of the &lt;CLS&gt; token\n\n        # (4) combine the paragraph embeddings with features_by_sp_idx, features_by_word_idx\n        if features_by_sp_idx is not None:\n            p_embds_sp_order = torch.cat([p_embds_sp_order, features_by_sp_idx], dim=2)\n        if features_by_word_idx is not None:\n            p_embds_sp_order = torch.cat(\n                [p_embds_sp_order, features_by_word_idx[batch_index, scanpath]], dim=2\n            )  # we don't need scanpath_add1 here because no &lt;s&gt; token in the beginning\n\n        # (5) combine parts into a sequence of embeddings to feed the lstm\n        if (\n            self.prediction_mode\n            in BINARY_PARAGRAPH_ONLY_TASKS\n            + BINARY_P_AND_Q_TASKS\n            + REGRESSION_PARAGRAPH_ONLY_TASKS\n        ):\n            final_seq_embds = p_embds_sp_order\n            final_seq_lens = (\n                (scanpath != SCANPATH_PADDING_VAL).sum(dim=1).clone().detach()\n            )\n\n        else:\n            raise NotImplementedError(\n                f'Not implemented for prediction_mode: {self.prediction_mode}'\n            )\n\n        # pass through the LSTM layer\n        sorted_lengths, indices = torch.sort(final_seq_lens, descending=True)\n        final_seq_embds = final_seq_embds[\n            indices\n        ]  # reorder sequences according to the descending order of the lengths\n\n        # Pass the entire sequence through the LSTM layer\n        packed_final_seq_embds = nn.utils.rnn.pack_padded_sequence(\n            input=final_seq_embds,\n            lengths=sorted_lengths.to('cpu'),\n            batch_first=True,\n            enforce_sorted=True,\n        )\n\n        packed_output, (hn, cn) = self.fse_lstm(packed_final_seq_embds)\n        unpacked_output = nn.utils.rnn.unpack_sequence(packed_output)\n\n        # unpacked output is a list of batch_size tensors,\n        # each tensor is shaped [seq_len, no.directions*hidden_size]\n        # create a tensor of shape [batch_size, no.directions*hidden_size]\n        # by taking the mean of each element in the list\n        lstm_mean_hidden = torch.stack([torch.mean(t, dim=0) for t in unpacked_output])\n\n        # reorder the hidden states to the original order\n        lstm_mean_hidden = lstm_mean_hidden[\n            torch.argsort(indices)\n        ]  # Tested. Reorders correctly\n\n        logits = self.classifier_head(lstm_mean_hidden)\n\n        return logits, lstm_mean_hidden\n\n    def shared_step(\n        self,\n        batch: list,\n    ) -&gt; tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n        \"\"\"\n\n        Args:\n            batch (tuple): _description_\n\n        Returns:\n            tuple[torch.Tensor, torch.Tensor, torch.Tensor]: _description_\n\n        Notes:\n            - in input ids: 0 is for CLS, 2 is for SEP, 1 is for PAD\n        \"\"\"\n        batch_data = self.unpack_batch(batch)\n        assert batch_data.input_ids is not None, 'input_ids not in batch_dict'\n        assert batch_data.input_masks is not None, 'input_masks not in batch_dict'\n        assert batch_data.scanpath is not None, 'scanpath not in batch_dict'\n        assert batch_data.fixation_features is not None, 'eyes_tensor not in batch_dict'\n        assert batch_data.scanpath_pads is not None, 'scanpath_pads not in batch_dict'\n        assert batch_data.eyes is not None, 'eye not in batch_dict'\n        assert batch_data.p_input_ids is not None, 'p_input_ids not in batch_dict'\n\n        # -----------------------------------------\n\n        logits, _ = self.forward(\n            input_ids=batch_data.input_ids,\n            input_masks=batch_data.input_masks,\n            p_input_ids=batch_data.p_input_ids,\n            scanpath=batch_data.scanpath,\n            fixation_features=batch_data.fixation_features,\n            scanpath_pads=batch_data.scanpath_pads,\n            eyes=batch_data.eyes,\n        )\n\n        labels = batch_data.labels\n\n        if logits.ndim == 1:\n            logits = logits.unsqueeze(0)\n        loss = self.loss(logits, labels)\n\n        return labels, loss, logits.squeeze()\n</code></pre>"},{"location":"reference/models/plm_as_f_model/#models.plm_as_f_model.PLMASFModel.organise_sp_fixation_features","title":"<code>organise_sp_fixation_features(fixation_features, ia_features)</code>","text":"<p>Organize scanpath and fixation features for the model.</p> <p>Parameters:</p> Name Type Description Default <code>fixation_features</code> <code>Tensor</code> <p>Tensor of fixation features indexed by scanpath position</p> required <code>ia_features</code> <code>Tensor</code> <p>Tensor of interest area features indexed by word position</p> required <p>Returns:</p> Name Type Description <code>features_by_sp_idx</code> <code>Tensor</code> <p>Features indexed by scanpath position (8 features)</p> <code>features_by_word_idx</code> <code>Tensor</code> <p>Features indexed by word position (2 features)</p> Source code in <code>src/models/plm_as_f_model.py</code> <pre><code>def organise_sp_fixation_features(\n    self, fixation_features: torch.Tensor, ia_features: torch.Tensor\n) -&gt; tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Organize scanpath and fixation features for the model.\n\n    Args:\n        fixation_features: Tensor of fixation features indexed by scanpath position\n        ia_features: Tensor of interest area features indexed by word position\n\n    Returns:\n        features_by_sp_idx: Features indexed by scanpath position (8 features)\n        features_by_word_idx: Features indexed by word position (2 features)\n    \"\"\"\n\n    # Create safe feature getters\n    def get_fix_feat(name: str) -&gt; torch.Tensor:\n        if name not in self.fixation_feature_indices:\n            raise ValueError(\n                f\"Feature '{name}' not found in fixation_features. \"\n                f'Available: {list(self.fixation_feature_indices.keys())}'\n            )\n        return fixation_features[..., self.fixation_feature_indices[name]]\n\n    def get_eye_feat(name: str) -&gt; torch.Tensor:\n        if name not in self.eye_feature_indices:\n            raise ValueError(\n                f\"Feature '{name}' not found in eye_features. \"\n                f'Available: {list(self.eye_feature_indices.keys())}'\n            )\n        return ia_features[..., self.eye_feature_indices[name]]\n\n    features_by_sp_idx = []\n    features_by_word_idx = []\n\n    # Following the PLMAS-F paper:\n    # 1. Horizontal position of the fixation (CURRENT_FIX_Y) - fixation_features\n    features_by_sp_idx.append(get_fix_feat('CURRENT_FIX_Y'))\n\n    # 2. Total gaze duration (sum of all fixations on the word) (IA_DWELL_TIME) - ia_features\n    features_by_word_idx.append(get_eye_feat('IA_DWELL_TIME'))\n\n    # 3. Landing position of first fixation within the word (IA_FIRST_RUN_LANDING_POSITION) - ia_features\n    # features_by_word_idx.append(ia_features[..., 1])  #! removed because of nans\n\n    # 4. Landing position of last fixation within the word (IA_LAST_RUN_LANDING_POSITION) - ia_features\n    # features_by_word_idx.append(ia_features[..., 2])  #! removed because of nans\n\n    # 5. Duration of first fixation (IA_FIRST_FIXATION_DURATION) - ia_features\n    features_by_word_idx.append(get_eye_feat('IA_FIRST_FIXATION_DURATION'))\n\n    # 6. Duration of outgoing saccade (NEXT_SAC_DURATION) - fixation_features\n    features_by_sp_idx.append(get_fix_feat('NEXT_SAC_DURATION'))\n\n    # 7. Horizontal distance of outgoing saccade (NEXT_SAC_END_X - NEXT_SAC_START_X) - fixation_features\n    features_by_sp_idx.append(\n        get_fix_feat('NEXT_SAC_END_X') - get_fix_feat('NEXT_SAC_START_X')\n    )\n\n    # 8. Vertical distance of outgoing saccade (NEXT_SAC_END_Y - NEXT_SAC_START_Y) - fixation_features\n    features_by_sp_idx.append(\n        get_fix_feat('NEXT_SAC_END_Y') - get_fix_feat('NEXT_SAC_START_Y')\n    )\n\n    # 9. Total distance of outgoing saccade (pitagoras of 7 and 8) - fixation_features\n    features_by_sp_idx.append(\n        torch.sqrt(\n            torch.pow(\n                get_fix_feat('NEXT_SAC_END_X') - get_fix_feat('NEXT_SAC_START_X'), 2\n            )\n            + torch.pow(\n                get_fix_feat('NEXT_SAC_END_Y') - get_fix_feat('NEXT_SAC_START_Y'), 2\n            )\n        )\n    )\n\n    # 10. Duration of incoming saccade (NEXT_SAC_DURATION of previous fixation) - fixation_features\n    ## v[i] = v[i-1], v[0] = 0\n    tmp = torch.zeros_like(get_fix_feat('NEXT_SAC_DURATION')).to(\n        fixation_features.device\n    )\n    tmp[..., 1:] = get_fix_feat('NEXT_SAC_DURATION')[..., :-1]\n    features_by_sp_idx.append(tmp)\n\n    # 11. Horizontal distance of incoming saccade (NEXT_SAC_END_X - NEXT_SAC_START_X of previous) - fixation_features\n    prev_sac_end_x = torch.zeros_like(get_fix_feat('NEXT_SAC_END_X')).to(\n        fixation_features.device\n    )\n    prev_sac_end_x[..., 1:] = get_fix_feat('NEXT_SAC_END_X')[..., :-1]\n    prev_sac_start_x = torch.zeros_like(get_fix_feat('NEXT_SAC_START_X')).to(\n        fixation_features.device\n    )\n    prev_sac_start_x[..., 1:] = get_fix_feat('NEXT_SAC_START_X')[..., :-1]\n    features_by_sp_idx.append(prev_sac_end_x - prev_sac_start_x)\n\n    # 12. Vertical distance of incoming saccade (NEXT_SAC_END_Y - NEXT_SAC_START_Y of previous) - fixation_features\n    prev_sac_end_y = torch.zeros_like(get_fix_feat('NEXT_SAC_END_Y')).to(\n        fixation_features.device\n    )\n    prev_sac_end_y[..., 1:] = get_fix_feat('NEXT_SAC_END_Y')[..., :-1]\n    prev_sac_start_y = torch.zeros_like(get_fix_feat('NEXT_SAC_START_Y')).to(\n        fixation_features.device\n    )\n    prev_sac_start_y[..., 1:] = get_fix_feat('NEXT_SAC_START_Y')[..., :-1]\n    features_by_sp_idx.append(prev_sac_end_y - prev_sac_start_y)\n\n    features_by_sp_idx_tensor = torch.stack(features_by_sp_idx, dim=2)\n    features_by_word_idx_tensor = torch.stack(features_by_word_idx, dim=2)\n\n    return features_by_sp_idx_tensor, features_by_word_idx_tensor\n</code></pre>"},{"location":"reference/models/plm_as_f_model/#models.plm_as_f_model.PLMASFModel.shared_step","title":"<code>shared_step(batch)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>batch</code> <code>tuple</code> <p>description</p> required <p>Returns:</p> Type Description <code>tuple[Tensor, Tensor, Tensor]</code> <p>tuple[torch.Tensor, torch.Tensor, torch.Tensor]: description</p> Notes <ul> <li>in input ids: 0 is for CLS, 2 is for SEP, 1 is for PAD</li> </ul> Source code in <code>src/models/plm_as_f_model.py</code> <pre><code>def shared_step(\n    self,\n    batch: list,\n) -&gt; tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n\n    Args:\n        batch (tuple): _description_\n\n    Returns:\n        tuple[torch.Tensor, torch.Tensor, torch.Tensor]: _description_\n\n    Notes:\n        - in input ids: 0 is for CLS, 2 is for SEP, 1 is for PAD\n    \"\"\"\n    batch_data = self.unpack_batch(batch)\n    assert batch_data.input_ids is not None, 'input_ids not in batch_dict'\n    assert batch_data.input_masks is not None, 'input_masks not in batch_dict'\n    assert batch_data.scanpath is not None, 'scanpath not in batch_dict'\n    assert batch_data.fixation_features is not None, 'eyes_tensor not in batch_dict'\n    assert batch_data.scanpath_pads is not None, 'scanpath_pads not in batch_dict'\n    assert batch_data.eyes is not None, 'eye not in batch_dict'\n    assert batch_data.p_input_ids is not None, 'p_input_ids not in batch_dict'\n\n    # -----------------------------------------\n\n    logits, _ = self.forward(\n        input_ids=batch_data.input_ids,\n        input_masks=batch_data.input_masks,\n        p_input_ids=batch_data.p_input_ids,\n        scanpath=batch_data.scanpath,\n        fixation_features=batch_data.fixation_features,\n        scanpath_pads=batch_data.scanpath_pads,\n        eyes=batch_data.eyes,\n    )\n\n    labels = batch_data.labels\n\n    if logits.ndim == 1:\n        logits = logits.unsqueeze(0)\n    loss = self.loss(logits, labels)\n\n    return labels, loss, logits.squeeze()\n</code></pre>"},{"location":"reference/models/plm_as_model/","title":"plm_as_model","text":"<p>PLM-AS model for fixation sequence encoding and classification.</p>"},{"location":"reference/models/plm_as_model/#models.plm_as_model.PLMASModel","title":"<code>PLMASModel</code>","text":"<p>               Bases: <code>BaseModel</code></p> Source code in <code>src/models/plm_as_model.py</code> <pre><code>@register_model\nclass PLMASModel(BaseModel):\n    def __init__(\n        self,\n        model_args: PLMASArgs,\n        trainer_args: TrainerDL,\n        data_args: DataArgs,\n    ):\n        super().__init__(\n            model_args=model_args, trainer_args=trainer_args, data_args=data_args\n        )\n\n        self.model_args = model_args\n        self.backbone = model_args.backbone\n        self.freeze_bert = model_args.freeze\n\n        self.fast_tokenizer = AutoTokenizer.from_pretrained(self.backbone)\n        self.pad_token_id = self.fast_tokenizer.pad_token_id\n        # Cache tokenizer with add_prefix_space for scanpath processing\n        self.fast_tokenizer_prefix = AutoTokenizer.from_pretrained(\n            self.backbone, add_prefix_space=True\n        )\n\n        # ? self.preorder = False\n\n        self.classifier_head = nn.Linear(model_args.text_dim, self.num_classes)\n        self.bert_dim = model_args.text_dim\n\n        encoder_config = AutoConfig.from_pretrained(self.backbone)\n        encoder_config.output_hidden_states = True\n        # initiate Bert with pre-trained weights\n        print('keeping Bert with pre-trained weights')\n        self.bert_encoder = AutoModel.from_pretrained(\n            self.backbone, config=encoder_config\n        )  # type: ignore\n\n        # freeze the parameters in Bert model\n        if self.freeze_bert:\n            for param in self.bert_encoder.parameters():  # type: ignore\n                param.requires_grad = False\n\n        # create fse_lstm\n        self.fse_lstm = nn.GRU(\n            input_size=self.bert_dim,\n            hidden_size=self.bert_dim,\n            num_layers=self.model_args.lstm_num_layers,\n            batch_first=True,\n            bidirectional=False,\n            dropout=self.model_args.lstm_dropout,\n        )\n\n        self.train()\n        self.save_hyperparameters()\n\n    def fixation_sequence_encoder(\n        self,\n        sn_emd,\n        sn_mask,\n        word_ids_sn,\n        sn_word_len,\n        sp_emd,\n        sp_pos,\n        sp_fix_dur,\n        sp_landing_pos,\n        word_ids_sp,\n        scanpath,\n        features_by_sp_idx=None,\n        features_by_word_idx=None,\n    ):\n        \"\"\"A LSTM based encoder for the fixation sequence (scanpath)\n        Args:\n            sp_emd (torch.Tensor): A tensor containing the text input_ids ordered according to the scanpath\n            sp_pos (torch.Tensor): The word index of each fixation in the scanpath (the word the fixation is on)\n            sp_fix_dur (torch.Tensor): The total fixation duration of each word in the scanpath (fixation)\n            sp_landing_pos (torch.Tensor): The landing position of each word in the scanpath (fixation)\n            sp_mask (torch.Tensor): The mask for the scanpath\n            word_ids_sp (torch.Tensor): The word index of each input_id in the scanpath\n            scanpath (torch.Tensor): The scanpath tensor\n\n        \"\"\"\n\n        # used for computing sp_merged_word_mask\n        # x = self.bert_encoder.embeddings.word_embeddings(sp_emd)\n        # x[sp_emd == self.pad_token_id] = 0\n        # # Pool bert subword to word level for English corpus\n        # _, sp_merged_word_mask = self.pool_subword_to_word(\n        #     x, word_ids_sp, target='sp', pool_method='sum'\n        # )\n\n        with torch.no_grad():\n            outputs = self.bert_encoder(input_ids=sn_emd, attention_mask=sn_mask)\n        #  Make the embedding of the &lt;pad&gt; token to be zeros\n        outputs.last_hidden_state[sn_emd == self.pad_token_id] = 0\n        # Pool bert subword to word level for english corpus\n        merged_word_emb, sn_mask_word = pool_subword_to_word(\n            outputs.last_hidden_state,\n            word_ids_sn,\n            target='sn',\n            max_seq_len=self.actual_max_needed_len,\n            bert_dim=self.bert_dim,\n            pool_method='sum',\n        )\n        batch_index = torch.arange(scanpath.shape[0]).unsqueeze(1).expand_as(scanpath)\n        scanpath_add1 = scanpath.clone()\n        scanpath_add1[scanpath != SCANPATH_PADDING_VAL] += SCANPATH_PADDING_VAL\n        x = merged_word_emb[\n            batch_index, scanpath_add1\n        ]  # [batch, max_sp_length, emb_dim], word_emb_sn\n\n        if features_by_sp_idx is not None:\n            x = torch.cat([x, features_by_sp_idx], dim=2)\n        if features_by_word_idx is not None:\n            x = torch.cat(\n                [x, features_by_word_idx[batch_index, scanpath]], dim=2\n            )  # we don't need scanpath_add1 here because no &lt;s&gt; token in the beginning\n\n        # pass through the LSTM layer\n        sorted_lengths, indices = torch.sort(\n            (scanpath != SCANPATH_PADDING_VAL).sum(dim=1), descending=True\n        )\n        x = x[\n            indices\n        ]  # reorder sequences according to the descending order of the lengths\n\n        # Pass the entire sequence through the LSTM layer\n        packed_x = nn.utils.rnn.pack_padded_sequence(\n            input=x,\n            lengths=sorted_lengths.to('cpu'),\n            batch_first=True,\n            enforce_sorted=True,\n        )\n\n        # set h0 as [CLS] token embedding of each sequence\n        h0 = outputs.last_hidden_state[indices, 0]\n\n        # fit h0 and c0 to the shape of the LSTM\n        # h0 is [batch, hidden_size] where h0[i] is the initial hidden state for the sequence i\n        h0 = h0.unsqueeze(0).repeat(self.fse_lstm.num_layers, 1, 1)\n        c0 = torch.zeros_like(h0)\n        # ensure contiguous\n        h0 = h0.contiguous()\n        c0 = c0.contiguous()\n\n        # * c0 isn't used when using GRU\n\n        packed_output, ht = self.fse_lstm(packed_x, h0)\n        lstm_last_hidden = ht[-1].squeeze(\n            1\n        )  # Take the hidden state of the last LSTM layer.\n\n        # reorder the hidden states to the original order\n        lstm_last_hidden = lstm_last_hidden[\n            torch.argsort(indices)\n        ]  # Tested. Reorders correctly\n\n        # Clear unused tensors to free memory\n        del outputs, packed_x, packed_output, ht, x, merged_word_emb\n\n        return lstm_last_hidden  # Take the hidden state of the 8th LSTM layer.\n\n    def forward(\n        self,\n        sn_emd,\n        sn_mask,\n        word_ids_sn,\n        sn_word_len,\n        sp_emd,  # (Batch, Maximum length of the scanpath in TOKENS + 1)\n        sp_pos,  # (Batch, Scanpath_length + 1) The +1 is for the &lt;s&gt; token in the beginning\n        sp_fix_dur,  # (Batch, Scanpath_length + 1) The +1 is for the &lt;s&gt; token in the beginning\n        sp_landing_pos,  # (Batch, Scanpath_length + 1) The +1 is for the &lt;s&gt; token in the beginning\n        word_ids_sp,  # (Batch, Maximum length of the scanpath in TOKENS + 1)\n        scanpath,  # (Batch, Maximum length of the scanpath in WORDS)\n        features_by_sp_idx=None,\n        features_by_word_idx=None,\n    ):\n        assert (\n            sn_emd[:, 0].sum().item() == 0\n        )  # The CLS token is always present first (and 0 in roberta)\n\n        fse_output = self.fixation_sequence_encoder(\n            sn_emd=sn_emd,\n            sn_mask=sn_mask,\n            word_ids_sn=word_ids_sn,\n            sn_word_len=sn_word_len,\n            sp_emd=sp_emd,\n            sp_pos=sp_pos,\n            sp_fix_dur=sp_fix_dur,\n            sp_landing_pos=sp_landing_pos,\n            word_ids_sp=word_ids_sp,\n            scanpath=scanpath,\n            features_by_sp_idx=features_by_sp_idx,\n            features_by_word_idx=features_by_word_idx,\n        )  # [batch, step, dec_o_dim]\n\n        pred = self.classifier_head(fse_output)\n\n        return pred, fse_output\n\n    def shared_step(\n        self,\n        batch: list,\n    ) -&gt; tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n        \"\"\"\n\n        Args:\n            batch (tuple): _description_\n\n        Returns:\n            tuple[torch.Tensor, torch.Tensor, torch.Tensor]: _description_\n\n        Notes:\n            - in input ids: 0 is for CLS, 2 is for SEP, 1 is for PAD\n        \"\"\"\n        batch_data = self.unpack_batch(batch)\n        assert batch_data.input_ids is not None, 'input_ids not in batch_dict'\n        assert batch_data.input_masks is not None, 'input_masks not in batch_dict'\n        assert batch_data.scanpath is not None, 'scanpath not in batch_dict'\n        assert batch_data.fixation_features is not None, 'eyes_tensor not in batch_dict'\n        assert batch_data.scanpath_pads is not None, 'scanpath_pads not in batch_dict'\n        assert batch_data.eyes is not None, 'eye not in batch_dict'\n\n        shortest_scanpath_pad = batch_data.scanpath_pads.min()\n        longest_batch_scanpath: int = int(\n            self.max_scanpath_length - shortest_scanpath_pad\n        )\n\n        scanpath = batch_data.scanpath[..., :longest_batch_scanpath]\n        fixation_features = batch_data.fixation_features[\n            ..., :longest_batch_scanpath, :\n        ]\n\n        # scanpath masks\n        sp_masks = torch.ones_like(scanpath)\n        sp_masks[scanpath == self.pad_token_id] = 0\n\n        decoded_to_txt_input_ids = self.fast_tokenizer.batch_decode(\n            batch_data.input_ids, return_tensors='pt'\n        )\n\n        word_ids_sn = align_word_ids_with_input_ids(\n            tokenizer=self.fast_tokenizer,\n            input_ids=batch_data.input_ids,\n            decoded_to_txt_input_ids=decoded_to_txt_input_ids,\n        )\n\n        # in the decoded texts, space between &lt;pad&gt;&lt;pad&gt;, &lt;pad&gt;&lt;s&gt;, etc.\n        decoded_to_txt_input_ids = list(\n            map(\n                lambda x: x.replace('&lt;', ' &lt;').split(' ')[1:],\n                decoded_to_txt_input_ids,\n            )\n        )\n\n        sn_word_len = get_sn_word_lens(\n            input_ids=batch_data.input_ids,\n            decoded_to_txt_input_ids=decoded_to_txt_input_ids,\n        )\n\n        word_ids_sp, sp_input_ids = calc_sp_word_input_ids(\n            input_ids=batch_data.input_ids,\n            decoded_to_txt_input_ids=decoded_to_txt_input_ids,\n            roberta_tokenizer_prefix_space=self.fast_tokenizer_prefix,\n            scanpath=scanpath,\n        )\n\n        sp_pos, sp_fix_dur, sp_landing_pos = eyettention_legacy_code(\n            scanpath=scanpath,\n            fixation_features=fixation_features,\n        )\n\n        sn_embed = batch_data.input_ids\n        sn_mask = batch_data.input_masks\n        # if the second dimension of the scanpath is more than the maximum context length (of self.bert_encoder), cut it and notify\n        bert_encoder_max_len = self.bert_encoder.config.max_position_embeddings\n        if sp_input_ids.shape[1] &gt; bert_encoder_max_len - 1:\n            # print(\n            #     f'Text length is more than the maximum context length of the model ({bert_encoder_max_len}). Cutting from the BEGINNING of the text to max length.'\n            # )\n            sn_embed = sn_embed[:, : bert_encoder_max_len - 1]\n            sn_mask = sn_mask[:, : bert_encoder_max_len - 1]\n\n        logits, _ = self(\n            sn_emd=sn_embed,\n            sn_mask=sn_mask,\n            word_ids_sn=word_ids_sn,\n            sn_word_len=sn_word_len,\n            sp_emd=sp_input_ids,\n            sp_pos=sp_pos,\n            sp_fix_dur=sp_fix_dur,\n            sp_landing_pos=sp_landing_pos,\n            word_ids_sp=word_ids_sp,\n            scanpath=scanpath,\n        )\n\n        labels = batch_data.labels\n\n        if logits.ndim == 1:\n            logits = logits.unsqueeze(0)\n        loss = self.loss(logits, labels)\n\n        # Clear intermediate tensors to prevent memory buildup\n        del scanpath, fixation_features, sp_input_ids, word_ids_sp\n        del sp_pos, sp_fix_dur, sp_landing_pos, sn_embed, sn_mask\n        del word_ids_sn, sn_word_len\n\n        return labels, loss, logits.squeeze()\n</code></pre>"},{"location":"reference/models/plm_as_model/#models.plm_as_model.PLMASModel.fixation_sequence_encoder","title":"<code>fixation_sequence_encoder(sn_emd, sn_mask, word_ids_sn, sn_word_len, sp_emd, sp_pos, sp_fix_dur, sp_landing_pos, word_ids_sp, scanpath, features_by_sp_idx=None, features_by_word_idx=None)</code>","text":"<p>A LSTM based encoder for the fixation sequence (scanpath) Args:     sp_emd (torch.Tensor): A tensor containing the text input_ids ordered according to the scanpath     sp_pos (torch.Tensor): The word index of each fixation in the scanpath (the word the fixation is on)     sp_fix_dur (torch.Tensor): The total fixation duration of each word in the scanpath (fixation)     sp_landing_pos (torch.Tensor): The landing position of each word in the scanpath (fixation)     sp_mask (torch.Tensor): The mask for the scanpath     word_ids_sp (torch.Tensor): The word index of each input_id in the scanpath     scanpath (torch.Tensor): The scanpath tensor</p> Source code in <code>src/models/plm_as_model.py</code> <pre><code>def fixation_sequence_encoder(\n    self,\n    sn_emd,\n    sn_mask,\n    word_ids_sn,\n    sn_word_len,\n    sp_emd,\n    sp_pos,\n    sp_fix_dur,\n    sp_landing_pos,\n    word_ids_sp,\n    scanpath,\n    features_by_sp_idx=None,\n    features_by_word_idx=None,\n):\n    \"\"\"A LSTM based encoder for the fixation sequence (scanpath)\n    Args:\n        sp_emd (torch.Tensor): A tensor containing the text input_ids ordered according to the scanpath\n        sp_pos (torch.Tensor): The word index of each fixation in the scanpath (the word the fixation is on)\n        sp_fix_dur (torch.Tensor): The total fixation duration of each word in the scanpath (fixation)\n        sp_landing_pos (torch.Tensor): The landing position of each word in the scanpath (fixation)\n        sp_mask (torch.Tensor): The mask for the scanpath\n        word_ids_sp (torch.Tensor): The word index of each input_id in the scanpath\n        scanpath (torch.Tensor): The scanpath tensor\n\n    \"\"\"\n\n    # used for computing sp_merged_word_mask\n    # x = self.bert_encoder.embeddings.word_embeddings(sp_emd)\n    # x[sp_emd == self.pad_token_id] = 0\n    # # Pool bert subword to word level for English corpus\n    # _, sp_merged_word_mask = self.pool_subword_to_word(\n    #     x, word_ids_sp, target='sp', pool_method='sum'\n    # )\n\n    with torch.no_grad():\n        outputs = self.bert_encoder(input_ids=sn_emd, attention_mask=sn_mask)\n    #  Make the embedding of the &lt;pad&gt; token to be zeros\n    outputs.last_hidden_state[sn_emd == self.pad_token_id] = 0\n    # Pool bert subword to word level for english corpus\n    merged_word_emb, sn_mask_word = pool_subword_to_word(\n        outputs.last_hidden_state,\n        word_ids_sn,\n        target='sn',\n        max_seq_len=self.actual_max_needed_len,\n        bert_dim=self.bert_dim,\n        pool_method='sum',\n    )\n    batch_index = torch.arange(scanpath.shape[0]).unsqueeze(1).expand_as(scanpath)\n    scanpath_add1 = scanpath.clone()\n    scanpath_add1[scanpath != SCANPATH_PADDING_VAL] += SCANPATH_PADDING_VAL\n    x = merged_word_emb[\n        batch_index, scanpath_add1\n    ]  # [batch, max_sp_length, emb_dim], word_emb_sn\n\n    if features_by_sp_idx is not None:\n        x = torch.cat([x, features_by_sp_idx], dim=2)\n    if features_by_word_idx is not None:\n        x = torch.cat(\n            [x, features_by_word_idx[batch_index, scanpath]], dim=2\n        )  # we don't need scanpath_add1 here because no &lt;s&gt; token in the beginning\n\n    # pass through the LSTM layer\n    sorted_lengths, indices = torch.sort(\n        (scanpath != SCANPATH_PADDING_VAL).sum(dim=1), descending=True\n    )\n    x = x[\n        indices\n    ]  # reorder sequences according to the descending order of the lengths\n\n    # Pass the entire sequence through the LSTM layer\n    packed_x = nn.utils.rnn.pack_padded_sequence(\n        input=x,\n        lengths=sorted_lengths.to('cpu'),\n        batch_first=True,\n        enforce_sorted=True,\n    )\n\n    # set h0 as [CLS] token embedding of each sequence\n    h0 = outputs.last_hidden_state[indices, 0]\n\n    # fit h0 and c0 to the shape of the LSTM\n    # h0 is [batch, hidden_size] where h0[i] is the initial hidden state for the sequence i\n    h0 = h0.unsqueeze(0).repeat(self.fse_lstm.num_layers, 1, 1)\n    c0 = torch.zeros_like(h0)\n    # ensure contiguous\n    h0 = h0.contiguous()\n    c0 = c0.contiguous()\n\n    # * c0 isn't used when using GRU\n\n    packed_output, ht = self.fse_lstm(packed_x, h0)\n    lstm_last_hidden = ht[-1].squeeze(\n        1\n    )  # Take the hidden state of the last LSTM layer.\n\n    # reorder the hidden states to the original order\n    lstm_last_hidden = lstm_last_hidden[\n        torch.argsort(indices)\n    ]  # Tested. Reorders correctly\n\n    # Clear unused tensors to free memory\n    del outputs, packed_x, packed_output, ht, x, merged_word_emb\n\n    return lstm_last_hidden  # Take the hidden state of the 8th LSTM layer.\n</code></pre>"},{"location":"reference/models/plm_as_model/#models.plm_as_model.PLMASModel.shared_step","title":"<code>shared_step(batch)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>batch</code> <code>tuple</code> <p>description</p> required <p>Returns:</p> Type Description <code>tuple[Tensor, Tensor, Tensor]</code> <p>tuple[torch.Tensor, torch.Tensor, torch.Tensor]: description</p> Notes <ul> <li>in input ids: 0 is for CLS, 2 is for SEP, 1 is for PAD</li> </ul> Source code in <code>src/models/plm_as_model.py</code> <pre><code>def shared_step(\n    self,\n    batch: list,\n) -&gt; tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n\n    Args:\n        batch (tuple): _description_\n\n    Returns:\n        tuple[torch.Tensor, torch.Tensor, torch.Tensor]: _description_\n\n    Notes:\n        - in input ids: 0 is for CLS, 2 is for SEP, 1 is for PAD\n    \"\"\"\n    batch_data = self.unpack_batch(batch)\n    assert batch_data.input_ids is not None, 'input_ids not in batch_dict'\n    assert batch_data.input_masks is not None, 'input_masks not in batch_dict'\n    assert batch_data.scanpath is not None, 'scanpath not in batch_dict'\n    assert batch_data.fixation_features is not None, 'eyes_tensor not in batch_dict'\n    assert batch_data.scanpath_pads is not None, 'scanpath_pads not in batch_dict'\n    assert batch_data.eyes is not None, 'eye not in batch_dict'\n\n    shortest_scanpath_pad = batch_data.scanpath_pads.min()\n    longest_batch_scanpath: int = int(\n        self.max_scanpath_length - shortest_scanpath_pad\n    )\n\n    scanpath = batch_data.scanpath[..., :longest_batch_scanpath]\n    fixation_features = batch_data.fixation_features[\n        ..., :longest_batch_scanpath, :\n    ]\n\n    # scanpath masks\n    sp_masks = torch.ones_like(scanpath)\n    sp_masks[scanpath == self.pad_token_id] = 0\n\n    decoded_to_txt_input_ids = self.fast_tokenizer.batch_decode(\n        batch_data.input_ids, return_tensors='pt'\n    )\n\n    word_ids_sn = align_word_ids_with_input_ids(\n        tokenizer=self.fast_tokenizer,\n        input_ids=batch_data.input_ids,\n        decoded_to_txt_input_ids=decoded_to_txt_input_ids,\n    )\n\n    # in the decoded texts, space between &lt;pad&gt;&lt;pad&gt;, &lt;pad&gt;&lt;s&gt;, etc.\n    decoded_to_txt_input_ids = list(\n        map(\n            lambda x: x.replace('&lt;', ' &lt;').split(' ')[1:],\n            decoded_to_txt_input_ids,\n        )\n    )\n\n    sn_word_len = get_sn_word_lens(\n        input_ids=batch_data.input_ids,\n        decoded_to_txt_input_ids=decoded_to_txt_input_ids,\n    )\n\n    word_ids_sp, sp_input_ids = calc_sp_word_input_ids(\n        input_ids=batch_data.input_ids,\n        decoded_to_txt_input_ids=decoded_to_txt_input_ids,\n        roberta_tokenizer_prefix_space=self.fast_tokenizer_prefix,\n        scanpath=scanpath,\n    )\n\n    sp_pos, sp_fix_dur, sp_landing_pos = eyettention_legacy_code(\n        scanpath=scanpath,\n        fixation_features=fixation_features,\n    )\n\n    sn_embed = batch_data.input_ids\n    sn_mask = batch_data.input_masks\n    # if the second dimension of the scanpath is more than the maximum context length (of self.bert_encoder), cut it and notify\n    bert_encoder_max_len = self.bert_encoder.config.max_position_embeddings\n    if sp_input_ids.shape[1] &gt; bert_encoder_max_len - 1:\n        # print(\n        #     f'Text length is more than the maximum context length of the model ({bert_encoder_max_len}). Cutting from the BEGINNING of the text to max length.'\n        # )\n        sn_embed = sn_embed[:, : bert_encoder_max_len - 1]\n        sn_mask = sn_mask[:, : bert_encoder_max_len - 1]\n\n    logits, _ = self(\n        sn_emd=sn_embed,\n        sn_mask=sn_mask,\n        word_ids_sn=word_ids_sn,\n        sn_word_len=sn_word_len,\n        sp_emd=sp_input_ids,\n        sp_pos=sp_pos,\n        sp_fix_dur=sp_fix_dur,\n        sp_landing_pos=sp_landing_pos,\n        word_ids_sp=word_ids_sp,\n        scanpath=scanpath,\n    )\n\n    labels = batch_data.labels\n\n    if logits.ndim == 1:\n        logits = logits.unsqueeze(0)\n    loss = self.loss(logits, labels)\n\n    # Clear intermediate tensors to prevent memory buildup\n    del scanpath, fixation_features, sp_input_ids, word_ids_sp\n    del sp_pos, sp_fix_dur, sp_landing_pos, sn_embed, sn_mask\n    del word_ids_sn, sn_word_len\n\n    return labels, loss, logits.squeeze()\n</code></pre>"},{"location":"reference/models/plm_as_model/#models.plm_as_model.align_word_ids_with_input_ids","title":"<code>align_word_ids_with_input_ids(tokenizer, input_ids, decoded_to_txt_input_ids)</code>","text":"<p>Returns a tensor of the same shape as input_ids, containing the word index of each input_id (token)</p> Source code in <code>src/models/plm_as_model.py</code> <pre><code>def align_word_ids_with_input_ids(\n    tokenizer,\n    input_ids: torch.Tensor,\n    decoded_to_txt_input_ids: list,\n):\n    \"\"\"\n    Returns a tensor of the same shape as input_ids, containing the word index of each input_id (token)\n    \"\"\"\n    word_ids_sn_lst = []\n    retokenized_sn = tokenizer(\n        decoded_to_txt_input_ids,\n        return_tensors='pt',\n    )\n    for i in range(input_ids.shape[0]):\n        word_ids_sn_lst.append(retokenized_sn.word_ids(i)[1:-1])\n\n    word_ids_sn = torch.tensor(word_ids_sn_lst).to(input_ids.device)\n\n    return word_ids_sn\n</code></pre>"},{"location":"reference/models/plm_as_model/#models.plm_as_model.calc_sp_word_input_ids","title":"<code>calc_sp_word_input_ids(input_ids, decoded_to_txt_input_ids, roberta_tokenizer_prefix_space, scanpath)</code>","text":"<p>This function calculates the word input ids for the scanpath</p> <p>Returns:</p> Name Type Description <code>word_ids_sp</code> <code>Tensor</code> <p>The word index of each input_id (token) in the scanpath text</p> <code>sp_input_ids</code> <code>Tensor</code> <p>The input ids of the scanpath text</p> <p>Parameters:</p> Name Type Description Default <code>input_ids</code> <code>Tensor</code> <p>The word sequence input ids.     Tensor of (batch_size, max_text_length_in_tokens)</p> required <code>decoded_to_txt_input_ids</code> <code>list</code> <p>The decoded input ids.     (list of lists of strings)</p> required <code>roberta_tokenizer_prefix_space</code> <code>object</code> <p>The tokenizer with add_prefix_space=True</p> required <code>scanpath</code> <code>Tensor</code> <p>A scanpath tensor containing the word indices in the scanpath order     Tensor of (batch_size, max_scanpath_length_in_words)</p> required Source code in <code>src/models/plm_as_model.py</code> <pre><code>def calc_sp_word_input_ids(\n    input_ids: torch.Tensor,\n    decoded_to_txt_input_ids: List[List[str]],\n    roberta_tokenizer_prefix_space: object,\n    scanpath: torch.Tensor,\n):\n    \"\"\"This function calculates the word input ids for the scanpath\n\n    Returns:\n        word_ids_sp (torch.Tensor): The word index of each input_id (token) in the scanpath text\n        sp_input_ids (torch.Tensor): The input ids of the scanpath text\n\n    Args:\n        input_ids (torch.Tensor): The word sequence input ids.\n                Tensor of (batch_size, max_text_length_in_tokens)\n        decoded_to_txt_input_ids (list): The decoded input ids.\n                (list of lists of strings)\n        roberta_tokenizer_prefix_space: The tokenizer with add_prefix_space=True\n        scanpath (torch.Tensor): A scanpath tensor containing the word indices in the scanpath order\n                Tensor of (batch_size, max_scanpath_length_in_words)\n    \"\"\"\n    SP_word_ids, SP_input_ids = [], []\n\n    sp_tokens_strs = convert_positions_to_words_sp(\n        scanpath=scanpath,\n        decoded_to_txt_input_ids=decoded_to_txt_input_ids,\n        roberta_tokenizer_prefix_space=roberta_tokenizer_prefix_space,\n    )\n\n    tokenized_SPs = roberta_tokenizer_prefix_space.batch_encode_plus(\n        sp_tokens_strs,\n        add_special_tokens=False,\n        truncation=False,\n        padding='longest',\n        return_attention_mask=True,\n        is_split_into_words=True,\n    )\n    for i in range(scanpath.shape[0]):\n        encoded_sp = tokenized_SPs['input_ids'][i]\n        word_ids_sp = tokenized_SPs.word_ids(i)  # -&gt; Take the &lt;sep&gt; into account\n        word_ids_sp = [\n            val if val is not None else SCANPATH_PADDING_VAL for val in word_ids_sp\n        ]\n\n        SP_word_ids.append(word_ids_sp)\n        SP_input_ids.append(encoded_sp)\n\n    word_ids_sp = torch.tensor(SP_word_ids).to(input_ids.device)\n    sp_input_ids = torch.tensor(SP_input_ids).to(input_ids.device)\n\n    return word_ids_sp, sp_input_ids\n</code></pre>"},{"location":"reference/models/post_fusion_model/","title":"post_fusion_model","text":""},{"location":"reference/models/roberteye_model/","title":"roberteye_model","text":"<p>roberteye.py</p>"},{"location":"reference/models/roberteye_model/#models.roberteye_model.RoBERTeyeEncoderModel","title":"<code>RoBERTeyeEncoderModel</code>","text":"<p>               Bases: <code>RobertaModel</code></p> <p>This class is a modified version of the RobertaModel class from the transformers library. It adds the MAG module to the forward pass.</p> Source code in <code>src/models/roberteye_model.py</code> <pre><code>class RoBERTeyeEncoderModel(RobertaModel):\n    \"\"\"\n    This class is a modified version of the RobertaModel class from the transformers library.\n    It adds the MAG module to the forward pass.\n    \"\"\"\n\n    def __init__(\n        self,\n        config: RobertaConfig,\n        multimodal_config: MultimodalConfig,\n        add_pooling_layer: bool = True,\n    ):\n        super().__init__(config, add_pooling_layer)\n        self.config = config\n\n        self.embeddings = RoberteyeEmbeddings(config, multimodal_config)\n        self.encoder = RobertaEncoder(config)\n        # for param in self.encoder.parameters():\n        #     param.requires_grad = False\n\n        self.pooler = RobertaPooler(config) if add_pooling_layer else None\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    # Copied from transformers.models.roberta.modeling_roberta.RobertaModel.forward\n    def forward(\n        self,\n        input_ids: torch.Tensor | None = None,\n        gaze_features: torch.Tensor | None = None,\n        gaze_positions: torch.Tensor | None = None,\n        attention_mask: torch.Tensor | None = None,\n        token_type_ids: torch.Tensor | None = None,\n        eye_token_type_ids: torch.Tensor | None = None,\n        position_ids: torch.Tensor | None = None,\n        head_mask: torch.Tensor | None = None,\n        inputs_embeds: torch.Tensor | None = None,\n        encoder_hidden_states: torch.Tensor | None = None,\n        encoder_attention_mask: torch.Tensor | None = None,\n        past_key_values: list[torch.FloatTensor] | None = None,\n        use_cache: bool | None = None,\n        output_attentions: bool | None = None,\n        output_hidden_states: bool | None = None,\n    ) -&gt; BaseModelOutputWithPoolingAndCrossAttentions:\n        r\"\"\"\n        encoder_hidden_states  (`torch.FloatTensor` of shape\n        `(batch_size, sequence_length, hidden_size)`, *optional*):\n            Sequence of hidden-states at the output of the last layer of the encoder.\n            Used in the cross-attention if the model is configured as a decoder.\n        encoder_attention_mask (`torch.FloatTensor` of shape\n        `(batch_size, sequence_length)`, *optional*):\n            Mask to avoid performing attention on the padding token indices of the encoder input.\n            This mask is used in the cross-attention if the model is configured as a decoder.\n            Mask values selected in `[0, 1]`:\n\n            - 1 for tokens that are **not masked**,\n            - 0 for tokens that are **masked**.\n        past_key_values (`tuple(tuple(torch.FloatTensor))` of length `config.n_layers` with\n        each tuple having 4 tensors of shape\n        `(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`):\n            Contains precomputed key and value hidden states of the attention blocks.\n            Can be used to speed up decoding.\n\n            If `past_key_values` are used, can optionally input only the last `decoder_input_ids`\n            (those that don't have their past key value states given to this model) of shape\n            `(batch_size, 1)` instead of all `decoder_input_ids` of\n            shape `(batch_size, sequence_length)`.\n        use_cache (`bool`, *optional*):\n            If set to `True`, `past_key_values` key value states are returned\n            and can be used to speed up decoding (see `past_key_values`).\n        \"\"\"\n        output_attentions = (\n            output_attentions\n            if output_attentions is not None\n            else self.config.output_attentions\n        )\n        output_hidden_states = (\n            output_hidden_states\n            if output_hidden_states is not None\n            else self.config.output_hidden_states\n        )\n\n        if self.config.is_decoder:\n            use_cache = use_cache if use_cache is not None else self.config.use_cache\n        else:\n            use_cache = False\n\n        if input_ids is not None and inputs_embeds is not None:\n            raise ValueError(\n                'You cannot specify both input_ids and inputs_embeds at the same time',\n            )\n        elif input_ids is not None:\n            input_shape = input_ids.size()\n        elif inputs_embeds is not None:\n            input_shape = inputs_embeds.size()[:-1]\n        else:\n            raise ValueError('You have to specify either input_ids or inputs_embeds')\n\n        if gaze_features is not None:\n            input_shape = torch.Size(\n                (input_shape[0], input_shape[1] + gaze_features.size()[1]),\n            )\n\n        batch_size, seq_length = input_shape\n\n        # past_key_values_length\n        past_key_values_length = (\n            past_key_values[0][0].shape[2] if past_key_values is not None else 0\n        )\n\n        if attention_mask is None:\n            attention_mask = torch.ones(\n                ((batch_size, seq_length + past_key_values_length)),\n                device=self.device,\n            )\n\n        if token_type_ids is None:\n            if hasattr(self.embeddings, 'token_type_ids'):\n                token_seq_length = input_ids.size()[1]\n                buffered_token_type_ids = self.embeddings.token_type_ids[\n                    :, :token_seq_length\n                ]\n                buffered_token_type_ids_expanded = buffered_token_type_ids.expand(\n                    batch_size, token_seq_length\n                )\n                token_type_ids = buffered_token_type_ids_expanded\n            else:\n                token_type_ids = torch.zeros(\n                    input_shape, dtype=torch.long, device=self.device\n                )\n\n        # We can provide a self-attention mask of dimensions\n        # [batch_size, from_seq_length, to_seq_length]\n        # ourselves in which case we just need to make it broadcastable to all heads.\n        extended_attention_mask: torch.Tensor = self.get_extended_attention_mask(\n            attention_mask,\n            input_shape,\n        )\n\n        # If a 2D or 3D attention mask is provided for the cross-attention\n        # we need to make broadcastable to [batch_size, num_heads, seq_length, seq_length]\n        if self.config.is_decoder and encoder_hidden_states is not None:\n            (\n                encoder_batch_size,\n                encoder_sequence_length,\n                _,\n            ) = encoder_hidden_states.size()\n            encoder_hidden_shape = (encoder_batch_size, encoder_sequence_length)\n            if encoder_attention_mask is None:\n                encoder_attention_mask = torch.ones(\n                    encoder_hidden_shape,\n                    device=self.device,\n                )\n            encoder_extended_attention_mask = self.invert_attention_mask(\n                encoder_attention_mask,\n            )\n        else:\n            encoder_extended_attention_mask = None\n\n        # Prepare head mask if needed\n        # 1.0 in head_mask indicate we keep the head\n        # attention_probs has shape bsz x n_heads x N x N\n        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\n        # and head_mask is converted to shape\n        # [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n        head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n\n        embedding_output = self.embeddings(\n            input_ids=input_ids,\n            position_ids=position_ids,\n            token_type_ids=token_type_ids,\n            inputs_embeds=inputs_embeds,\n            past_key_values_length=past_key_values_length,\n            eye_token_type_ids=eye_token_type_ids,\n            eye_embeds=gaze_features,\n            eye_positions=gaze_positions,\n        )\n\n        encoder_outputs = self.encoder(\n            embedding_output,\n            attention_mask=extended_attention_mask,\n            head_mask=head_mask,\n            encoder_hidden_states=encoder_hidden_states,\n            encoder_attention_mask=encoder_extended_attention_mask,\n            past_key_values=past_key_values,\n            use_cache=use_cache,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n        )\n\n        sequence_output = encoder_outputs[0]\n        pooled_output = (\n            self.pooler(sequence_output) if self.pooler is not None else None\n        )\n\n        return BaseModelOutputWithPoolingAndCrossAttentions(\n            last_hidden_state=sequence_output,\n            pooler_output=pooled_output,\n            past_key_values=encoder_outputs.past_key_values,\n            hidden_states=encoder_outputs.hidden_states,\n            attentions=encoder_outputs.attentions,\n            cross_attentions=encoder_outputs.cross_attentions,\n        )\n</code></pre>"},{"location":"reference/models/roberteye_model/#models.roberteye_model.RoBERTeyeEncoderModel.forward","title":"<code>forward(input_ids=None, gaze_features=None, gaze_positions=None, attention_mask=None, token_type_ids=None, eye_token_type_ids=None, position_ids=None, head_mask=None, inputs_embeds=None, encoder_hidden_states=None, encoder_attention_mask=None, past_key_values=None, use_cache=None, output_attentions=None, output_hidden_states=None)</code>","text":"<p>encoder_hidden_states  (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, optional):     Sequence of hidden-states at the output of the last layer of the encoder.     Used in the cross-attention if the model is configured as a decoder. encoder_attention_mask (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>, optional):     Mask to avoid performing attention on the padding token indices of the encoder input.     This mask is used in the cross-attention if the model is configured as a decoder.     Mask values selected in <code>[0, 1]</code>:</p> <pre><code>- 1 for tokens that are **not masked**,\n- 0 for tokens that are **masked**.\n</code></pre> <p>past_key_values (<code>tuple(tuple(torch.FloatTensor))</code> of length <code>config.n_layers</code> with each tuple having 4 tensors of shape <code>(batch_size, num_heads, sequence_length - 1, embed_size_per_head)</code>):     Contains precomputed key and value hidden states of the attention blocks.     Can be used to speed up decoding.</p> <pre><code>If `past_key_values` are used, can optionally input only the last `decoder_input_ids`\n(those that don't have their past key value states given to this model) of shape\n`(batch_size, 1)` instead of all `decoder_input_ids` of\nshape `(batch_size, sequence_length)`.\n</code></pre> <p>use_cache (<code>bool</code>, optional):     If set to <code>True</code>, <code>past_key_values</code> key value states are returned     and can be used to speed up decoding (see <code>past_key_values</code>).</p> Source code in <code>src/models/roberteye_model.py</code> <pre><code>def forward(\n    self,\n    input_ids: torch.Tensor | None = None,\n    gaze_features: torch.Tensor | None = None,\n    gaze_positions: torch.Tensor | None = None,\n    attention_mask: torch.Tensor | None = None,\n    token_type_ids: torch.Tensor | None = None,\n    eye_token_type_ids: torch.Tensor | None = None,\n    position_ids: torch.Tensor | None = None,\n    head_mask: torch.Tensor | None = None,\n    inputs_embeds: torch.Tensor | None = None,\n    encoder_hidden_states: torch.Tensor | None = None,\n    encoder_attention_mask: torch.Tensor | None = None,\n    past_key_values: list[torch.FloatTensor] | None = None,\n    use_cache: bool | None = None,\n    output_attentions: bool | None = None,\n    output_hidden_states: bool | None = None,\n) -&gt; BaseModelOutputWithPoolingAndCrossAttentions:\n    r\"\"\"\n    encoder_hidden_states  (`torch.FloatTensor` of shape\n    `(batch_size, sequence_length, hidden_size)`, *optional*):\n        Sequence of hidden-states at the output of the last layer of the encoder.\n        Used in the cross-attention if the model is configured as a decoder.\n    encoder_attention_mask (`torch.FloatTensor` of shape\n    `(batch_size, sequence_length)`, *optional*):\n        Mask to avoid performing attention on the padding token indices of the encoder input.\n        This mask is used in the cross-attention if the model is configured as a decoder.\n        Mask values selected in `[0, 1]`:\n\n        - 1 for tokens that are **not masked**,\n        - 0 for tokens that are **masked**.\n    past_key_values (`tuple(tuple(torch.FloatTensor))` of length `config.n_layers` with\n    each tuple having 4 tensors of shape\n    `(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`):\n        Contains precomputed key and value hidden states of the attention blocks.\n        Can be used to speed up decoding.\n\n        If `past_key_values` are used, can optionally input only the last `decoder_input_ids`\n        (those that don't have their past key value states given to this model) of shape\n        `(batch_size, 1)` instead of all `decoder_input_ids` of\n        shape `(batch_size, sequence_length)`.\n    use_cache (`bool`, *optional*):\n        If set to `True`, `past_key_values` key value states are returned\n        and can be used to speed up decoding (see `past_key_values`).\n    \"\"\"\n    output_attentions = (\n        output_attentions\n        if output_attentions is not None\n        else self.config.output_attentions\n    )\n    output_hidden_states = (\n        output_hidden_states\n        if output_hidden_states is not None\n        else self.config.output_hidden_states\n    )\n\n    if self.config.is_decoder:\n        use_cache = use_cache if use_cache is not None else self.config.use_cache\n    else:\n        use_cache = False\n\n    if input_ids is not None and inputs_embeds is not None:\n        raise ValueError(\n            'You cannot specify both input_ids and inputs_embeds at the same time',\n        )\n    elif input_ids is not None:\n        input_shape = input_ids.size()\n    elif inputs_embeds is not None:\n        input_shape = inputs_embeds.size()[:-1]\n    else:\n        raise ValueError('You have to specify either input_ids or inputs_embeds')\n\n    if gaze_features is not None:\n        input_shape = torch.Size(\n            (input_shape[0], input_shape[1] + gaze_features.size()[1]),\n        )\n\n    batch_size, seq_length = input_shape\n\n    # past_key_values_length\n    past_key_values_length = (\n        past_key_values[0][0].shape[2] if past_key_values is not None else 0\n    )\n\n    if attention_mask is None:\n        attention_mask = torch.ones(\n            ((batch_size, seq_length + past_key_values_length)),\n            device=self.device,\n        )\n\n    if token_type_ids is None:\n        if hasattr(self.embeddings, 'token_type_ids'):\n            token_seq_length = input_ids.size()[1]\n            buffered_token_type_ids = self.embeddings.token_type_ids[\n                :, :token_seq_length\n            ]\n            buffered_token_type_ids_expanded = buffered_token_type_ids.expand(\n                batch_size, token_seq_length\n            )\n            token_type_ids = buffered_token_type_ids_expanded\n        else:\n            token_type_ids = torch.zeros(\n                input_shape, dtype=torch.long, device=self.device\n            )\n\n    # We can provide a self-attention mask of dimensions\n    # [batch_size, from_seq_length, to_seq_length]\n    # ourselves in which case we just need to make it broadcastable to all heads.\n    extended_attention_mask: torch.Tensor = self.get_extended_attention_mask(\n        attention_mask,\n        input_shape,\n    )\n\n    # If a 2D or 3D attention mask is provided for the cross-attention\n    # we need to make broadcastable to [batch_size, num_heads, seq_length, seq_length]\n    if self.config.is_decoder and encoder_hidden_states is not None:\n        (\n            encoder_batch_size,\n            encoder_sequence_length,\n            _,\n        ) = encoder_hidden_states.size()\n        encoder_hidden_shape = (encoder_batch_size, encoder_sequence_length)\n        if encoder_attention_mask is None:\n            encoder_attention_mask = torch.ones(\n                encoder_hidden_shape,\n                device=self.device,\n            )\n        encoder_extended_attention_mask = self.invert_attention_mask(\n            encoder_attention_mask,\n        )\n    else:\n        encoder_extended_attention_mask = None\n\n    # Prepare head mask if needed\n    # 1.0 in head_mask indicate we keep the head\n    # attention_probs has shape bsz x n_heads x N x N\n    # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\n    # and head_mask is converted to shape\n    # [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n    head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n\n    embedding_output = self.embeddings(\n        input_ids=input_ids,\n        position_ids=position_ids,\n        token_type_ids=token_type_ids,\n        inputs_embeds=inputs_embeds,\n        past_key_values_length=past_key_values_length,\n        eye_token_type_ids=eye_token_type_ids,\n        eye_embeds=gaze_features,\n        eye_positions=gaze_positions,\n    )\n\n    encoder_outputs = self.encoder(\n        embedding_output,\n        attention_mask=extended_attention_mask,\n        head_mask=head_mask,\n        encoder_hidden_states=encoder_hidden_states,\n        encoder_attention_mask=encoder_extended_attention_mask,\n        past_key_values=past_key_values,\n        use_cache=use_cache,\n        output_attentions=output_attentions,\n        output_hidden_states=output_hidden_states,\n    )\n\n    sequence_output = encoder_outputs[0]\n    pooled_output = (\n        self.pooler(sequence_output) if self.pooler is not None else None\n    )\n\n    return BaseModelOutputWithPoolingAndCrossAttentions(\n        last_hidden_state=sequence_output,\n        pooler_output=pooled_output,\n        past_key_values=encoder_outputs.past_key_values,\n        hidden_states=encoder_outputs.hidden_states,\n        attentions=encoder_outputs.attentions,\n        cross_attentions=encoder_outputs.cross_attentions,\n    )\n</code></pre>"},{"location":"reference/models/roberteye_model/#models.roberteye_model.RobertEyeForSequenceClassification","title":"<code>RobertEyeForSequenceClassification</code>","text":"<p>               Bases: <code>RobertaForSequenceClassification</code></p> <p>This class is a modified version of the RobertaForSequenceClassification class from the transformers library.</p> <p>Copied from transformers.models.roberta.modeling_roberta.RobertaForSequenceClassification</p> Source code in <code>src/models/roberteye_model.py</code> <pre><code>class RobertEyeForSequenceClassification(RobertaForSequenceClassification):\n    \"\"\"\n    This class is a modified version of the RobertaForSequenceClassification class\n    from the transformers library.\n\n    Copied from transformers.models.roberta.modeling_roberta.RobertaForSequenceClassification\n    \"\"\"\n\n    _keys_to_ignore_on_load_missing = [r'position_ids']\n\n    def __init__(self, config: RobertaConfig, multimodal_config: MultimodalConfig):\n        super().__init__(config)\n        self.num_labels = config.num_labels\n        self.config = config\n\n        self.roberta = RoBERTeyeEncoderModel(\n            config,\n            multimodal_config,\n            add_pooling_layer=False,\n        )\n        self.classifier = RobertaClassificationHead(config)\n\n        # Initialize weights and apply final processing\n        self.post_init()\n\n        # self.roberta = torch.compile(self.roberta, fullgraph=True)\n\n    def forward(\n        self,\n        input_ids: torch.Tensor | None = None,\n        gaze_features: torch.Tensor | None = None,\n        gaze_positions: torch.Tensor | None = None,\n        attention_mask: torch.Tensor | None = None,\n        token_type_ids: torch.LongTensor | None = None,\n        eye_token_type_ids: torch.LongTensor | None = None,\n        position_ids: torch.LongTensor | None = None,\n        head_mask: torch.FloatTensor | None = None,\n        inputs_embeds: torch.FloatTensor | None = None,\n        labels: torch.Tensor | None = None,\n        output_attentions: bool | None = None,\n        output_hidden_states: bool | None = None,\n    ) -&gt; tuple[torch.Tensor] | SequenceClassifierOutput:\n        r\"\"\"\n        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n            Labels for computing the sequence classification/regression loss.\n            Indices should be in `[0, ..., config.num_labels - 1]`.\n            If `config.num_labels == 1` a regression loss is computed (Mean-Square loss),\n            If `config.num_labels &gt; 1` a classification loss is computed (Cross-Entropy).\n        \"\"\"\n        outputs = self.roberta(\n            input_ids=input_ids,\n            gaze_features=gaze_features,\n            gaze_positions=gaze_positions,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n            eye_token_type_ids=eye_token_type_ids,\n            position_ids=position_ids,\n            head_mask=head_mask,\n            inputs_embeds=inputs_embeds,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n        )\n        sequence_output = outputs[0]\n        logits = self.classifier(sequence_output)\n\n        loss = None\n        if labels is not None:\n            # move labels to correct device to enable model parallelism\n            labels = labels.to(logits.device)\n            if self.config.problem_type is None:\n                if self.num_labels == 1:\n                    self.config.problem_type = 'regression'\n                elif self.num_labels &gt; 1 and (labels.dtype in (torch.long, torch.int)):\n                    self.config.problem_type = 'single_label_classification'\n                else:\n                    self.config.problem_type = 'multi_label_classification'\n\n            if self.config.problem_type == 'regression':\n                loss_fct = MSELoss()\n                if self.num_labels == 1:\n                    loss = loss_fct(logits.squeeze(), labels.squeeze())\n                else:\n                    loss = loss_fct(logits, labels)\n            elif self.config.problem_type == 'single_label_classification':\n                loss_fct = CrossEntropyLoss()\n                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n            elif self.config.problem_type == 'multi_label_classification':\n                loss_fct = BCEWithLogitsLoss()\n                loss = loss_fct(logits, labels)\n\n        return SequenceClassifierOutput(\n            loss=loss,\n            logits=logits,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )\n</code></pre>"},{"location":"reference/models/roberteye_model/#models.roberteye_model.RobertEyeForSequenceClassification.forward","title":"<code>forward(input_ids=None, gaze_features=None, gaze_positions=None, attention_mask=None, token_type_ids=None, eye_token_type_ids=None, position_ids=None, head_mask=None, inputs_embeds=None, labels=None, output_attentions=None, output_hidden_states=None)</code>","text":"<p>labels (<code>torch.LongTensor</code> of shape <code>(batch_size,)</code>, optional):     Labels for computing the sequence classification/regression loss.     Indices should be in <code>[0, ..., config.num_labels - 1]</code>.     If <code>config.num_labels == 1</code> a regression loss is computed (Mean-Square loss),     If <code>config.num_labels &gt; 1</code> a classification loss is computed (Cross-Entropy).</p> Source code in <code>src/models/roberteye_model.py</code> <pre><code>def forward(\n    self,\n    input_ids: torch.Tensor | None = None,\n    gaze_features: torch.Tensor | None = None,\n    gaze_positions: torch.Tensor | None = None,\n    attention_mask: torch.Tensor | None = None,\n    token_type_ids: torch.LongTensor | None = None,\n    eye_token_type_ids: torch.LongTensor | None = None,\n    position_ids: torch.LongTensor | None = None,\n    head_mask: torch.FloatTensor | None = None,\n    inputs_embeds: torch.FloatTensor | None = None,\n    labels: torch.Tensor | None = None,\n    output_attentions: bool | None = None,\n    output_hidden_states: bool | None = None,\n) -&gt; tuple[torch.Tensor] | SequenceClassifierOutput:\n    r\"\"\"\n    labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n        Labels for computing the sequence classification/regression loss.\n        Indices should be in `[0, ..., config.num_labels - 1]`.\n        If `config.num_labels == 1` a regression loss is computed (Mean-Square loss),\n        If `config.num_labels &gt; 1` a classification loss is computed (Cross-Entropy).\n    \"\"\"\n    outputs = self.roberta(\n        input_ids=input_ids,\n        gaze_features=gaze_features,\n        gaze_positions=gaze_positions,\n        attention_mask=attention_mask,\n        token_type_ids=token_type_ids,\n        eye_token_type_ids=eye_token_type_ids,\n        position_ids=position_ids,\n        head_mask=head_mask,\n        inputs_embeds=inputs_embeds,\n        output_attentions=output_attentions,\n        output_hidden_states=output_hidden_states,\n    )\n    sequence_output = outputs[0]\n    logits = self.classifier(sequence_output)\n\n    loss = None\n    if labels is not None:\n        # move labels to correct device to enable model parallelism\n        labels = labels.to(logits.device)\n        if self.config.problem_type is None:\n            if self.num_labels == 1:\n                self.config.problem_type = 'regression'\n            elif self.num_labels &gt; 1 and (labels.dtype in (torch.long, torch.int)):\n                self.config.problem_type = 'single_label_classification'\n            else:\n                self.config.problem_type = 'multi_label_classification'\n\n        if self.config.problem_type == 'regression':\n            loss_fct = MSELoss()\n            if self.num_labels == 1:\n                loss = loss_fct(logits.squeeze(), labels.squeeze())\n            else:\n                loss = loss_fct(logits, labels)\n        elif self.config.problem_type == 'single_label_classification':\n            loss_fct = CrossEntropyLoss()\n            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n        elif self.config.problem_type == 'multi_label_classification':\n            loss_fct = BCEWithLogitsLoss()\n            loss = loss_fct(logits, labels)\n\n    return SequenceClassifierOutput(\n        loss=loss,\n        logits=logits,\n        hidden_states=outputs.hidden_states,\n        attentions=outputs.attentions,\n    )\n</code></pre>"},{"location":"reference/models/roberteye_model/#models.roberteye_model.Roberteye","title":"<code>Roberteye</code>","text":"<p>               Bases: <code>BaseMultiModalRoberta</code></p> <p>Model for Multiple Choice Question Answering and question prediction tasks.</p> Source code in <code>src/models/roberteye_model.py</code> <pre><code>@register_model\nclass Roberteye(BaseMultiModalRoberta):\n    \"\"\"\n    Model for Multiple Choice Question Answering and question prediction tasks.\n    \"\"\"\n\n    def __init__(\n        self,\n        model_args: RoberteyeArgs,\n        trainer_args: TrainerDL,\n        data_args: DataArgs,\n    ):\n        super().__init__(\n            model_args=model_args, trainer_args=trainer_args, data_args=data_args\n        )\n        print(f'Is training: {model_args.is_training}')\n\n        if model_args.use_fixation_report:\n            eyes_projection_input_dim = model_args.fixation_dim\n        else:\n            eyes_projection_input_dim = model_args.eyes_dim\n\n        assert isinstance(eyes_projection_input_dim, int)\n\n        self.multimodal_config = MultimodalConfig(\n            text_dim=model_args.text_dim,\n            eyes_dim=eyes_projection_input_dim,\n            dropout=model_args.eye_projection_dropout,\n            eye_projection_MAGModule=model_args.eye_projection_MAGModule,\n        )\n\n        if (\n            self.prediction_mode\n            in []\n            + BINARY_PARAGRAPH_ONLY_TASKS\n            + BINARY_P_AND_Q_TASKS\n            + REGRESSION_PARAGRAPH_ONLY_TASKS\n        ):\n            if model_args.is_training:\n                if data_args.is_english:\n                    model = RobertEyeForSequenceClassification.from_pretrained(\n                        model_args.backbone,\n                        num_labels=self.num_classes,\n                        multimodal_config=self.multimodal_config,\n                    )\n                else:\n                    model = XLMRobertEyeForSequenceClassification.from_pretrained(\n                        model_args.backbone,\n                        num_labels=self.num_classes,\n                        multimodal_config=self.multimodal_config,\n                    )\n                model = adjust_model_for_eyes(\n                    model,\n                    eye_token_id=model_args.eye_token_id,\n                    sep_token_id=model_args.sep_token_id,\n                    token_type_num=model_args.token_type_num,\n                )\n                self.model = model\n                self.train()\n            else:\n                if data_args.is_english:\n                    robertaconfig = RobertaConfig.from_pretrained(\n                        model_args.backbone,\n                        vocab_size=model_args.vocab_size,\n                        type_vocab_size=model_args.token_type_num,\n                        num_labels=self.num_classes,\n                    )\n\n                    self.model = RobertEyeForSequenceClassification(\n                        config=robertaconfig,\n                        multimodal_config=self.multimodal_config,\n                    )\n                else:\n                    robertaconfig = XLMRobertaConfig.from_pretrained(\n                        model_args.backbone,\n                        vocab_size=model_args.vocab_size,\n                        type_vocab_size=model_args.token_type_num,\n                        num_labels=self.num_classes,\n                    )\n\n                    self.model = XLMRobertEyeForSequenceClassification(\n                        config=robertaconfig,\n                        multimodal_config=self.multimodal_config,\n                    )\n\n        else:\n            raise ValueError(\n                f'Invalid combination: prediction_mode - {self.prediction_mode}, '\n            )\n\n        if model_args.freeze:\n            # Freeze all model parameters except specific ones\n            for name, param in self.named_parameters():\n                if (\n                    name.startswith('model.roberta.embeddings.eye_projection')\n                    or name.startswith(\n                        'model.roberta.embeddings.eye_position_embeddings',\n                    )\n                    or name.startswith(\n                        'model.roberta.embeddings.EyeProjectionMAGModule',\n                    )\n                    or name.startswith('model.roberta.embeddings.token_type_embeddings')\n                    or name.startswith('model.classifier')\n                ):\n                    param.requires_grad = True\n                else:\n                    param.requires_grad = False\n\n        # self.model = torch.compile(self.model, fullgraph=True)\n        self.train()\n        self.save_hyperparameters()\n</code></pre>"},{"location":"reference/models/roberteye_model/#models.roberteye_model.RoberteyeEmbeddings","title":"<code>RoberteyeEmbeddings</code>","text":"<p>               Bases: <code>RobertaEmbeddings</code></p> <p>Same as BertEmbeddings with a tiny tweak for positional embeddings indexing. Based on https://github.com/uclanlp/visualbert/tree/master/visualbert</p> Source code in <code>src/models/roberteye_model.py</code> <pre><code>class RoberteyeEmbeddings(RobertaEmbeddings):\n    \"\"\"\n    Same as BertEmbeddings with a tiny tweak for positional embeddings indexing.\n    Based on https://github.com/uclanlp/visualbert/tree/master/visualbert\n    \"\"\"\n\n    def __init__(self, config: RobertaConfig, multimodal_config: MultimodalConfig):\n        super().__init__(config)\n        # Token type and position embedding for eye features\n        self.eye_position_embeddings = nn.Embedding(\n            num_embeddings=config.max_position_embeddings,\n            embedding_dim=config.hidden_size,\n            padding_idx=config.pad_token_id,\n        )\n\n        self.eye_position_embeddings.weight.data = nn.Parameter(\n            self.position_embeddings.weight.data.clone(),\n            requires_grad=True,\n        )\n        projection_dropout = multimodal_config.dropout\n        self.eye_projection = nn.Sequential(\n            nn.Linear(\n                in_features=multimodal_config.eyes_dim,\n                out_features=config.hidden_size // 2,\n            ),\n            nn.ReLU(),\n            nn.Dropout(p=projection_dropout),\n            nn.Linear(\n                in_features=config.hidden_size // 2,\n                out_features=config.hidden_size,\n            ),\n        )\n\n        self.project_eyes_with_MAG = multimodal_config.eye_projection_MAGModule\n        if self.project_eyes_with_MAG:\n            self.EyeProjectionMAGModule = MAGModule(\n                hidden_size=config.hidden_size,\n                beta_shift=0.5,\n                dropout_prob=projection_dropout,\n                text_dim=config.hidden_size,\n                eyes_dim=multimodal_config.eyes_dim,\n            )\n\n    def forward(\n        self,\n        input_ids: torch.Tensor | None = None,\n        token_type_ids: torch.Tensor | None = None,\n        position_ids: torch.Tensor | None = None,\n        inputs_embeds: torch.Tensor | None = None,\n        past_key_values_length: int = 0,\n        eye_embeds: torch.Tensor | None = None,\n        eye_token_type_ids: torch.Tensor | None = None,\n        eye_position_ids: torch.Tensor | None = None,\n        eye_positions: torch.Tensor | None = None,\n    ) -&gt; torch.Tensor:\n        if position_ids is None:\n            if input_ids is not None:\n                # Create the position ids from the input token ids. Any padded tokens remain padded.\n                position_ids = create_position_ids_from_input_ids(\n                    input_ids,\n                    self.padding_idx,\n                    past_key_values_length,\n                )\n            else:\n                position_ids = self.create_position_ids_from_inputs_embeds(\n                    inputs_embeds,\n                )\n\n        if input_ids is not None:\n            input_shape = input_ids.size()\n        elif inputs_embeds is not None:\n            input_shape = inputs_embeds.size()[:-1]\n        else:\n            raise ValueError('Either input_ids or inputs_embeds must be provided.')\n\n        seq_length = input_shape[1]\n\n        # Setting the token_type_ids to the registered buffer in constructor where it is all zeros, which usually occurs\n        # when its auto-generated, registered buffer helps users when tracing the model without passing token_type_ids,\n        # solves\n        # issue #5664\n        if token_type_ids is None:\n            if hasattr(self, 'token_type_ids'):\n                buffered_token_type_ids = self.token_type_ids[:, :seq_length]\n                buffered_token_type_ids_expanded = buffered_token_type_ids.expand(\n                    input_shape[0],\n                    seq_length,\n                )\n                token_type_ids = buffered_token_type_ids_expanded\n            else:\n                token_type_ids = torch.zeros(\n                    input_shape,\n                    dtype=torch.long,\n                    device=self.position_ids.device,\n                )\n\n        if inputs_embeds is None:\n            inputs_embeds = self.word_embeddings(input_ids)\n        token_type_embeddings = self.token_type_embeddings(token_type_ids)\n\n        embeddings = inputs_embeds + token_type_embeddings\n        if self.position_embedding_type == 'absolute':\n            position_embeddings = self.position_embeddings(position_ids)\n            embeddings += position_embeddings\n\n        # Eye movements addition\n        if eye_embeds is not None:\n            if eye_token_type_ids is None:\n                eye_token_type_ids = torch.ones(\n                    eye_embeds.size()[:-1],\n                    dtype=torch.long,\n                    device=self.position_ids.device,\n                )\n            assert eye_positions is not None\n            if self.project_eyes_with_MAG:\n                # Initialize an empty tensor to store the results\n                batch_size, seq_len, num_positions = eye_positions.shape\n                embed_dim = inputs_embeds.shape[-1]\n                average_embeddings = torch.zeros(\n                    (batch_size, seq_len, embed_dim),\n                    device=inputs_embeds.device,\n                )\n\n                # Compute the average embeddings\n                for i in range(batch_size):\n                    for j in range(seq_len):\n                        # Get the valid positions (ignoring -1)\n                        valid_positions = eye_positions[i, j][eye_positions[i, j] != -1]\n                        if len(valid_positions) &gt; 0:\n                            # Gather the word embeddings at the valid positions\n                            selected_embeddings = inputs_embeds[i, valid_positions]\n                            # Compute the average and store it\n                            average_embeddings[i, j] = selected_embeddings.mean(dim=0)\n\n                eye_embeds = self.EyeProjectionMAGModule(\n                    text_embedding=average_embeddings,\n                    gaze_features=eye_embeds,\n                )\n            else:\n                eye_embeds = self.eye_projection(eye_embeds)\n            eye_token_type_embeddings = self.token_type_embeddings(eye_token_type_ids)\n\n            # image_text_alignment = Batch x image_length x alignment_number.\n            # Each element denotes the position of the word corresponding to the image feature. -1 is the padding value.\n\n            dtype = token_type_embeddings.dtype\n            eyes_text_alignment_mask = (eye_positions != -1).long()\n            # Get rid of the -1.\n            eye_positions = eyes_text_alignment_mask * eye_positions\n\n            # Batch x image_length x alignment length x dim\n            eye_position_embeddings = self.position_embeddings(eye_positions)\n            eye_position_embeddings *= eyes_text_alignment_mask.to(\n                dtype=dtype,\n            ).unsqueeze(-1)\n            eye_position_embeddings = eye_position_embeddings.sum(2)\n\n            # We want to average along the alignment_number dimension.\n            eyes_text_alignment_mask = eyes_text_alignment_mask.to(dtype=dtype).sum(2)\n\n            if (eyes_text_alignment_mask == 0).sum() != 0:\n                eyes_text_alignment_mask[eyes_text_alignment_mask == 0] = (\n                    1  # Avoid divide by zero error\n                )\n                # print(\n                #     \"Found 0 values in `image_text_alignment_mask`. Setting them to 1 to avoid divide-by-zero\"\n                #     \" error.\"\n                # )\n            eye_position_embeddings = (\n                eye_position_embeddings / eyes_text_alignment_mask.unsqueeze(-1)\n            )\n\n            # visual_position_ids = torch.zeros(\n            #     *eye_embeds.size()[:-1], dtype=torch.long, device=eye_embeds.device\n            # )\n\n            # When fine-tuning the detector , the image_text_alignment is sometimes padded too long.\n            if eye_position_embeddings.size(1) != eye_embeds.size(1):\n                if eye_position_embeddings.size(1) &lt; eye_embeds.size(1):\n                    raise ValueError(\n                        f'Visual position embeddings length: {eye_position_embeddings.size(1)} '\n                        f'should be the same as `eye_embeds` length: {eye_embeds.size(1)}',\n                    )\n                eye_position_embeddings = eye_position_embeddings[\n                    :,\n                    : eye_embeds.size(1),\n                    :,\n                ]\n\n            # eye_position_embeddings = eye_position_embeddings + self.eye_position_embeddings(\n            #     visual_position_ids\n            # )\n\n            # if eye_position_ids is None:\n            #     eye_position_ids = create_position_ids_from_input_ids(\n            #         eye_positions,\n            #         self.padding_idx,\n            #         past_key_values_length,\n            #     )\n            # eye_position_embeddings = self.eye_position_embeddings(eye_position_ids)\n\n            final_eye_embeds = (\n                eye_embeds + eye_position_embeddings + eye_token_type_embeddings\n            )\n\n            cls_token, rest_embedding_output = (\n                embeddings[:, 0:1, :],\n                embeddings[:, 1:, :],\n            )\n            # Concatenate the CLS token, eye, and the rest of the embedding_output\n            embeddings = torch.cat(\n                (cls_token, final_eye_embeds, rest_embedding_output),\n                dim=1,\n            )\n            # Final format: CLS EYES EYE_TOKEN SEP_TOKEN REST_OF_THE_TEXT\n        embeddings = self.LayerNorm(embeddings)\n        embeddings = self.dropout(embeddings)\n        return embeddings\n</code></pre>"},{"location":"reference/models/roberteye_model/#models.roberteye_model.XLMRoBERTeyeEncoderModel","title":"<code>XLMRoBERTeyeEncoderModel</code>","text":"<p>               Bases: <code>XLMRobertaModel</code></p> <p>This class is a modified version of the RobertaModel class from the transformers library. It adds the MAG module to the forward pass.</p> Source code in <code>src/models/roberteye_model.py</code> <pre><code>class XLMRoBERTeyeEncoderModel(XLMRobertaModel):\n    \"\"\"\n    This class is a modified version of the RobertaModel class from the transformers library.\n    It adds the MAG module to the forward pass.\n    \"\"\"\n\n    def __init__(\n        self,\n        config: XLMRobertaConfig,\n        multimodal_config: MultimodalConfig,\n        add_pooling_layer: bool = True,\n    ):\n        super().__init__(config, add_pooling_layer)\n        self.config = config\n\n        self.embeddings = XLMRoberteyeEmbeddings(config, multimodal_config)\n        self.encoder = XLMRobertaEncoder(config)\n        # for param in self.encoder.parameters():\n        #     param.requires_grad = False\n\n        self.pooler = XLMRobertaPooler(config) if add_pooling_layer else None\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    # Copied from transformers.models.roberta.modeling_roberta.RobertaModel.forward\n    def forward(\n        self,\n        input_ids: torch.Tensor | None = None,\n        gaze_features: torch.Tensor | None = None,\n        gaze_positions: torch.Tensor | None = None,\n        attention_mask: torch.Tensor | None = None,\n        token_type_ids: torch.Tensor | None = None,\n        eye_token_type_ids: torch.Tensor | None = None,\n        position_ids: torch.Tensor | None = None,\n        head_mask: torch.Tensor | None = None,\n        inputs_embeds: torch.Tensor | None = None,\n        encoder_hidden_states: torch.Tensor | None = None,\n        encoder_attention_mask: torch.Tensor | None = None,\n        past_key_values: list[torch.FloatTensor] | None = None,\n        use_cache: bool | None = None,\n        output_attentions: bool | None = None,\n        output_hidden_states: bool | None = None,\n    ) -&gt; BaseModelOutputWithPoolingAndCrossAttentions:\n        r\"\"\"\n        encoder_hidden_states  (`torch.FloatTensor` of shape\n        `(batch_size, sequence_length, hidden_size)`, *optional*):\n            Sequence of hidden-states at the output of the last layer of the encoder.\n            Used in the cross-attention if the model is configured as a decoder.\n        encoder_attention_mask (`torch.FloatTensor` of shape\n        `(batch_size, sequence_length)`, *optional*):\n            Mask to avoid performing attention on the padding token indices of the encoder input.\n            This mask is used in the cross-attention if the model is configured as a decoder.\n            Mask values selected in `[0, 1]`:\n\n            - 1 for tokens that are **not masked**,\n            - 0 for tokens that are **masked**.\n        past_key_values (`tuple(tuple(torch.FloatTensor))` of length `config.n_layers` with\n        each tuple having 4 tensors of shape\n        `(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`):\n            Contains precomputed key and value hidden states of the attention blocks.\n            Can be used to speed up decoding.\n\n            If `past_key_values` are used, can optionally input only the last `decoder_input_ids`\n            (those that don't have their past key value states given to this model) of shape\n            `(batch_size, 1)` instead of all `decoder_input_ids` of\n            shape `(batch_size, sequence_length)`.\n        use_cache (`bool`, *optional*):\n            If set to `True`, `past_key_values` key value states are returned\n            and can be used to speed up decoding (see `past_key_values`).\n        \"\"\"\n        output_attentions = (\n            output_attentions\n            if output_attentions is not None\n            else self.config.output_attentions\n        )\n        output_hidden_states = (\n            output_hidden_states\n            if output_hidden_states is not None\n            else self.config.output_hidden_states\n        )\n\n        if self.config.is_decoder:\n            use_cache = use_cache if use_cache is not None else self.config.use_cache\n        else:\n            use_cache = False\n\n        if input_ids is not None and inputs_embeds is not None:\n            raise ValueError(\n                'You cannot specify both input_ids and inputs_embeds at the same time',\n            )\n        elif input_ids is not None:\n            input_shape = input_ids.size()\n        elif inputs_embeds is not None:\n            input_shape = inputs_embeds.size()[:-1]\n        else:\n            raise ValueError('You have to specify either input_ids or inputs_embeds')\n\n        if gaze_features is not None:\n            input_shape = torch.Size(\n                (input_shape[0], input_shape[1] + gaze_features.size()[1]),\n            )\n\n        batch_size, seq_length = input_shape\n\n        # past_key_values_length\n        past_key_values_length = (\n            past_key_values[0][0].shape[2] if past_key_values is not None else 0\n        )\n\n        if attention_mask is None:\n            attention_mask = torch.ones(\n                ((batch_size, seq_length + past_key_values_length)),\n                device=self.device,\n            )\n\n        if token_type_ids is None:\n            if hasattr(self.embeddings, 'token_type_ids'):\n                token_seq_length = input_ids.size()[1]\n                buffered_token_type_ids = self.embeddings.token_type_ids[\n                    :, :token_seq_length\n                ]\n                buffered_token_type_ids_expanded = buffered_token_type_ids.expand(\n                    batch_size, token_seq_length\n                )\n                token_type_ids = buffered_token_type_ids_expanded\n            else:\n                token_type_ids = torch.zeros(\n                    input_shape, dtype=torch.long, device=self.device\n                )\n\n        # We can provide a self-attention mask of dimensions\n        # [batch_size, from_seq_length, to_seq_length]\n        # ourselves in which case we just need to make it broadcastable to all heads.\n        extended_attention_mask: torch.Tensor = self.get_extended_attention_mask(\n            attention_mask,\n            input_shape,\n        )\n\n        # If a 2D or 3D attention mask is provided for the cross-attention\n        # we need to make broadcastable to [batch_size, num_heads, seq_length, seq_length]\n        if self.config.is_decoder and encoder_hidden_states is not None:\n            (\n                encoder_batch_size,\n                encoder_sequence_length,\n                _,\n            ) = encoder_hidden_states.size()\n            encoder_hidden_shape = (encoder_batch_size, encoder_sequence_length)\n            if encoder_attention_mask is None:\n                encoder_attention_mask = torch.ones(\n                    encoder_hidden_shape,\n                    device=self.device,\n                )\n            encoder_extended_attention_mask = self.invert_attention_mask(\n                encoder_attention_mask,\n            )\n        else:\n            encoder_extended_attention_mask = None\n\n        # Prepare head mask if needed\n        # 1.0 in head_mask indicate we keep the head\n        # attention_probs has shape bsz x n_heads x N x N\n        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\n        # and head_mask is converted to shape\n        # [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n        head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n\n        embedding_output = self.embeddings(\n            input_ids=input_ids,\n            position_ids=position_ids,\n            token_type_ids=token_type_ids,\n            inputs_embeds=inputs_embeds,\n            past_key_values_length=past_key_values_length,\n            eye_token_type_ids=eye_token_type_ids,\n            eye_embeds=gaze_features,\n            eye_positions=gaze_positions,\n        )\n\n        encoder_outputs = self.encoder(\n            embedding_output,\n            attention_mask=extended_attention_mask,\n            head_mask=head_mask,\n            encoder_hidden_states=encoder_hidden_states,\n            encoder_attention_mask=encoder_extended_attention_mask,\n            past_key_values=past_key_values,\n            use_cache=use_cache,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n        )\n\n        sequence_output = encoder_outputs[0]\n        pooled_output = (\n            self.pooler(sequence_output) if self.pooler is not None else None\n        )\n\n        return BaseModelOutputWithPoolingAndCrossAttentions(\n            last_hidden_state=sequence_output,\n            pooler_output=pooled_output,\n            past_key_values=encoder_outputs.past_key_values,\n            hidden_states=encoder_outputs.hidden_states,\n            attentions=encoder_outputs.attentions,\n            cross_attentions=encoder_outputs.cross_attentions,\n        )\n</code></pre>"},{"location":"reference/models/roberteye_model/#models.roberteye_model.XLMRoBERTeyeEncoderModel.forward","title":"<code>forward(input_ids=None, gaze_features=None, gaze_positions=None, attention_mask=None, token_type_ids=None, eye_token_type_ids=None, position_ids=None, head_mask=None, inputs_embeds=None, encoder_hidden_states=None, encoder_attention_mask=None, past_key_values=None, use_cache=None, output_attentions=None, output_hidden_states=None)</code>","text":"<p>encoder_hidden_states  (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, optional):     Sequence of hidden-states at the output of the last layer of the encoder.     Used in the cross-attention if the model is configured as a decoder. encoder_attention_mask (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>, optional):     Mask to avoid performing attention on the padding token indices of the encoder input.     This mask is used in the cross-attention if the model is configured as a decoder.     Mask values selected in <code>[0, 1]</code>:</p> <pre><code>- 1 for tokens that are **not masked**,\n- 0 for tokens that are **masked**.\n</code></pre> <p>past_key_values (<code>tuple(tuple(torch.FloatTensor))</code> of length <code>config.n_layers</code> with each tuple having 4 tensors of shape <code>(batch_size, num_heads, sequence_length - 1, embed_size_per_head)</code>):     Contains precomputed key and value hidden states of the attention blocks.     Can be used to speed up decoding.</p> <pre><code>If `past_key_values` are used, can optionally input only the last `decoder_input_ids`\n(those that don't have their past key value states given to this model) of shape\n`(batch_size, 1)` instead of all `decoder_input_ids` of\nshape `(batch_size, sequence_length)`.\n</code></pre> <p>use_cache (<code>bool</code>, optional):     If set to <code>True</code>, <code>past_key_values</code> key value states are returned     and can be used to speed up decoding (see <code>past_key_values</code>).</p> Source code in <code>src/models/roberteye_model.py</code> <pre><code>def forward(\n    self,\n    input_ids: torch.Tensor | None = None,\n    gaze_features: torch.Tensor | None = None,\n    gaze_positions: torch.Tensor | None = None,\n    attention_mask: torch.Tensor | None = None,\n    token_type_ids: torch.Tensor | None = None,\n    eye_token_type_ids: torch.Tensor | None = None,\n    position_ids: torch.Tensor | None = None,\n    head_mask: torch.Tensor | None = None,\n    inputs_embeds: torch.Tensor | None = None,\n    encoder_hidden_states: torch.Tensor | None = None,\n    encoder_attention_mask: torch.Tensor | None = None,\n    past_key_values: list[torch.FloatTensor] | None = None,\n    use_cache: bool | None = None,\n    output_attentions: bool | None = None,\n    output_hidden_states: bool | None = None,\n) -&gt; BaseModelOutputWithPoolingAndCrossAttentions:\n    r\"\"\"\n    encoder_hidden_states  (`torch.FloatTensor` of shape\n    `(batch_size, sequence_length, hidden_size)`, *optional*):\n        Sequence of hidden-states at the output of the last layer of the encoder.\n        Used in the cross-attention if the model is configured as a decoder.\n    encoder_attention_mask (`torch.FloatTensor` of shape\n    `(batch_size, sequence_length)`, *optional*):\n        Mask to avoid performing attention on the padding token indices of the encoder input.\n        This mask is used in the cross-attention if the model is configured as a decoder.\n        Mask values selected in `[0, 1]`:\n\n        - 1 for tokens that are **not masked**,\n        - 0 for tokens that are **masked**.\n    past_key_values (`tuple(tuple(torch.FloatTensor))` of length `config.n_layers` with\n    each tuple having 4 tensors of shape\n    `(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`):\n        Contains precomputed key and value hidden states of the attention blocks.\n        Can be used to speed up decoding.\n\n        If `past_key_values` are used, can optionally input only the last `decoder_input_ids`\n        (those that don't have their past key value states given to this model) of shape\n        `(batch_size, 1)` instead of all `decoder_input_ids` of\n        shape `(batch_size, sequence_length)`.\n    use_cache (`bool`, *optional*):\n        If set to `True`, `past_key_values` key value states are returned\n        and can be used to speed up decoding (see `past_key_values`).\n    \"\"\"\n    output_attentions = (\n        output_attentions\n        if output_attentions is not None\n        else self.config.output_attentions\n    )\n    output_hidden_states = (\n        output_hidden_states\n        if output_hidden_states is not None\n        else self.config.output_hidden_states\n    )\n\n    if self.config.is_decoder:\n        use_cache = use_cache if use_cache is not None else self.config.use_cache\n    else:\n        use_cache = False\n\n    if input_ids is not None and inputs_embeds is not None:\n        raise ValueError(\n            'You cannot specify both input_ids and inputs_embeds at the same time',\n        )\n    elif input_ids is not None:\n        input_shape = input_ids.size()\n    elif inputs_embeds is not None:\n        input_shape = inputs_embeds.size()[:-1]\n    else:\n        raise ValueError('You have to specify either input_ids or inputs_embeds')\n\n    if gaze_features is not None:\n        input_shape = torch.Size(\n            (input_shape[0], input_shape[1] + gaze_features.size()[1]),\n        )\n\n    batch_size, seq_length = input_shape\n\n    # past_key_values_length\n    past_key_values_length = (\n        past_key_values[0][0].shape[2] if past_key_values is not None else 0\n    )\n\n    if attention_mask is None:\n        attention_mask = torch.ones(\n            ((batch_size, seq_length + past_key_values_length)),\n            device=self.device,\n        )\n\n    if token_type_ids is None:\n        if hasattr(self.embeddings, 'token_type_ids'):\n            token_seq_length = input_ids.size()[1]\n            buffered_token_type_ids = self.embeddings.token_type_ids[\n                :, :token_seq_length\n            ]\n            buffered_token_type_ids_expanded = buffered_token_type_ids.expand(\n                batch_size, token_seq_length\n            )\n            token_type_ids = buffered_token_type_ids_expanded\n        else:\n            token_type_ids = torch.zeros(\n                input_shape, dtype=torch.long, device=self.device\n            )\n\n    # We can provide a self-attention mask of dimensions\n    # [batch_size, from_seq_length, to_seq_length]\n    # ourselves in which case we just need to make it broadcastable to all heads.\n    extended_attention_mask: torch.Tensor = self.get_extended_attention_mask(\n        attention_mask,\n        input_shape,\n    )\n\n    # If a 2D or 3D attention mask is provided for the cross-attention\n    # we need to make broadcastable to [batch_size, num_heads, seq_length, seq_length]\n    if self.config.is_decoder and encoder_hidden_states is not None:\n        (\n            encoder_batch_size,\n            encoder_sequence_length,\n            _,\n        ) = encoder_hidden_states.size()\n        encoder_hidden_shape = (encoder_batch_size, encoder_sequence_length)\n        if encoder_attention_mask is None:\n            encoder_attention_mask = torch.ones(\n                encoder_hidden_shape,\n                device=self.device,\n            )\n        encoder_extended_attention_mask = self.invert_attention_mask(\n            encoder_attention_mask,\n        )\n    else:\n        encoder_extended_attention_mask = None\n\n    # Prepare head mask if needed\n    # 1.0 in head_mask indicate we keep the head\n    # attention_probs has shape bsz x n_heads x N x N\n    # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\n    # and head_mask is converted to shape\n    # [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n    head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n\n    embedding_output = self.embeddings(\n        input_ids=input_ids,\n        position_ids=position_ids,\n        token_type_ids=token_type_ids,\n        inputs_embeds=inputs_embeds,\n        past_key_values_length=past_key_values_length,\n        eye_token_type_ids=eye_token_type_ids,\n        eye_embeds=gaze_features,\n        eye_positions=gaze_positions,\n    )\n\n    encoder_outputs = self.encoder(\n        embedding_output,\n        attention_mask=extended_attention_mask,\n        head_mask=head_mask,\n        encoder_hidden_states=encoder_hidden_states,\n        encoder_attention_mask=encoder_extended_attention_mask,\n        past_key_values=past_key_values,\n        use_cache=use_cache,\n        output_attentions=output_attentions,\n        output_hidden_states=output_hidden_states,\n    )\n\n    sequence_output = encoder_outputs[0]\n    pooled_output = (\n        self.pooler(sequence_output) if self.pooler is not None else None\n    )\n\n    return BaseModelOutputWithPoolingAndCrossAttentions(\n        last_hidden_state=sequence_output,\n        pooler_output=pooled_output,\n        past_key_values=encoder_outputs.past_key_values,\n        hidden_states=encoder_outputs.hidden_states,\n        attentions=encoder_outputs.attentions,\n        cross_attentions=encoder_outputs.cross_attentions,\n    )\n</code></pre>"},{"location":"reference/models/roberteye_model/#models.roberteye_model.XLMRobertEyeForSequenceClassification","title":"<code>XLMRobertEyeForSequenceClassification</code>","text":"<p>               Bases: <code>XLMRobertaForSequenceClassification</code></p> <p>This class is a modified version of the RobertaForSequenceClassification class from the transformers library.</p> <p>Copied from transformers.models.roberta.modeling_roberta.RobertaForSequenceClassification</p> Source code in <code>src/models/roberteye_model.py</code> <pre><code>class XLMRobertEyeForSequenceClassification(XLMRobertaForSequenceClassification):\n    \"\"\"\n    This class is a modified version of the RobertaForSequenceClassification class\n    from the transformers library.\n\n    Copied from transformers.models.roberta.modeling_roberta.RobertaForSequenceClassification\n    \"\"\"\n\n    _keys_to_ignore_on_load_missing = [r'position_ids']\n\n    def __init__(self, config: XLMRobertaConfig, multimodal_config: MultimodalConfig):\n        super().__init__(config)\n        self.num_labels = config.num_labels\n        self.config = config\n\n        self.roberta = XLMRoBERTeyeEncoderModel(\n            config,\n            multimodal_config,\n            add_pooling_layer=False,\n        )\n        self.classifier = XLMRobertaClassificationHead(config)\n\n        # Initialize weights and apply final processing\n        self.post_init()\n\n        # self.roberta = torch.compile(self.roberta, fullgraph=True)\n\n    def forward(\n        self,\n        input_ids: torch.Tensor | None = None,\n        gaze_features: torch.Tensor | None = None,\n        gaze_positions: torch.Tensor | None = None,\n        attention_mask: torch.Tensor | None = None,\n        token_type_ids: torch.LongTensor | None = None,\n        eye_token_type_ids: torch.LongTensor | None = None,\n        position_ids: torch.LongTensor | None = None,\n        head_mask: torch.FloatTensor | None = None,\n        inputs_embeds: torch.FloatTensor | None = None,\n        labels: torch.Tensor | None = None,\n        output_attentions: bool | None = None,\n        output_hidden_states: bool | None = None,\n    ) -&gt; tuple[torch.Tensor] | SequenceClassifierOutput:\n        r\"\"\"\n        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n            Labels for computing the sequence classification/regression loss.\n            Indices should be in `[0, ..., config.num_labels - 1]`.\n            If `config.num_labels == 1` a regression loss is computed (Mean-Square loss),\n            If `config.num_labels &gt; 1` a classification loss is computed (Cross-Entropy).\n        \"\"\"\n        outputs = self.roberta(\n            input_ids=input_ids,\n            gaze_features=gaze_features,\n            gaze_positions=gaze_positions,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n            eye_token_type_ids=eye_token_type_ids,\n            position_ids=position_ids,\n            head_mask=head_mask,\n            inputs_embeds=inputs_embeds,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n        )\n        sequence_output = outputs[0]\n        logits = self.classifier(sequence_output)\n\n        loss = None\n        if labels is not None:\n            # move labels to correct device to enable model parallelism\n            labels = labels.to(logits.device)\n            if self.config.problem_type is None:\n                if self.num_labels == 1:\n                    self.config.problem_type = 'regression'\n                elif self.num_labels &gt; 1 and (labels.dtype in (torch.long, torch.int)):\n                    self.config.problem_type = 'single_label_classification'\n                else:\n                    self.config.problem_type = 'multi_label_classification'\n\n            if self.config.problem_type == 'regression':\n                loss_fct = MSELoss()\n                if self.num_labels == 1:\n                    loss = loss_fct(logits.squeeze(), labels.squeeze())\n                else:\n                    loss = loss_fct(logits, labels)\n            elif self.config.problem_type == 'single_label_classification':\n                loss_fct = CrossEntropyLoss()\n                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n            elif self.config.problem_type == 'multi_label_classification':\n                loss_fct = BCEWithLogitsLoss()\n                loss = loss_fct(logits, labels)\n\n        return SequenceClassifierOutput(\n            loss=loss,\n            logits=logits,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )\n</code></pre>"},{"location":"reference/models/roberteye_model/#models.roberteye_model.XLMRobertEyeForSequenceClassification.forward","title":"<code>forward(input_ids=None, gaze_features=None, gaze_positions=None, attention_mask=None, token_type_ids=None, eye_token_type_ids=None, position_ids=None, head_mask=None, inputs_embeds=None, labels=None, output_attentions=None, output_hidden_states=None)</code>","text":"<p>labels (<code>torch.LongTensor</code> of shape <code>(batch_size,)</code>, optional):     Labels for computing the sequence classification/regression loss.     Indices should be in <code>[0, ..., config.num_labels - 1]</code>.     If <code>config.num_labels == 1</code> a regression loss is computed (Mean-Square loss),     If <code>config.num_labels &gt; 1</code> a classification loss is computed (Cross-Entropy).</p> Source code in <code>src/models/roberteye_model.py</code> <pre><code>def forward(\n    self,\n    input_ids: torch.Tensor | None = None,\n    gaze_features: torch.Tensor | None = None,\n    gaze_positions: torch.Tensor | None = None,\n    attention_mask: torch.Tensor | None = None,\n    token_type_ids: torch.LongTensor | None = None,\n    eye_token_type_ids: torch.LongTensor | None = None,\n    position_ids: torch.LongTensor | None = None,\n    head_mask: torch.FloatTensor | None = None,\n    inputs_embeds: torch.FloatTensor | None = None,\n    labels: torch.Tensor | None = None,\n    output_attentions: bool | None = None,\n    output_hidden_states: bool | None = None,\n) -&gt; tuple[torch.Tensor] | SequenceClassifierOutput:\n    r\"\"\"\n    labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n        Labels for computing the sequence classification/regression loss.\n        Indices should be in `[0, ..., config.num_labels - 1]`.\n        If `config.num_labels == 1` a regression loss is computed (Mean-Square loss),\n        If `config.num_labels &gt; 1` a classification loss is computed (Cross-Entropy).\n    \"\"\"\n    outputs = self.roberta(\n        input_ids=input_ids,\n        gaze_features=gaze_features,\n        gaze_positions=gaze_positions,\n        attention_mask=attention_mask,\n        token_type_ids=token_type_ids,\n        eye_token_type_ids=eye_token_type_ids,\n        position_ids=position_ids,\n        head_mask=head_mask,\n        inputs_embeds=inputs_embeds,\n        output_attentions=output_attentions,\n        output_hidden_states=output_hidden_states,\n    )\n    sequence_output = outputs[0]\n    logits = self.classifier(sequence_output)\n\n    loss = None\n    if labels is not None:\n        # move labels to correct device to enable model parallelism\n        labels = labels.to(logits.device)\n        if self.config.problem_type is None:\n            if self.num_labels == 1:\n                self.config.problem_type = 'regression'\n            elif self.num_labels &gt; 1 and (labels.dtype in (torch.long, torch.int)):\n                self.config.problem_type = 'single_label_classification'\n            else:\n                self.config.problem_type = 'multi_label_classification'\n\n        if self.config.problem_type == 'regression':\n            loss_fct = MSELoss()\n            if self.num_labels == 1:\n                loss = loss_fct(logits.squeeze(), labels.squeeze())\n            else:\n                loss = loss_fct(logits, labels)\n        elif self.config.problem_type == 'single_label_classification':\n            loss_fct = CrossEntropyLoss()\n            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n        elif self.config.problem_type == 'multi_label_classification':\n            loss_fct = BCEWithLogitsLoss()\n            loss = loss_fct(logits, labels)\n\n    return SequenceClassifierOutput(\n        loss=loss,\n        logits=logits,\n        hidden_states=outputs.hidden_states,\n        attentions=outputs.attentions,\n    )\n</code></pre>"},{"location":"reference/models/roberteye_model/#models.roberteye_model.XLMRoberteyeEmbeddings","title":"<code>XLMRoberteyeEmbeddings</code>","text":"<p>               Bases: <code>XLMRobertaEmbeddings</code></p> <p>Same as BertEmbeddings with a tiny tweak for positional embeddings indexing. Based on https://github.com/uclanlp/visualbert/tree/master/visualbert</p> Source code in <code>src/models/roberteye_model.py</code> <pre><code>class XLMRoberteyeEmbeddings(XLMRobertaEmbeddings):\n    \"\"\"\n    Same as BertEmbeddings with a tiny tweak for positional embeddings indexing.\n    Based on https://github.com/uclanlp/visualbert/tree/master/visualbert\n    \"\"\"\n\n    def __init__(self, config: XLMRobertaConfig, multimodal_config: MultimodalConfig):\n        super().__init__(config)\n        # Token type and position embedding for eye features\n        self.eye_position_embeddings = nn.Embedding(\n            num_embeddings=config.max_position_embeddings,\n            embedding_dim=config.hidden_size,\n            padding_idx=config.pad_token_id,\n        )\n\n        self.eye_position_embeddings.weight.data = nn.Parameter(\n            self.position_embeddings.weight.data.clone(),\n            requires_grad=True,\n        )\n        projection_dropout = multimodal_config.dropout\n        self.eye_projection = nn.Sequential(\n            nn.Linear(\n                in_features=multimodal_config.eyes_dim,\n                out_features=config.hidden_size // 2,\n            ),\n            nn.ReLU(),\n            nn.Dropout(p=projection_dropout),\n            nn.Linear(\n                in_features=config.hidden_size // 2,\n                out_features=config.hidden_size,\n            ),\n        )\n\n        self.project_eyes_with_MAG = multimodal_config.eye_projection_MAGModule\n        if self.project_eyes_with_MAG:\n            self.EyeProjectionMAGModule = MAGModule(\n                hidden_size=config.hidden_size,\n                beta_shift=0.5,\n                dropout_prob=projection_dropout,\n                text_dim=config.hidden_size,\n                eyes_dim=multimodal_config.eyes_dim,\n            )\n\n    def forward(\n        self,\n        input_ids: torch.Tensor | None = None,\n        token_type_ids: torch.Tensor | None = None,\n        position_ids: torch.Tensor | None = None,\n        inputs_embeds: torch.Tensor | None = None,\n        past_key_values_length: int = 0,\n        eye_embeds: torch.Tensor | None = None,\n        eye_token_type_ids: torch.Tensor | None = None,\n        eye_position_ids: torch.Tensor | None = None,\n        eye_positions: torch.Tensor | None = None,\n    ) -&gt; torch.Tensor:\n        if position_ids is None:\n            if input_ids is not None:\n                # Create the position ids from the input token ids. Any padded tokens remain padded.\n                position_ids = create_position_ids_from_input_ids(\n                    input_ids,\n                    self.padding_idx,\n                    past_key_values_length,\n                )\n            else:\n                position_ids = self.create_position_ids_from_inputs_embeds(\n                    inputs_embeds,\n                )\n\n        if input_ids is not None:\n            input_shape = input_ids.size()\n        elif inputs_embeds is not None:\n            input_shape = inputs_embeds.size()[:-1]\n        else:\n            raise ValueError('Either input_ids or inputs_embeds must be provided.')\n\n        seq_length = input_shape[1]\n\n        # Setting the token_type_ids to the registered buffer in constructor where it is all zeros, which usually occurs\n        # when its auto-generated, registered buffer helps users when tracing the model without passing token_type_ids,\n        # solves\n        # issue #5664\n        if token_type_ids is None:\n            if hasattr(self, 'token_type_ids'):\n                buffered_token_type_ids = self.token_type_ids[:, :seq_length]\n                buffered_token_type_ids_expanded = buffered_token_type_ids.expand(\n                    input_shape[0],\n                    seq_length,\n                )\n                token_type_ids = buffered_token_type_ids_expanded\n            else:\n                token_type_ids = torch.zeros(\n                    input_shape,\n                    dtype=torch.long,\n                    device=self.position_ids.device,\n                )\n\n        if inputs_embeds is None:\n            inputs_embeds = self.word_embeddings(input_ids)\n        token_type_embeddings = self.token_type_embeddings(token_type_ids)\n\n        embeddings = inputs_embeds + token_type_embeddings\n        if self.position_embedding_type == 'absolute':\n            position_embeddings = self.position_embeddings(position_ids)\n            embeddings += position_embeddings\n\n        # Eye movements addition\n        if eye_embeds is not None:\n            if eye_token_type_ids is None:\n                eye_token_type_ids = torch.ones(\n                    eye_embeds.size()[:-1],\n                    dtype=torch.long,\n                    device=self.position_ids.device,\n                )\n            assert eye_positions is not None\n            if self.project_eyes_with_MAG:\n                # Initialize an empty tensor to store the results\n                batch_size, seq_len, num_positions = eye_positions.shape\n                embed_dim = inputs_embeds.shape[-1]\n                average_embeddings = torch.zeros(\n                    (batch_size, seq_len, embed_dim),\n                    device=inputs_embeds.device,\n                )\n\n                # Compute the average embeddings\n                for i in range(batch_size):\n                    for j in range(seq_len):\n                        # Get the valid positions (ignoring -1)\n                        valid_positions = eye_positions[i, j][eye_positions[i, j] != -1]\n                        if len(valid_positions) &gt; 0:\n                            # Gather the word embeddings at the valid positions\n                            selected_embeddings = inputs_embeds[i, valid_positions]\n                            # Compute the average and store it\n                            average_embeddings[i, j] = selected_embeddings.mean(dim=0)\n\n                eye_embeds = self.EyeProjectionMAGModule(\n                    text_embedding=average_embeddings,\n                    gaze_features=eye_embeds,\n                )\n            else:\n                eye_embeds = self.eye_projection(eye_embeds)\n            eye_token_type_embeddings = self.token_type_embeddings(eye_token_type_ids)\n\n            # image_text_alignment = Batch x image_length x alignment_number.\n            # Each element denotes the position of the word corresponding to the image feature. -1 is the padding value.\n\n            dtype = token_type_embeddings.dtype\n            eyes_text_alignment_mask = (eye_positions != -1).long()\n            # Get rid of the -1.\n            eye_positions = eyes_text_alignment_mask * eye_positions\n\n            # Batch x image_length x alignment length x dim\n            eye_position_embeddings = self.position_embeddings(eye_positions)\n            eye_position_embeddings *= eyes_text_alignment_mask.to(\n                dtype=dtype,\n            ).unsqueeze(-1)\n            eye_position_embeddings = eye_position_embeddings.sum(2)\n\n            # We want to average along the alignment_number dimension.\n            eyes_text_alignment_mask = eyes_text_alignment_mask.to(dtype=dtype).sum(2)\n\n            if (eyes_text_alignment_mask == 0).sum() != 0:\n                eyes_text_alignment_mask[eyes_text_alignment_mask == 0] = (\n                    1  # Avoid divide by zero error\n                )\n                # print(\n                #     \"Found 0 values in `image_text_alignment_mask`. Setting them to 1 to avoid divide-by-zero\"\n                #     \" error.\"\n                # )\n            eye_position_embeddings = (\n                eye_position_embeddings / eyes_text_alignment_mask.unsqueeze(-1)\n            )\n\n            # visual_position_ids = torch.zeros(\n            #     *eye_embeds.size()[:-1], dtype=torch.long, device=eye_embeds.device\n            # )\n\n            # When fine-tuning the detector , the image_text_alignment is sometimes padded too long.\n            if eye_position_embeddings.size(1) != eye_embeds.size(1):\n                if eye_position_embeddings.size(1) &lt; eye_embeds.size(1):\n                    raise ValueError(\n                        f'Visual position embeddings length: {eye_position_embeddings.size(1)} '\n                        f'should be the same as `eye_embeds` length: {eye_embeds.size(1)}',\n                    )\n                eye_position_embeddings = eye_position_embeddings[\n                    :,\n                    : eye_embeds.size(1),\n                    :,\n                ]\n\n            # eye_position_embeddings = eye_position_embeddings + self.eye_position_embeddings(\n            #     visual_position_ids\n            # )\n\n            # if eye_position_ids is None:\n            #     eye_position_ids = create_position_ids_from_input_ids(\n            #         eye_positions,\n            #         self.padding_idx,\n            #         past_key_values_length,\n            #     )\n            # eye_position_embeddings = self.eye_position_embeddings(eye_position_ids)\n\n            final_eye_embeds = (\n                eye_embeds + eye_position_embeddings + eye_token_type_embeddings\n            )\n\n            cls_token, rest_embedding_output = (\n                embeddings[:, 0:1, :],\n                embeddings[:, 1:, :],\n            )\n            # Concatenate the CLS token, eye, and the rest of the embedding_output\n            embeddings = torch.cat(\n                (cls_token, final_eye_embeds, rest_embedding_output),\n                dim=1,\n            )\n            # Final format: CLS EYES EYE_TOKEN SEP_TOKEN REST_OF_THE_TEXT\n        embeddings = self.LayerNorm(embeddings)\n        embeddings = self.dropout(embeddings)\n        return embeddings\n</code></pre>"},{"location":"reference/models/utils/","title":"utils","text":"<p>This module contains utility functions for the models.</p>"},{"location":"reference/models/utils/#models.utils.compute_kernel_bias","title":"<code>compute_kernel_bias(vecs)</code>","text":"<p>vecs shape is (batch_size, max_seq_len, hidden_size) Calculate Kernel &amp; Bias for the final transformation - y = (x + bias).dot(kernel)</p> Source code in <code>src/models/utils.py</code> <pre><code>def compute_kernel_bias(vecs: np.ndarray) -&gt; tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    vecs shape is (batch_size, max_seq_len, hidden_size)\n    Calculate Kernel &amp; Bias for the final transformation - y = (x + bias).dot(kernel)\n    \"\"\"\n    mu = vecs.mean(dim=0, keepdim=True)\n    cov = np.cov(vecs.T)\n    u, s, unused_vh = np.linalg.svd(cov)\n    w = np.dot(u, np.diag(s**0.5))\n    w = np.linalg.inv(w.T)\n    return w, -mu\n</code></pre>"},{"location":"reference/models/utils/#models.utils.normalize","title":"<code>normalize(vecs)</code>","text":"<p>Standardization</p> Source code in <code>src/models/utils.py</code> <pre><code>def normalize(vecs: np.ndarray) -&gt; np.ndarray:\n    \"\"\"\n    Standardization\n    \"\"\"\n    return vecs / (vecs**2).sum(axis=1, keepdims=True) ** 0.5\n</code></pre>"},{"location":"reference/models/utils/#models.utils.transform_and_normalize","title":"<code>transform_and_normalize(vecs, kernel, bias)</code>","text":"<p>Applying transformation then standardize</p> Source code in <code>src/models/utils.py</code> <pre><code>def transform_and_normalize(\n    vecs: np.ndarray, kernel: np.ndarray, bias: np.ndarray\n) -&gt; np.ndarray:\n    \"\"\"\n    Applying transformation then standardize\n    \"\"\"\n    if not (kernel is None or bias is None):\n        vecs = (vecs + bias).dot(kernel)\n    return normalize(vecs)\n</code></pre>"},{"location":"reference/run/multi_run/__init__/","title":"init","text":"<p>Auto-discovery for registered datamodules and models.</p> <p>Exposes <code>supported_datamodules</code> and <code>supported_models</code> which lazily populate on first access by importing all submodules and collecting from factory registries.</p>"},{"location":"reference/run/multi_run/cleanup_models/","title":"cleanup_models","text":""},{"location":"reference/run/multi_run/cleanup_models/#run.multi_run.cleanup_models.count_wandb_runs","title":"<code>count_wandb_runs(search_path)</code>","text":"<p>Print the number of wandb runs in each subfolder.</p> <p>Parameters:</p> Name Type Description Default <code>search_path</code> <code>Path</code> <p>Base path to search for wandb runs</p> required Source code in <code>src/run/multi_run/cleanup_models.py</code> <pre><code>def count_wandb_runs(search_path: Path) -&gt; None:\n    \"\"\"\n    Print the number of wandb runs in each subfolder.\n\n    Args:\n        search_path: Base path to search for wandb runs\n    \"\"\"\n    for subfolder in search_path.glob('*'):\n        if not subfolder.is_dir():\n            continue\n\n        for sub_subfolder in subfolder.glob('fold_index=*'):\n            if not sub_subfolder.is_dir():\n                continue\n\n            count = len(list(sub_subfolder.glob('wandb/*run*')))\n            print(f'{sub_subfolder}: {count} wandb runs')\n</code></pre>"},{"location":"reference/run/multi_run/cleanup_models/#run.multi_run.cleanup_models.get_non_lowest_checkpoint_paths","title":"<code>get_non_lowest_checkpoint_paths(search_path, checkpoint_template, keep_one_lowest=False)</code>","text":"<p>Find checkpoint files and return all except those with the lowest score.</p> <p>Parameters:</p> Name Type Description Default <code>search_path</code> <code>Path</code> <p>Path to search for checkpoints</p> required <code>checkpoint_template</code> <code>str</code> <p>Template string to match checkpoint files</p> required <code>keep_one_lowest</code> <code>bool</code> <p>If True, keep only one checkpoint with the lowest score</p> <code>False</code> <p>Returns:</p> Type Description <code>list[Path]</code> <p>List of Path objects for checkpoints that don't have the lowest score</p> Source code in <code>src/run/multi_run/cleanup_models.py</code> <pre><code>def get_non_lowest_checkpoint_paths(\n    search_path: Path, checkpoint_template: str, keep_one_lowest: bool = False\n) -&gt; list[Path]:\n    \"\"\"\n    Find checkpoint files and return all except those with the lowest score.\n\n    Args:\n        search_path: Path to search for checkpoints\n        checkpoint_template: Template string to match checkpoint files\n        keep_one_lowest: If True, keep only one checkpoint with the lowest score\n\n    Returns:\n        List of Path objects for checkpoints that don't have the lowest score\n    \"\"\"\n    full_template = f'*{checkpoint_template}*.ckpt'\n    checkpoint_files = list(search_path.glob(full_template))\n\n    if not checkpoint_files:\n        return []\n\n    # Define regex pattern once to avoid repetition\n    pattern = rf'{checkpoint_template}-(\\d+\\.\\d+)(-v\\d+)?\\.ckpt$'\n\n    def extract_score(file_path: Path) -&gt; float:\n        \"\"\"Extract the score from a checkpoint filename or return infinity if not found.\"\"\"\n        match = re.search(pattern, str(file_path.name))\n        if match:\n            return float(match.group(1))\n        return float('inf')\n\n    checkpoint_files = sorted(checkpoint_files, key=extract_score)\n\n    # Find the minimum score\n    min_score = extract_score(checkpoint_files[0])\n\n    # Keep all models with the minimum score\n    lowest_checkpoints = [f for f in checkpoint_files if extract_score(f) == min_score]\n\n    if keep_one_lowest and lowest_checkpoints:\n        # Keep only one of the lowest models\n        lowest_checkpoints = [lowest_checkpoints[0]]\n\n    # Return all files except the ones with the lowest score\n    return [f for f in checkpoint_files if f not in lowest_checkpoints]\n</code></pre>"},{"location":"reference/run/multi_run/cleanup_models/#run.multi_run.cleanup_models.main","title":"<code>main()</code>","text":"<p>Main function to clean up model checkpoints.</p> Source code in <code>src/run/multi_run/cleanup_models.py</code> <pre><code>def main():\n    \"\"\"Main function to clean up model checkpoints.\"\"\"\n    parser = argparse.ArgumentParser(\n        description='Cleanup model checkpoints, keeping only the best ones.'\n    )\n    parser.add_argument(\n        '--real_run',\n        action='store_true',\n        help='Actually delete files. Without this flag, only reports what would be deleted.',\n    )\n    parser.add_argument(\n        '--keep_one_lowest',\n        action='store_true',\n        help='Keep only one of the lowest scoring models (instead of all with same lowest score).',\n    )\n    parser.add_argument(\n        '--print_num_wandb_runs_in_folder',\n        action='store_true',\n        help='Print the number of wandb runs in each folder.',\n    )\n    args = parser.parse_args()\n\n    search_paths = [\n        Path('.') / 'outputs',\n    ]\n\n    checkpoint_templates = [\n        'lowest_loss_val_all',\n    ]\n\n    for search_path in search_paths:\n        for checkpoint_template in checkpoint_templates:\n            total_sizes = process_checkpoints(\n                search_path=search_path,\n                checkpoint_template=checkpoint_template,\n                keep_one_lowest=args.keep_one_lowest,\n                real_run=args.real_run,\n            )\n\n            if total_sizes:\n                action = 'Deleted' if args.real_run else 'Would delete'\n                print(\n                    f'{action} non-lowest checkpoints for {checkpoint_template} in {search_path}: '\n                    f'{round(sum(total_sizes), 2)} GB (total {len(total_sizes)} files)'\n                )\n\n    if args.print_num_wandb_runs_in_folder:\n        for search_path in search_paths:\n            count_wandb_runs(search_path)\n</code></pre>"},{"location":"reference/run/multi_run/cleanup_models/#run.multi_run.cleanup_models.process_checkpoints","title":"<code>process_checkpoints(search_path, checkpoint_template, keep_one_lowest=False, real_run=False)</code>","text":"<p>Process checkpoints in the given path and return sizes of non-lowest checkpoints.</p> <p>Parameters:</p> Name Type Description Default <code>search_path</code> <code>Path</code> <p>Base path to search for checkpoints</p> required <code>checkpoint_template</code> <code>str</code> <p>Template string to match checkpoint files</p> required <code>keep_one_lowest</code> <code>bool</code> <p>If True, keep only one checkpoint with the lowest score</p> <code>False</code> <code>real_run</code> <code>bool</code> <p>If True, delete non-lowest checkpoints instead of just reporting</p> <code>False</code> <p>Returns:</p> Type Description <code>list[float]</code> <p>List of sizes (in GB) of the non-lowest checkpoints</p> Source code in <code>src/run/multi_run/cleanup_models.py</code> <pre><code>def process_checkpoints(\n    search_path: Path,\n    checkpoint_template: str,\n    keep_one_lowest: bool = False,\n    real_run: bool = False,\n) -&gt; list[float]:\n    \"\"\"\n    Process checkpoints in the given path and return sizes of non-lowest checkpoints.\n\n    Args:\n        search_path: Base path to search for checkpoints\n        checkpoint_template: Template string to match checkpoint files\n        keep_one_lowest: If True, keep only one checkpoint with the lowest score\n        real_run: If True, delete non-lowest checkpoints instead of just reporting\n\n    Returns:\n        List of sizes (in GB) of the non-lowest checkpoints\n    \"\"\"\n    total_sizes = []\n    for subfolder in search_path.glob(pattern='*'):\n        if not subfolder.is_dir():\n            continue\n\n        for sub_subfolder in subfolder.glob(pattern='fold_index=*'):\n            if not sub_subfolder.is_dir():\n                continue\n\n            non_lowest_checkpoints = get_non_lowest_checkpoint_paths(\n                search_path=sub_subfolder,\n                checkpoint_template=checkpoint_template,\n                keep_one_lowest=keep_one_lowest,\n            )\n\n            for checkpoint in non_lowest_checkpoints:\n                size = checkpoint.stat().st_size / (1024 * 1024 * 1024)  # Convert to GB\n                total_sizes.append(size)\n                if real_run:\n                    print(f'Deleting checkpoint: {checkpoint}')\n                    checkpoint.unlink()\n\n    return total_sizes\n</code></pre>"},{"location":"reference/run/multi_run/csv_to_latex/","title":"csv_to_latex","text":"<p>Convert the metric CSV from the EyeBench benchmark into a LaTeX table.</p>"},{"location":"reference/run/multi_run/csv_to_latex/#run.multi_run.csv_to_latex.build_task_wide_df","title":"<code>build_task_wide_df(all_metrics_data_val, all_metrics_data_test, task)</code>","text":"<p>Given all metrics data for val and test splits, build a wide DataFrame where: - Each row is a model (in MODEL_ORDER) - Each column pair is (metric_val, metric_test) - Values are from the 'All' column for the specified task</p> <p>Parameters:</p> Name Type Description Default <code>all_metrics_data_val</code> <code>dict</code> <p>Dict mapping metric name to (df_discri_eval, df_reg_eval) tuples for validation</p> required <code>all_metrics_data_test</code> <code>dict</code> <p>Dict mapping metric name to (df_discri_eval, df_reg_eval) tuples for test</p> required <code>task</code> <code>str</code> <p>The data task to extract metrics for</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>Wide DataFrame with models as rows and metric columns for both val and test</p> Source code in <code>src/run/multi_run/csv_to_latex.py</code> <pre><code>def build_task_wide_df(\n    all_metrics_data_val: dict,\n    all_metrics_data_test: dict,\n    task: str,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Given all metrics data for val and test splits, build a wide DataFrame where:\n    - Each row is a model (in MODEL_ORDER)\n    - Each column pair is (metric_val, metric_test)\n    - Values are from the 'All' column for the specified task\n\n    Args:\n        all_metrics_data_val: Dict mapping metric name to (df_discri_eval, df_reg_eval) tuples for validation\n        all_metrics_data_test: Dict mapping metric name to (df_discri_eval, df_reg_eval) tuples for test\n        task: The data task to extract metrics for\n\n    Returns:\n        Wide DataFrame with models as rows and metric columns for both val and test\n    \"\"\"\n    # Start with the full list of models\n    wide = pd.DataFrame({'Model': MODEL_ORDER_CLASSIFICATION})\n\n    # Determine which metrics apply to this task\n    is_regression_task = task in REG_TASKS\n\n    # Process each metric for both val and test\n    for metric_name in all_metrics_data_test.keys():\n        # Skip regression metric for classification tasks and vice versa\n        if is_regression_task and metric_name not in RegrSupportedMetrics:\n            continue\n        if not is_regression_task and metric_name in RegrSupportedMetrics:\n            continue\n\n        # Process validation data\n        df_discri_eval_val, df_reg_eval_val = all_metrics_data_val[metric_name]\n        if metric_name in RegrSupportedMetrics:\n            df_eval_val = df_reg_eval_val.copy()\n            if not df_eval_val.empty:\n                for ml_col, dl_col in ML_REGRESSION_TO_CLASSIFICATION.items():\n                    df_eval_val.loc[df_eval_val['Model'] == ml_col, 'Model'] = dl_col\n        else:\n            df_eval_val = df_discri_eval_val\n\n        # Process test data\n        df_discri_eval_test, df_reg_eval_test = all_metrics_data_test[metric_name]\n        if metric_name in RegrSupportedMetrics:\n            df_eval_test = df_reg_eval_test.copy()\n            if not df_eval_test.empty:\n                for ml_col, dl_col in ML_REGRESSION_TO_CLASSIFICATION.items():\n                    df_eval_test.loc[df_eval_test['Model'] == ml_col, 'Model'] = dl_col\n        else:\n            df_eval_test = df_discri_eval_test\n\n        # Extract validation data for this task\n        if not df_eval_val.empty:\n            task_data_val = df_eval_val[df_eval_val['Data'] == task]\n            if not task_data_val.empty:\n                col_name = f'{METRICS_LABELS.get(metric_name, metric_name)}_Val'\n                subset = task_data_val.set_index('Model')['All'].rename(col_name)\n                wide = wide.join(subset, on='Model')\n\n        # Extract test data for this task\n        if not df_eval_test.empty:\n            task_data_test = df_eval_test[df_eval_test['Data'] == task]\n            if not task_data_test.empty:\n                col_name = f'{METRICS_LABELS.get(metric_name, metric_name)}_Test'\n                subset = task_data_test.set_index('Model')['All'].rename(col_name)\n                wide = wide.join(subset, on='Model')\n\n    return wide.fillna('')\n</code></pre>"},{"location":"reference/run/multi_run/csv_to_latex/#run.multi_run.csv_to_latex.build_task_wide_df_by_regime","title":"<code>build_task_wide_df_by_regime(all_metrics_data, task, eval_type)</code>","text":"<p>Given all metrics data for a single eval split, build a wide DataFrame where: - Each row is a model (in MODEL_ORDER) - Columns are organized by regime (Unseen Reader, Unseen Text, Unseen Both, All) - For each regime, all relevant metrics are shown - Values are from the specified regime columns</p> <p>Parameters:</p> Name Type Description Default <code>all_metrics_data</code> <code>dict</code> <p>Dict mapping metric name to (df_discri_eval, df_reg_eval) tuples</p> required <code>task</code> <code>str</code> <p>The data task to extract metrics for</p> required <code>eval_type</code> <code>str</code> <p>'val' or 'test'</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>Wide DataFrame with models as rows and (regime, metric) multi-index columns</p> Source code in <code>src/run/multi_run/csv_to_latex.py</code> <pre><code>def build_task_wide_df_by_regime(\n    all_metrics_data: dict,\n    task: str,\n    eval_type: str,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Given all metrics data for a single eval split, build a wide DataFrame where:\n    - Each row is a model (in MODEL_ORDER)\n    - Columns are organized by regime (Unseen Reader, Unseen Text, Unseen Both, All)\n    - For each regime, all relevant metrics are shown\n    - Values are from the specified regime columns\n\n    Args:\n        all_metrics_data: Dict mapping metric name to (df_discri_eval, df_reg_eval) tuples\n        task: The data task to extract metrics for\n        eval_type: 'val' or 'test'\n\n    Returns:\n        Wide DataFrame with models as rows and (regime, metric) multi-index columns\n    \"\"\"\n    # Start with the full list of models\n    wide = pd.DataFrame({'Model': MODEL_ORDER_CLASSIFICATION})\n\n    # Determine which metrics apply to this task\n    is_regression_task = task in REG_TASKS\n\n    # Process each metric\n    for metric_name in all_metrics_data.keys():\n        # Skip regression metric for classification tasks and vice versa\n        if is_regression_task and metric_name not in RegrSupportedMetrics:\n            continue\n        if not is_regression_task and metric_name in RegrSupportedMetrics:\n            continue\n\n        # Get the appropriate dataframe\n        df_discri_eval, df_reg_eval = all_metrics_data[metric_name]\n        if metric_name in RegrSupportedMetrics:\n            df_eval = df_reg_eval.copy()\n            if not df_eval.empty:\n                for ml_col, dl_col in ML_REGRESSION_TO_CLASSIFICATION.items():\n                    df_eval.loc[df_eval['Model'] == ml_col, 'Model'] = dl_col\n        else:\n            df_eval = df_discri_eval\n\n        # Extract data for this task\n        if not df_eval.empty:\n            task_data = df_eval[df_eval['Data'] == task]\n            if not task_data.empty:\n                # For each regime column, add it to the wide dataframe\n                for source_col, regime_label in REGIME_COLS.items():\n                    if source_col in task_data.columns:\n                        col_name = f'{regime_label}_{METRICS_LABELS.get(metric_name, metric_name)}'\n                        subset = task_data.set_index('Model')[source_col].rename(\n                            col_name\n                        )\n                        wide = wide.join(subset, on='Model')\n\n    return wide.fillna('')\n</code></pre>"},{"location":"reference/run/multi_run/csv_to_latex/#run.multi_run.csv_to_latex.build_wide_df","title":"<code>build_wide_df(df_discri_eval, df_reg_eval, include_regression=True)</code>","text":"<p>Given a filtered DataFrame for one eval split, pivot it so that each row is a model (in MODEL_ORDER), each column is a task (in TASK_ORDER), and missing values are kept as empty strings.</p> <p>Parameters:</p> Name Type Description Default <code>df_discri_eval</code> <code>DataFrame</code> <p>DataFrame with classification metrics</p> required <code>df_reg_eval</code> <code>DataFrame</code> <p>DataFrame with regression metrics</p> required <code>include_regression</code> <code>bool</code> <p>If True, include regression tasks. If False, only classification tasks.</p> <code>True</code> Source code in <code>src/run/multi_run/csv_to_latex.py</code> <pre><code>def build_wide_df(\n    df_discri_eval: pd.DataFrame,\n    df_reg_eval: pd.DataFrame,\n    include_regression: bool = True,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Given a filtered DataFrame for one eval split, pivot it so that each\n    row is a model (in MODEL_ORDER), each column is a task (in TASK_ORDER),\n    and missing values are kept as empty strings.\n\n    Args:\n        df_discri_eval: DataFrame with classification metrics\n        df_reg_eval: DataFrame with regression metrics\n        include_regression: If True, include regression tasks. If False, only classification tasks.\n    \"\"\"\n    # keep in df_reg_eval only data_tasks which are for regression\n    if not df_reg_eval.empty:\n        df_reg_eval = df_reg_eval[df_reg_eval['Data'].isin(REG_TASKS)].reset_index(\n            drop=True\n        )\n        # replace some ML columns names in the regression DataFrame using ML_REGRESSION_TO_CLASSIFICATION\n        for ml_col, dl_col in ML_REGRESSION_TO_CLASSIFICATION.items():\n            df_reg_eval.loc[df_reg_eval['Model'] == ml_col, 'Model'] = dl_col\n\n    # keep in df_discri_eval only data_tasks which are for discrete metrics\n    if not df_discri_eval.empty:\n        df_discri_eval = df_discri_eval[~df_discri_eval['Data'].isin(REG_TASKS)]\n\n    # concat based on include_regression flag\n    if include_regression:\n        if not df_discri_eval.empty and not df_reg_eval.empty:\n            df_eval = pd.concat([df_discri_eval, df_reg_eval], ignore_index=True)\n        elif not df_discri_eval.empty:\n            df_eval = df_discri_eval\n        elif not df_reg_eval.empty:\n            df_eval = df_reg_eval\n        else:\n            df_eval = pd.DataFrame()\n    else:\n        df_eval = df_discri_eval\n\n    # start with the full list of models\n    wide = pd.DataFrame({'Model': MODEL_ORDER_CLASSIFICATION})\n\n    # flatten the groups into a single ordered list of datasets\n    task_order = [ds for grp in GROUPS.values() for ds in grp]\n\n    # Filter task_order based on include_regression flag\n    if not include_regression:\n        task_order = [ds for ds in task_order if ds not in REG_TASKS]\n\n    if not df_eval.empty:\n        for ds in task_order:\n            col_name = DATASET_TO_COLUMN.get(ds, ds)\n            subset = (\n                df_eval[df_eval['Data'] == ds]\n                .set_index('Model')['All']\n                .rename(col_name)\n            )\n            # join will insert NaN for models with no data\n            wide = wide.join(subset, on='Model')\n\n    return wide.fillna('')\n</code></pre>"},{"location":"reference/run/multi_run/csv_to_latex/#run.multi_run.csv_to_latex.compute_aggregated_results","title":"<code>compute_aggregated_results(df_discri_eval, df_reg_eval)</code>","text":"<p>Compute two aggregated versions of the results: 1. Normalized scores: Normalize the scores then take mean across metrics and tasks 2. Ranking: For each metric and task, compute ranking of all models, then average rank</p> <p>Parameters:</p> Name Type Description Default <code>df_discri_eval</code> <code>DataFrame</code> <p>DataFrame with discrete metrics (accuracy, auroc, etc.)</p> required <code>df_reg_eval</code> <code>DataFrame</code> <p>DataFrame with regression metrics (rmse)</p> required <p>Returns:</p> Type Description <code>tuple[DataFrame, DataFrame]</code> <p>tuple of (normalized_agg_df, ranking_agg_df)</p> Source code in <code>src/run/multi_run/csv_to_latex.py</code> <pre><code>def compute_aggregated_results(\n    df_discri_eval: pd.DataFrame, df_reg_eval: pd.DataFrame\n) -&gt; tuple[pd.DataFrame, pd.DataFrame]:\n    \"\"\"\n    Compute two aggregated versions of the results:\n    1. Normalized scores: Normalize the scores then take mean across metrics and tasks\n    2. Ranking: For each metric and task, compute ranking of all models, then average rank\n\n    Args:\n        df_discri_eval: DataFrame with discrete metrics (accuracy, auroc, etc.)\n        df_reg_eval: DataFrame with regression metrics (rmse)\n\n    Returns:\n        tuple of (normalized_agg_df, ranking_agg_df)\n    \"\"\"\n\n    # Prepare data for aggregation\n    df_reg_eval = df_reg_eval[df_reg_eval['Data'].isin(REG_TASKS)].reset_index(\n        drop=True\n    )\n    df_discri_eval = df_discri_eval[\n        ~df_discri_eval['Data'].isin(REG_TASKS)\n    ].reset_index(drop=True)\n\n    # Replace some ML column names in the regression DataFrame\n    for ml_col, dl_col in ML_REGRESSION_TO_CLASSIFICATION.items():\n        df_reg_eval.loc[df_reg_eval['Model'] == ml_col, 'Model'] = dl_col\n\n    # Combine all data\n    df_combined = pd.concat([df_discri_eval, df_reg_eval], ignore_index=True)\n\n    # Get all unique model-task combinations\n    results_list = []\n    rankings_list = []\n\n    # Process each task\n    task_order = [ds for grp in GROUPS.values() for ds in grp]\n\n    logger.info(f'Processing {len(task_order)} tasks for aggregation: {task_order}')\n\n    for task in task_order:\n        task_data = df_combined[df_combined['Data'] == task].copy()\n        if task_data.empty:\n            logger.warning(f'No data found for task: {task}')\n            continue\n\n        # Get the metric column (should be 'All')\n        scores = task_data[['Model', 'All']].copy()\n        scores = scores.dropna()\n\n        if scores.empty:\n            logger.warning(f'No valid scores found for task: {task}')\n            continue\n\n        # Extract numeric values from the string format\n        scores['numeric_score'] = scores['All'].apply(extract_numeric_value)\n        scores = scores.dropna(subset=['numeric_score'])\n\n        if len(scores) &lt; 2:  # Need at least 2 models for meaningful comparison\n            logger.warning(f'Less than 2 valid scores for task: {task}')\n            continue\n\n        logger.info(f'Processing task {task} with {len(scores)} models')\n\n        # For RMSE (regression), lower is better, so we invert for normalization\n        if task in REG_TASKS:\n            # For ranking: rank ascending (lower RMSE = better rank)\n            scores['rank'] = scores['numeric_score'].rank(method='min', ascending=True)\n            # For normalization: invert RMSE so higher normalized score = better\n            max_rmse = scores['numeric_score'].max()\n            min_rmse = scores['numeric_score'].min()\n            if max_rmse != min_rmse:\n                scores['normalized'] = 1 - (scores['numeric_score'] - min_rmse) / (\n                    max_rmse - min_rmse\n                )\n            else:\n                scores['normalized'] = 0.5  # All same, assign middle value\n        else:\n            # For other metrics (accuracy, auroc, etc.), higher is better\n            scores['rank'] = scores['numeric_score'].rank(method='min', ascending=False)\n            # Normalize to 0-1 range\n            max_score = scores['numeric_score'].max()\n            min_score = scores['numeric_score'].min()\n            if max_score != min_score:\n                scores['normalized'] = (scores['numeric_score'] - min_score) / (\n                    max_score - min_score\n                )\n            else:\n                scores['normalized'] = 0.5  # All same, assign middle value\n\n        # Add task info\n        scores['Task'] = task\n        results_list.append(scores[['Model', 'Task', 'normalized']])\n        rankings_list.append(scores[['Model', 'Task', 'rank']])\n\n    if not results_list:\n        logger.warning('No results to aggregate')\n        # Return empty DataFrames if no data\n        return pd.DataFrame(), pd.DataFrame()\n\n    # Combine all results\n    all_normalized = pd.concat(results_list, ignore_index=True)\n    all_rankings = pd.concat(rankings_list, ignore_index=True)\n\n    logger.info(\n        f'Aggregating results for {len(all_normalized[\"Model\"].unique())} models across {len(all_normalized[\"Task\"].unique())} tasks'\n    )\n\n    # Compute aggregated normalized scores (mean across tasks)\n    normalized_agg = all_normalized.groupby('Model')['normalized'].mean().reset_index()\n    normalized_agg.columns = ['Model', 'Avg_Normalized_Score']\n    normalized_agg = normalized_agg.sort_values('Avg_Normalized_Score', ascending=False)\n\n    # Compute aggregated rankings (mean rank across tasks)\n    ranking_agg = all_rankings.groupby('Model')['rank'].mean().reset_index()\n    ranking_agg.columns = ['Model', 'Avg_Rank']\n    ranking_agg = ranking_agg.sort_values('Avg_Rank', ascending=True)\n\n    logger.info(\n        f'Generated aggregated results: {len(normalized_agg)} models in normalized scores, {len(ranking_agg)} models in rankings'\n    )\n\n    return normalized_agg, ranking_agg\n</code></pre>"},{"location":"reference/run/multi_run/csv_to_latex/#run.multi_run.csv_to_latex.compute_aggregated_results_across_all_metrics","title":"<code>compute_aggregated_results_across_all_metrics(all_metrics_data)</code>","text":"<p>Compute aggregated results across ALL metrics and tasks.</p> <p>Parameters:</p> Name Type Description Default <code>all_metrics_data</code> <code>dict</code> <p>Dict mapping metric name to (df_discri_eval, df_reg_eval) tuples</p> required <p>Returns:</p> Type Description <code>tuple[DataFrame, DataFrame]</code> <p>tuple of (normalized_agg_df, ranking_agg_df)</p> Source code in <code>src/run/multi_run/csv_to_latex.py</code> <pre><code>def compute_aggregated_results_across_all_metrics(\n    all_metrics_data: dict,\n) -&gt; tuple[pd.DataFrame, pd.DataFrame]:\n    \"\"\"\n    Compute aggregated results across ALL metrics and tasks.\n\n    Args:\n        all_metrics_data: Dict mapping metric name to (df_discri_eval, df_reg_eval) tuples\n\n    Returns:\n        tuple of (normalized_agg_df, ranking_agg_df)\n    \"\"\"\n\n    results_list = []\n    rankings_list = []\n\n    task_order = [ds for grp in GROUPS.values() for ds in grp]\n\n    logger.info(\n        f'Computing aggregated results across {len(all_metrics_data)} metrics and {len(task_order)} tasks'\n    )\n\n    # Process each metric\n    for metric_name, (df_discri_eval, df_reg_eval) in all_metrics_data.items():\n        # Prepare data for this metric\n        if not df_reg_eval.empty:\n            df_reg_eval = df_reg_eval[df_reg_eval['Data'].isin(REG_TASKS)].reset_index(\n                drop=True\n            )\n            # Replace some ML column names in the regression DataFrame\n            for ml_col, dl_col in ML_REGRESSION_TO_CLASSIFICATION.items():\n                df_reg_eval.loc[df_reg_eval['Model'] == ml_col, 'Model'] = dl_col\n\n        if not df_discri_eval.empty:\n            df_discri_eval = df_discri_eval[\n                ~df_discri_eval['Data'].isin(REG_TASKS)\n            ].reset_index(drop=True)\n\n        # Combine all data for this metric\n        if not df_discri_eval.empty and not df_reg_eval.empty:\n            df_combined = pd.concat([df_discri_eval, df_reg_eval], ignore_index=True)\n        elif not df_discri_eval.empty:\n            df_combined = df_discri_eval\n        elif not df_reg_eval.empty:\n            df_combined = df_reg_eval\n        else:\n            continue  # Skip if both are empty\n\n        # Process each task for this metric\n        for task in task_order:\n            task_data = df_combined[df_combined['Data'] == task].copy()\n            if task_data.empty:\n                continue\n\n            scores = task_data[['Model', 'All']].copy()\n            scores = scores.dropna()\n\n            if scores.empty:\n                continue\n\n            # Extract numeric values\n            scores['numeric_score'] = scores['All'].apply(extract_numeric_value)\n            scores = scores.dropna(subset=['numeric_score'])\n\n            if len(scores) &lt; 2:\n                continue\n\n            # Determine if higher is better based on the metric, not the task type\n            higher_is_better = is_metric_higher_better(metric_name)\n\n            if higher_is_better:\n                # For metrics where higher is better (e.g., AUROC, R2)\n                scores['rank'] = scores['numeric_score'].rank(\n                    method='min', ascending=False\n                )\n                max_score = scores['numeric_score'].max()\n                min_score = scores['numeric_score'].min()\n                if max_score != min_score:\n                    scores['normalized'] = (scores['numeric_score'] - min_score) / (\n                        max_score - min_score\n                    )\n                else:\n                    scores['normalized'] = 0.5\n            else:\n                # For metrics where lower is better (e.g., RMSE, MAE)\n                scores['rank'] = scores['numeric_score'].rank(\n                    method='min', ascending=True\n                )\n                max_val = scores['numeric_score'].max()\n                min_val = scores['numeric_score'].min()\n                if max_val != min_val:\n                    scores['normalized'] = 1 - (scores['numeric_score'] - min_val) / (\n                        max_val - min_val\n                    )\n                else:\n                    scores['normalized'] = 0.5\n\n            # Add task and metric info\n            scores['Task'] = task\n            scores['Metric'] = metric_name\n            results_list.append(scores[['Model', 'Task', 'Metric', 'normalized']])\n            rankings_list.append(scores[['Model', 'Task', 'Metric', 'rank']])\n\n    if not results_list:\n        logger.warning('No results to aggregate across metrics')\n        return pd.DataFrame(), pd.DataFrame()\n\n    # Combine all results across all metrics and tasks\n    all_normalized = pd.concat(results_list, ignore_index=True)\n    all_rankings = pd.concat(rankings_list, ignore_index=True)\n\n    logger.info(\n        f'Aggregating results for {len(all_normalized[\"Model\"].unique())} models across {len(all_normalized[\"Task\"].unique())} tasks and {len(all_normalized[\"Metric\"].unique())} metrics'\n    )\n\n    # Compute aggregated normalized scores (mean across tasks and metrics)\n    normalized_agg = all_normalized.groupby('Model')['normalized'].mean().reset_index()\n    normalized_agg.columns = ['Model', 'Avg_Normalized_Score']\n    normalized_agg = normalized_agg.sort_values('Avg_Normalized_Score', ascending=False)\n\n    # Compute aggregated rankings (mean rank across tasks and metrics)\n    ranking_agg = all_rankings.groupby('Model')['rank'].mean().reset_index()\n    ranking_agg.columns = ['Model', 'Avg_Rank']\n    ranking_agg = ranking_agg.sort_values('Avg_Rank', ascending=True)\n\n    logger.info(\n        f'Generated aggregated results: {len(normalized_agg)} models in normalized scores, {len(ranking_agg)} models in rankings'\n    )\n\n    return normalized_agg, ranking_agg\n</code></pre>"},{"location":"reference/run/multi_run/csv_to_latex/#run.multi_run.csv_to_latex.extract_numeric_value","title":"<code>extract_numeric_value(val_str)</code>","text":"<p>Extract numeric value from string format like '65.0 \u00b1 0.0'</p> Source code in <code>src/run/multi_run/csv_to_latex.py</code> <pre><code>def extract_numeric_value(val_str):\n    \"\"\"Extract numeric value from string format like '65.0 \u00b1 0.0'\"\"\"\n    if pd.isna(val_str) or val_str == '' or val_str == '-':\n        return np.nan\n    val_str = str(val_str)\n    try:\n        return float(val_str.split(' \u00b1')[0])\n    except (ValueError, IndexError):\n        try:\n            return float(val_str)\n        except ValueError:\n            return np.nan\n</code></pre>"},{"location":"reference/run/multi_run/csv_to_latex/#run.multi_run.csv_to_latex.find_best_indices","title":"<code>find_best_indices(numeric_values, higher_is_better)</code>","text":"<p>Find all indices with the best value (handles ties).</p> <p>Parameters:</p> Name Type Description Default <code>numeric_values</code> <code>Series</code> <p>Series of numeric values</p> required <code>higher_is_better</code> <code>bool</code> <p>If True, find max values; if False, find min values</p> required <p>Returns:</p> Type Description <code>list</code> <p>List of indices with the best value</p> Source code in <code>src/run/multi_run/csv_to_latex.py</code> <pre><code>def find_best_indices(numeric_values: pd.Series, higher_is_better: bool) -&gt; list:\n    \"\"\"\n    Find all indices with the best value (handles ties).\n\n    Args:\n        numeric_values: Series of numeric values\n        higher_is_better: If True, find max values; if False, find min values\n\n    Returns:\n        List of indices with the best value\n    \"\"\"\n    if numeric_values.notna().sum() == 0:\n        return []\n\n    if higher_is_better:\n        best_value = numeric_values.max()\n    else:\n        best_value = numeric_values.min()\n\n    # Find all indices with the best value (handles ties)\n    best_indices = numeric_values[numeric_values == best_value].index.tolist()\n    return best_indices\n</code></pre>"},{"location":"reference/run/multi_run/csv_to_latex/#run.multi_run.csv_to_latex.format_value_with_subscript","title":"<code>format_value_with_subscript(value)</code>","text":"<p>Format a value string to use LaTeX subscript for standard deviation.</p> <p>Converts '65.0 \u00b1 2.3' to '65.0\\textsubscript{\u00b12.3}'</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>str</code> <p>String in format 'mean \u00b1 std' or just a value</p> required <p>Returns:</p> Type Description <code>str</code> <p>Formatted string with std as subscript</p> Source code in <code>src/run/multi_run/csv_to_latex.py</code> <pre><code>def format_value_with_subscript(value: str) -&gt; str:\n    \"\"\"\n    Format a value string to use LaTeX subscript for standard deviation.\n\n    Converts '65.0 \u00b1 2.3' to '65.0\\\\textsubscript{\u00b12.3}'\n\n    Args:\n        value: String in format 'mean \u00b1 std' or just a value\n\n    Returns:\n        Formatted string with std as subscript\n    \"\"\"\n    if not value or value == '' or value == '-':\n        return value\n\n    value_str = str(value)\n    if ' \u00b1 ' in value_str:\n        mean, std = value_str.split(' \u00b1 ')\n        return f'{mean}\\\\textsubscript{{\u00b1{std}}}'\n    return value_str\n</code></pre>"},{"location":"reference/run/multi_run/csv_to_latex/#run.multi_run.csv_to_latex.generate_aggregated_latex_table","title":"<code>generate_aggregated_latex_table(normalized_agg, ranking_agg, eval_type)</code>","text":"<p>Generate a LaTeX table showing both aggregated results with feature types.</p> Source code in <code>src/run/multi_run/csv_to_latex.py</code> <pre><code>def generate_aggregated_latex_table(\n    normalized_agg: pd.DataFrame, ranking_agg: pd.DataFrame, eval_type: str\n) -&gt; tuple[str, pd.DataFrame]:\n    \"\"\"\n    Generate a LaTeX table showing both aggregated results with feature types.\n    \"\"\"\n    # Define feature types for each model (hardcoded based on the table provided)\n\n    # Merge the two aggregations\n    merged = pd.merge(normalized_agg, ranking_agg, on='Model', how='outer')\n\n    # Format model names\n    merged['Model_Display'] = merged['Model'].map(MODEL_TO_COLUMN)\n\n    # Round values for display\n    merged['Avg_Normalized_Score'] = merged['Avg_Normalized_Score'].round(3)\n    merged['Avg_Rank'] = merged['Avg_Rank'].round(2)\n\n    # Sort by the same order as other tables (MODEL_ORDER_CLASSIFICATION)\n    # Create a mapping from model name to its position in MODEL_ORDER_CLASSIFICATION\n    model_order_map = {model: i for i, model in enumerate(MODEL_ORDER_CLASSIFICATION)}\n    merged['order'] = merged['Model'].map(model_order_map)\n    merged = merged.sort_values('order')\n    merged = merged.drop(columns=['order'])\n\n    # Find best models (handles ties)\n    best_norm_score_indices = find_best_indices(\n        merged['Avg_Normalized_Score'], higher_is_better=True\n    )\n    best_rank_indices = find_best_indices(merged['Avg_Rank'], higher_is_better=False)\n\n    # Build DataFrame representing the LaTeX table contents\n    feature_cols = [\n        'Layout',\n        'Saccade/Fixation',\n        'Word-Level',\n        'Trial-Level',\n        'Linguistic',\n        'Embeddings',\n    ]\n    csv_rows: list[dict[str, object]] = []\n    for _, row in merged.iterrows():\n        model_key = row['Model']\n        display_name = (\n            row['Model_Display'] if pd.notna(row['Model_Display']) else model_key\n        )\n        features = FEATURE_TYPES.get(model_key, {})\n        csv_row = {'Model': display_name}\n        for feature_col in feature_cols:\n            csv_row[feature_col] = features.get(feature_col, '-')\n        csv_row['Avg Normalized Score'] = row['Avg_Normalized_Score']\n        csv_row['Mean Rank'] = row['Avg_Rank']\n        csv_rows.append(csv_row)\n\n    csv_table = pd.DataFrame(csv_rows)\n\n    header = dedent(\"\"\"\n    \\\\begin{table}[ht]\n    \\\\centering\n    \\\\caption{Feature types used by each model, and aggregated model performance across all benchmark tasks and metrics. \\\\textbf{Layout} stands for information about the position of the text or fixations on the screen. Eye movement features are divided into three levels of granularity: \\\\textbf{Saccades/Fixations} (e.g., fixation duration), \\\\textbf{Words} (e.g., total fixation duration on a given word), and \\\\textbf{Trial} (e.g., average total fixation duration across all the words during the trial). Text features are divided into: \\\\textbf{Linguistic} word properties (e.g., word frequency) and contextual word \\\\textbf{Embeddings} (e.g., RoBERTa embeddings). \\\\textbf{Average Normalized Score} is the mean of min-max normalized scores across all tasks and metrics (higher is better). \\\\textbf{Mean Rank} is the mean ranking across all tasks and metrics (lower is better). Best performing model for each aggregation metric is shown in \\\\textbf{bold}.}\n    \\\\resizebox{\\\\linewidth}{!}{%\n    \\\\begin{tabular}{l|c|ccc|cc||cc}\n    \\\\toprule\n    \\\\textbf{Model} &amp; \\\\multicolumn{1}{c|}{\\\\textbf{Layout}} &amp; \\\\multicolumn{3}{c|}{\\\\textbf{Eye movement features}} &amp; \\\\multicolumn{2}{c||}{\\\\textbf{Text features}}  &amp; \\\\multicolumn{2}{c}{\\\\textbf{Aggregated performance}}\\\\\\\\\n    &amp;  &amp; \\\\makecell{Saccade/\\\\\\\\Fixation Level} &amp; \\\\makecell{Word\\\\\\\\Level} &amp; \\\\makecell{Trial\\\\\\\\Level} &amp; Linguistic &amp; Embeddings &amp; Avg. Normalized Score$\\\\uparrow$ &amp; Mean Rank$\\\\downarrow$\\\\\\\\\n    \\\\midrule\n    \"\"\")\n\n    body = ''\n    for idx, row in merged.iterrows():\n        model_name = (\n            row['Model_Display'] if pd.notna(row['Model_Display']) else row['Model']\n        )\n\n        # Get feature types for this model\n        features = FEATURE_TYPES[row['Model']]\n\n        # Format scores with bold for best\n        if pd.notna(row['Avg_Normalized_Score']):\n            norm_score = f'{row[\"Avg_Normalized_Score\"]:.3f}'\n            if idx in best_norm_score_indices:\n                norm_score = f'\\\\textbf{{{norm_score}}}'\n        else:\n            norm_score = '--'\n\n        if pd.notna(row['Avg_Rank']):\n            avg_rank = f'{row[\"Avg_Rank\"]:.2f}'\n            if idx in best_rank_indices:\n                avg_rank = f'\\\\textbf{{{avg_rank}}}'\n        else:\n            avg_rank = '--'\n\n        # Build row with feature types and aggregated performance\n        body += f'{model_name} &amp; {features[\"Layout\"]} &amp; {features[\"Saccade/Fixation\"]} &amp; {features[\"Word-Level\"]} &amp; {features[\"Trial-Level\"]} &amp; {features[\"Linguistic\"]} &amp; {features[\"Embeddings\"]} &amp; {norm_score} &amp; {avg_rank} \\\\\\\\\\n'\n\n        # Add horizontal lines after certain model groups\n        if row['Model'] in ['Roberta', 'RandomForestMLArgs']:\n            body += '\\\\midrule\\n'\n\n    footer = dedent(f\"\"\"\n    \\\\bottomrule\n    \\\\end{{tabular}}%\n    }}\n    \\\\label{{tab:features-per-model-results-{eval_type}}}\n    \\\\end{{table}}\n    \"\"\")\n\n    return header + body + footer, csv_table.reset_index(drop=True)\n</code></pre>"},{"location":"reference/run/multi_run/csv_to_latex/#run.multi_run.csv_to_latex.generate_breakdown_tables","title":"<code>generate_breakdown_tables(df, metric, metric_type)</code>","text":"<p>For each task, generate a LaTeX table showing per-model performance across evaluation regimes: unseen subject, unseen item, unseen both, and all.</p> Source code in <code>src/run/multi_run/csv_to_latex.py</code> <pre><code>def generate_breakdown_tables(df: pd.DataFrame, metric: str, metric_type: str):\n    \"\"\"\n    For each task, generate a LaTeX table showing per-model performance\n    across evaluation regimes: unseen subject, unseen item, unseen both, and all.\n    \"\"\"\n    eval_cols = [\n        'Unseen subject seen item',\n        'Seen subject unseen item',\n        'Unseen subject unseen item',\n        'All',\n    ]\n\n    for task_key, task_label in DATASET_TO_COLUMN.items():\n        df_task = df[df['Data'] == task_key].copy()\n\n        # Ensure all models from MODEL_ORDER are present\n        # Create a dataframe with all models\n        all_models_df = pd.DataFrame({'Model': MODEL_ORDER_BY_METRIC_TYPE[metric_type]})\n\n        # Merge with existing data, keeping all models\n        if not df_task.empty:\n            df_task = df_task[['Model'] + eval_cols]\n            df_task = all_models_df.merge(df_task, on='Model', how='left')\n        else:\n            df_task = all_models_df\n            for col in eval_cols:\n                df_task[col] = ''\n\n        # Replace NaN with '-'\n        df_task[eval_cols] = df_task[eval_cols].fillna('-')\n\n        # Replace empty strings with '-'\n        for col in eval_cols:\n            df_task[col] = df_task[col].replace('', '-')\n\n        # Save original model names before formatting\n        df_task['Model_Original'] = df_task['Model']\n\n        # Format model names\n        df_task['Model'] = df_task['Model'].map(MODEL_TO_COLUMN)\n        df_task['Model'] = df_task['Model'].fillna(df_task['Model_Original'])\n\n        # Find best model for each evaluation column\n        # Determine if higher or lower is better based on metric\n        is_higher_better = is_metric_higher_better(metric)\n\n        best_indices = {}\n        for col in eval_cols:\n            numeric_vals = df_task[col].apply(extract_numeric_value)\n            if numeric_vals.notna().any():\n                # Find all best indices (handles ties)\n                best_idx_list = find_best_indices(numeric_vals, is_higher_better)\n                best_indices[col] = best_idx_list\n            else:\n                best_indices[col] = []\n\n        # Build LaTeX\n        header = dedent(\n            rf\"\"\"\n        \\begin{{table}}[ht]\n        \\centering\n        \\caption{{{METRICS_LABELS[metric]} performance for task: \\textbf{{{task_label}}}, broken down by evaluation regime.}}\n        \\resizebox{{\\textwidth}}{{!}}{{%\n        \\begin{{tabular}}{{l|ccc|c}}\n        \\toprule\n        \\textbf{{Model}} &amp; \\textbf{{Unseen Reader}} &amp; \\textbf{{Unseen Text}} &amp; \\textbf{{Unseen Reader \\&amp; Text}} &amp; \\textbf{{All}} \\\\\n        \\midrule\n        \"\"\"\n        )\n\n        rows = ''\n        for idx, row in df_task.iterrows():\n            cells = [str(row['Model'])]\n\n            # Format each evaluation column, bolding if it's the best\n            for col in eval_cols:\n                val = str(row[col])\n                # Format with subscript for std deviation\n                val = format_value_with_subscript(val)\n                if idx in best_indices[col] and val != '-':\n                    val = f'\\\\textbf{{{val}}}'\n                cells.append(val)\n\n            rows += ' &amp; '.join(cells) + ' \\\\\\\\\\n'\n            if row['Model_Original'] in ['Roberta', 'RandomForestMLArgs']:\n                rows += '\\\\midrule\\n'\n\n        footer = dedent(f\"\"\"\n        \\\\bottomrule\n        \\\\end{{tabular}}%\n        }}\n        \\\\label{{tab:task-breakdown-{task_key.lower()}-{metric}}}\n        \\\\end{{table}}\n        \"\"\")\n\n        # Save to both locations\n        latex_content = header + rows + footer\n        save_to_both_locations(\n            latex_content, f'breakdown/{task_key}_{metric}.tex', is_csv=False\n        )\n        csv_output = df_task.drop(columns=['Model_Original'], errors='ignore')\n        save_to_both_locations(\n            csv_output, f'breakdown/{task_key}_{metric}.csv', is_csv=True\n        )\n</code></pre>"},{"location":"reference/run/multi_run/csv_to_latex/#run.multi_run.csv_to_latex.generate_combined_table","title":"<code>generate_combined_table(df_auroc_test, df_rmse_test)</code>","text":"<p>Generate a combined LaTeX table showing AUROC for classification tasks and RMSE for regression tasks in a single table.</p> <p>The table groups tasks by type: - Classification tasks (Reading Comprehension, Domain Expertise, Claim Verification, Dyslexia Detection) - Regression tasks (Subj. Text Difficulty, Reading Compr. Skill, Vocab. Knowledge)</p> <p>Parameters:</p> Name Type Description Default <code>df_auroc_test</code> <code>DataFrame</code> <p>DataFrame with AUROC results for test split</p> required <code>df_rmse_test</code> <code>DataFrame</code> <p>DataFrame with RMSE results for test split</p> required <p>Returns:</p> Type Description <code>tuple[str, DataFrame]</code> <p>Tuple containing the LaTeX table string and a DataFrame representation</p> Source code in <code>src/run/multi_run/csv_to_latex.py</code> <pre><code>def generate_combined_table(\n    df_auroc_test: pd.DataFrame,\n    df_rmse_test: pd.DataFrame,\n) -&gt; tuple[str, pd.DataFrame]:\n    \"\"\"\n    Generate a combined LaTeX table showing AUROC for classification tasks\n    and RMSE for regression tasks in a single table.\n\n    The table groups tasks by type:\n    - Classification tasks (Reading Comprehension, Domain Expertise, Claim Verification, Dyslexia Detection)\n    - Regression tasks (Subj. Text Difficulty, Reading Compr. Skill, Vocab. Knowledge)\n\n    Args:\n        df_auroc_test: DataFrame with AUROC results for test split\n        df_rmse_test: DataFrame with RMSE results for test split\n\n    Returns:\n        Tuple containing the LaTeX table string and a DataFrame representation\n    \"\"\"\n\n    all_tasks = classification_tasks + regression_tasks\n\n    # Prepare RMSE data: replace ML model names with DL equivalents\n    df_rmse_processed = df_rmse_test.copy()\n    for ml_col, dl_col in ML_REGRESSION_TO_CLASSIFICATION.items():\n        df_rmse_processed.loc[df_rmse_processed['Model'] == ml_col, 'Model'] = dl_col\n\n    # Build wide dataframe with all models\n    wide = pd.DataFrame({'Model': MODEL_ORDER_CLASSIFICATION})\n\n    # Add AUROC values for classification tasks\n    for task in classification_tasks:\n        task_data = df_auroc_test[df_auroc_test['Data'] == task]\n        if not task_data.empty:\n            subset = task_data.set_index('Model')['All'].rename(task)\n            wide = wide.join(subset, on='Model')\n\n    # Add RMSE values for regression tasks\n    for task in regression_tasks:\n        task_data = df_rmse_processed[df_rmse_processed['Data'] == task]\n        if not task_data.empty:\n            subset = task_data.set_index('Model')['All'].rename(task)\n            wide = wide.join(subset, on='Model')\n\n    # Replace NaN with empty string\n    wide = wide.fillna('')\n\n    # Find best model for each task\n    best_by_task = {}\n    for task in all_tasks:\n        if task not in wide.columns:\n            continue\n\n        numeric_values = wide[task].apply(extract_numeric_value)\n\n        # For regression tasks, use RMSE (lower is better); for classification, use AUROC (higher is better)\n        if task in regression_tasks:\n            higher_is_better = is_metric_higher_better('rmse')\n        else:\n            higher_is_better = is_metric_higher_better('auroc')\n\n        # Find all best indices (handles ties)\n        best_indices = find_best_indices(numeric_values, higher_is_better)\n        if best_indices:\n            best_by_task[task] = [wide.loc[idx, 'Model'] for idx in best_indices]\n\n    # Build table header\n    num_classification = len(classification_tasks)\n    num_regression = len(regression_tasks)\n\n    col_fmt = 'l|' + 'c' * num_classification + '|' + 'c' * num_regression\n\n    # Build second header row with task types\n    header_row2 = ' '\n\n    # Mapping of special commands per task\n    special_cmds_before = {\n        'OneStop_RC': r'',\n        'SBSAT_RC': r'\\newsetup{}',\n        'PoTeC_RC': r'\\newthing{}',\n        'PoTeC_DE': r'\\newthing{}',\n        'IITBHGC_CV': r'\\newthing{}',\n        'CopCo_TYP': r'',\n        'SB-SAT_REGR': r'',\n        'CopCo_REGR': r'',\n        'MECO_L2_REGR': r'\\newthing{}',\n    }\n    special_cmds_after = {\n        'OneStop_RC': r'',\n        'SBSAT_RC': r'',\n        'PoTeC_RC': r'',\n        'PoTeC_DE': r'\\woman{}+\\page{}',\n        'IITBHGC_CV': r'\\woman{}+\\page{}',\n        'CopCo_TYP': r'\\woman{}',\n        'SB-SAT_REGR': r'\\woman{}+\\page{}',\n        'CopCo_REGR': r'\\woman{}',\n        'MECO_L2_REGR': r'\\woman{}',\n    }\n\n    # Reading Comprehension tasks (first 3 classification tasks)\n    reading_compr_tasks = ['OneStop_RC', 'SBSAT_RC', 'PoTeC_RC']\n    num_reading_compr = len(\n        [t for t in reading_compr_tasks if t in classification_tasks]\n    )\n    if num_reading_compr &gt; 0:\n        header_row2 += f' &amp; \\\\multicolumn{{{num_reading_compr}}}{{c}}{{Reading Comprehension\\\\woman{{}}+\\\\page{{}}}}'\n\n    # Other classification tasks\n    other_class_tasks = ['PoTeC_DE', 'IITBHGC_CV', 'CopCo_TYP']\n    for task in other_class_tasks:\n        if task in classification_tasks:\n            label, _ = task_headers.get(task, ('Unknown', 'Unknown'))\n            cmd = special_cmds_before.get(task, '')\n            cmd2 = special_cmds_after.get(task, '')\n            if cmd:\n                label = f'{cmd} {label} {cmd2}'\n            header_row2 += f' &amp; \\\\makecell{{{label}}}'\n\n    # Regression tasks\n    for task in regression_tasks:\n        label, _ = task_headers.get(task, ('Unknown', 'Unknown'))\n        cmd = special_cmds_before.get(task, '')\n        cmd2 = special_cmds_after.get(task, '')\n        if cmd:\n            label = f'{cmd} {label} {cmd2}'\n        header_row2 += f' &amp; \\\\makecell{{{label}}}'\n\n    header_row2 += '\\\\\\\\\\n'\n\n    # Build third header row with dataset names\n    header_row3 = ' '\n    for task in classification_tasks + regression_tasks:\n        _, dataset = task_headers.get(task, ('Unknown', 'Unknown'))\n        header_row3 += f' &amp; {dataset}'\n    header_row3 += '\\\\\\\\\\n'\n\n    header = dedent(f\"\"\"\n    \\\\begin{{table}}[ht]\n    \\\\centering\n    \\\\caption{{Model performance across the benchmark tasks and datasets on test data. \\\\textbf{{AUROC}} (higher is better) for classification tasks and \\\\textbf{{RMSE}} (lower is better) for regression tasks. The best performing model per task and dataset is shown in \\\\textbf{{bold}}. Reported values indicate mean~$\\\\pm$~standard error across folds. The tasks belong to two categories, where \\\\woman{{}} indicates a reader characteristic prediction task and \\\\woman{{}}+\\\\page{{}} an interaction of a reader with a text. \\\\newthing{{}} indicates new tasks and task-dataset combinations introduced in \\\\benchmarkname. \\\\newsetup{{}} indicates a new experimental setup for the task-dataset combination.}}\n    \\\\resizebox{{\\\\textwidth}}{{!}}{{%\n    \\\\begin{{tabular}}{{@{{}}{col_fmt}@{{}}}}\n    \\\\toprule\n     &amp; \\\\multicolumn{{{num_classification}}}{{c|}}{{\\\\textbf{{Classification (AUROC$\\\\uparrow$)}}}} &amp; \\\\multicolumn{{{num_regression}}}{{c}}{{\\\\textbf{{Regression (RMSE$\\\\downarrow$)}}}} \\\\\\\\\n    \\\\addlinespace{header_row2}{header_row3}\\\\midrule\n    \\\\addlinespace\n    \"\"\")\n\n    # Build table body\n    body = ''\n    models_in_table: list[str] = []\n    for _, row in wide.iterrows():\n        model_name = str(row['Model'])\n\n        # Skip models with no data - check if all task values are empty\n        has_data = False\n        for task in all_tasks:\n            if task in wide.columns:\n                val_str = str(row[task])\n                if val_str and val_str != '' and val_str != 'nan':\n                    has_data = True\n                    break\n\n        if not has_data:\n            continue\n\n        models_in_table.append(model_name)\n\n        # First column: model name\n        model_label = MODEL_TO_COLUMN.get(model_name, model_name)\n        if model_label is None:\n            model_label = model_name\n        cells: list[str] = [model_label]\n\n        # Add values for each task (only tasks with data)\n        for task in classification_tasks + regression_tasks:\n            if task not in wide.columns:\n                cells.append('-')\n            else:\n                value = str(row[task])\n                if value == '' or value == 'nan':\n                    cells.append('-')\n                else:\n                    # Format with subscript for std deviation\n                    if ' \u00b1 ' in value:\n                        mean, std = value.split(' \u00b1 ')\n                        formatted_value = f'{mean}\\\\textsubscript{{\u00b1{std}}}'\n                    else:\n                        formatted_value = value\n\n                    # Bold if one of the best models for this task\n                    if model_name in best_by_task.get(task, []):\n                        formatted_value = f'\\\\textbf{{{formatted_value}}}'\n\n                    cells.append(formatted_value)\n\n        body += ' &amp; '.join(cells) + '\\\\\\\\\\n'\n\n        # Add horizontal lines after certain model groups\n        if model_name in ['Roberta', 'RandomForestMLArgs']:\n            body += '\\\\addlinespace[1ex]\\n\\\\hline\\n\\\\addlinespace[1ex]\\n'\n\n    # Build table footer\n    footer = dedent(\"\"\"\n    \\\\bottomrule\n    \\\\end{tabular}%\n    }\n    \\\\small\n    \\\\label{tab:task-results-combined}\n    \\\\end{table}\n    \"\"\")\n\n    latex_table = header + body + footer\n\n    wide_filtered = wide[wide['Model'].isin(models_in_table)].reset_index(drop=True)\n    csv_df = prepare_dataframe_for_csv(wide_filtered)\n\n    return latex_table, csv_df\n</code></pre>"},{"location":"reference/run/multi_run/csv_to_latex/#run.multi_run.csv_to_latex.generate_latex_table","title":"<code>generate_latex_table(wide, eval_type, discrete_metric, reg_metric='Not regression', include_regression=True)</code>","text":"<p>Produce a complete LaTeX table string for a given eval split ('test' or 'val') using the wide DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>wide</code> <code>DataFrame</code> <p>Wide DataFrame with model results</p> required <code>eval_type</code> <code>str</code> <p>'test' or 'val'</p> required <code>discrete_metric</code> <code>str</code> <p>Name of the discrete metric (accuracy, auroc, etc.)</p> required <code>reg_metric</code> <code>str</code> <p>Label for regression metric</p> <code>'Not regression'</code> <code>include_regression</code> <code>bool</code> <p>If True, include regression tasks. If False, only classification.</p> <code>True</code> Source code in <code>src/run/multi_run/csv_to_latex.py</code> <pre><code>def generate_latex_table(\n    wide: pd.DataFrame,\n    eval_type: str,\n    discrete_metric: str,\n    reg_metric: str = 'Not regression',\n    include_regression: bool = True,\n) -&gt; str:\n    \"\"\"\n    Produce a complete LaTeX table string for a given eval split\n    ('test' or 'val') using the wide DataFrame.\n\n    Args:\n        wide: Wide DataFrame with model results\n        eval_type: 'test' or 'val'\n        discrete_metric: Name of the discrete metric (accuracy, auroc, etc.)\n        reg_metric: Label for regression metric\n        include_regression: If True, include regression tasks. If False, only classification.\n    \"\"\"\n\n    # Get actual tasks that have data from the wide dataframe columns\n    # (excluding the 'Model' column)\n    available_columns = [col for col in wide.columns if col != 'Model']\n\n    # Only include tasks that are actually in the dataframe\n    task_order = []\n    for ds in [ds for grp in GROUPS.values() for ds in grp]:\n        col_name = DATASET_TO_COLUMN.get(ds, ds)\n        if col_name in available_columns:\n            # Check if column has any non-empty data\n            has_data = (wide[col_name] != '').any()\n            if has_data:\n                task_order.append(ds)\n\n    if not task_order:\n        # No tasks to display\n        return ''\n\n    # build column-format, inserting '|' before each group block\n    special_split_point = 'CopCo_TYP'\n    col_fmt = ['l']\n    col_fmt.append('|')  # vertical line between groups\n\n    for grp_name, grp_tasks in GROUPS.items():\n        # Only include tasks from this group that are in task_order\n        tasks_in_group = [t for t in grp_tasks if t in task_order]\n\n        for i, task in enumerate(tasks_in_group):\n            if task == special_split_point:\n                col_fmt.append(':')  # placeholder for dashed line logic\n            col_fmt.append('c')\n\n        if tasks_in_group:  # Only add separator if group has tasks\n            col_fmt.append('|')  # vertical line between groups\n\n    col_fmt = ''.join(col_fmt)\n\n    # Determine metric direction for caption\n    metric_name = METRICS_LABELS.get(discrete_metric, discrete_metric)\n\n    # Determine if this is a regression-only or classification-only table\n    has_classification_tasks = any(ds not in REG_TASKS for ds in task_order)\n    has_regression_tasks = any(ds in REG_TASKS for ds in task_order)\n\n    if has_regression_tasks and not has_classification_tasks:\n        # Regression-only table\n        metric_direction = f'Lower {reg_metric} values indicate better performance'\n        task_desc = f'\\\\textbf{{{reg_metric}}} values are presented for all tasks'\n    elif has_classification_tasks and not has_regression_tasks:\n        # Classification-only table\n        metric_direction = f'Higher {metric_name} values indicate better performance'\n        task_desc = f'\\\\textbf{{{metric_name}}} values are presented for all tasks'\n\n    # header: caption, resizebox, begin tabular\n    hdr = dedent(f\"\"\"\n    \\\\begin{{table}}[ht]\n    \\\\centering\n    \\\\caption{{Model performance across benchmark tasks grouped into Reader and Reader \\\\&amp; Text categories. {task_desc}, averaged across folds. {metric_direction}. Best performing model per task is shown in \\\\textbf{{bold}}.}}\n    \\\\resizebox{{\\\\textwidth}}{{!}}{{%\n    \\\\begin{{tabular}}{{@{{}}{col_fmt}@{{}}}}\n    \\\\toprule\n    \\\\multirow{{2}}{{*}}{{\\\\textbf{{Method}}}}\"\"\")\n\n    # multicolumn headers\n    for grp_name, ds_list in GROUPS.items():\n        # Filter to only tasks in this group that are in task_order\n        tasks_in_group = [t for t in ds_list if t in task_order]\n        if tasks_in_group:\n            hdr += f' &amp; \\\\multicolumn{{{len(tasks_in_group)}}}{{c|}}{{\\\\textbf{{{grp_name}}}}}'\n    hdr += ' \\\\\\\\\\n'\n\n    # cmidrules\n    cmid = []\n    offset = 2\n    for ds_list in GROUPS.values():\n        # Filter to only tasks in this group that are in task_order\n        tasks_in_group = [t for t in ds_list if t in task_order]\n        if tasks_in_group:\n            end = offset + len(tasks_in_group) - 1\n            cmid.append(f'\\\\cmidrule(lr){{{offset}-{end}}}')\n            offset = end + 1\n    hdr += ' '.join(cmid) + '\\n'\n\n    # second header row: task acronyms\n    hdr += ' &amp; ' + ' &amp; '.join(\n        f'\\\\textbf{{{DATASET_TO_COLUMN[ds]}}}' for ds in task_order\n    )\n    hdr += ' \\\\\\\\\\n\\\\midrule\\n'\n    formatted_hline = '\\\\addlinespace[1ex]\\n\\\\hline\\n\\\\addlinespace[1ex]\\n'\n    hdr += formatted_hline\n\n    # Find best model for each task\n    best_by_task = {}\n    for ds in task_order:\n        col_name = DATASET_TO_COLUMN[ds]\n        if col_name not in wide.columns:\n            continue\n\n        # Extract numeric values for comparison\n        numeric_values = wide[col_name].apply(extract_numeric_value)\n\n        # Determine metric for this task\n        if ds in REG_TASKS:\n            # For regression tasks, use reg_metric to determine direction\n            higher_is_better = is_metric_higher_better(reg_metric)\n        else:\n            # For classification tasks, use discrete_metric to determine direction\n            higher_is_better = is_metric_higher_better(discrete_metric)\n\n        # Find all best indices (handles ties)\n        best_indices = find_best_indices(numeric_values, higher_is_better)\n        if best_indices:\n            best_by_task[col_name] = [wide.loc[idx, 'Model'] for idx in best_indices]\n\n    # body rows\n    body = ''\n    for _, row in wide.iterrows():\n        first_cell = MODEL_TO_COLUMN[row['Model']]\n        cells = [first_cell]\n\n        for ds in task_order:\n            col_name = DATASET_TO_COLUMN[ds]\n            value = str(row[col_name])\n\n            # Format with subscript for std deviation\n            value = format_value_with_subscript(value)\n\n            # Bold if this is one of the best models for this task\n            if (\n                row['Model'] in best_by_task.get(col_name, [])\n                and value != ''\n                and value != '-'\n            ):\n                value = f'\\\\textbf{{{value}}}'\n\n            cells.append(value)\n\n        body += ' &amp; '.join(cells)\n        body += ' \\\\\\\\\\n'\n        if row['Model'] in ['Roberta', 'RandomForestMLArgs']:\n            body += formatted_hline\n\n    # table tail\n    tail = dedent(rf\"\"\"\n    \\bottomrule\n    \\end{{tabular}}%\n    }}\n    \\small\n    \\label{{tab:task-results-{eval_type}-{discrete_metric}}}\n    \\end{{table}}\n    \"\"\")\n\n    return hdr + body + tail\n</code></pre>"},{"location":"reference/run/multi_run/csv_to_latex/#run.multi_run.csv_to_latex.generate_latex_table_per_task","title":"<code>generate_latex_table_per_task(wide, task)</code>","text":"<p>Produce a complete LaTeX table string for a specific task with metrics as columns, showing both validation and test results side-by-side.</p> <p>Parameters:</p> Name Type Description Default <code>wide</code> <code>DataFrame</code> <p>Wide DataFrame with model results (rows=models, cols=metrics with _Val and _Test suffixes)</p> required <code>task</code> <code>str</code> <p>The data task key (e.g., 'CopCo_RCS')</p> required Source code in <code>src/run/multi_run/csv_to_latex.py</code> <pre><code>def generate_latex_table_per_task(\n    wide: pd.DataFrame,\n    task: str,\n) -&gt; str:\n    \"\"\"\n    Produce a complete LaTeX table string for a specific task with metrics as columns,\n    showing both validation and test results side-by-side.\n\n    Args:\n        wide: Wide DataFrame with model results (rows=models, cols=metrics with _Val and _Test suffixes)\n        task: The data task key (e.g., 'CopCo_RCS')\n    \"\"\"\n\n    if wide.empty:\n        return ''\n\n    # Get metric columns (all columns except 'Model')\n    all_cols = [col for col in wide.columns if col != 'Model']\n\n    if not all_cols:\n        return ''\n\n    # Group columns by metric (removing _Val and _Test suffixes)\n    metric_names = []\n    for col in all_cols:\n        if col.endswith('_Val'):\n            metric_name = col[:-4]\n            if metric_name not in metric_names:\n                metric_names.append(metric_name)\n        elif col.endswith('_Test'):\n            metric_name = col[:-5]\n            if metric_name not in metric_names:\n                metric_names.append(metric_name)\n\n    # Build column format: one column for model name, then 2 columns per metric (val, test)\n    num_metric_cols = sum(\n        1 for m in metric_names if f'{m}_Val' in all_cols or f'{m}_Test' in all_cols\n    )\n    col_fmt = 'l|' + 'cc|' * num_metric_cols\n\n    # Get task display name\n    task_label = DATASET_TO_COLUMN.get(task, task)\n\n    # Header: caption, resizebox, begin tabular\n    hdr = dedent(f\"\"\"\n    \\\\begin{{table}}[ht]\n    \\\\centering\n    \\\\caption{{Model performance on \\\\textbf{{{task_label}}} task across different metrics for validation and test sets, averaged across folds. Best performing model per metric and split is shown in \\\\textbf{{bold}}.}}\n    \\\\resizebox{{\\\\textwidth}}{{!}}{{%\n    \\\\begin{{tabular}}{{@{{}}{col_fmt}@{{}}}}\n    \\\\toprule\n    \\\\multirow{{2}}{{*}}{{\\\\textbf{{Method}}}}\"\"\")\n\n    # Add metric multicolumn headers\n    for metric_name in metric_names:\n        has_val = f'{metric_name}_Val' in all_cols\n        has_test = f'{metric_name}_Test' in all_cols\n        if has_val or has_test:\n            hdr += f' &amp; \\\\multicolumn{{2}}{{c|}}{{\\\\textbf{{{metric_name}}}}}'\n    hdr += ' \\\\\\\\\\n'\n\n    # Add cmidrules for each metric group\n    cmid = []\n    offset = 2\n    for metric_name in metric_names:\n        has_val = f'{metric_name}_Val' in all_cols\n        has_test = f'{metric_name}_Test' in all_cols\n        if has_val or has_test:\n            end = offset + 1\n            cmid.append(f'\\\\cmidrule(lr){{{offset}-{end}}}')\n            offset = end + 1\n    hdr += ' '.join(cmid) + '\\n'\n\n    # Second header row: Val/Test labels\n    hdr += ' '\n    for metric_name in metric_names:\n        has_val = f'{metric_name}_Val' in all_cols\n        has_test = f'{metric_name}_Test' in all_cols\n        if has_val or has_test:\n            hdr += ' &amp; \\\\textbf{Val} &amp; \\\\textbf{Test}'\n    hdr += ' \\\\\\\\\\n\\\\midrule\\n'\n\n    # Find best model for each metric column\n    best_by_col = {}\n    for col in all_cols:\n        # Extract numeric values for comparison\n        numeric_values = wide[col].apply(extract_numeric_value)\n\n        # Extract metric name from column (format: {Metric}_Val or {Metric}_Test)\n        if col.endswith('_Val') or col.endswith('_Test'):\n            metric_name = col.rsplit('_', 1)[0]\n        else:\n            metric_name = col\n\n        # Determine if higher is better for this metric\n        higher_is_better = is_metric_higher_better(metric_name)\n\n        # Find all best indices (handles ties)\n        best_indices = find_best_indices(numeric_values, higher_is_better)\n        if best_indices:\n            best_by_col[col] = [wide.loc[idx, 'Model'] for idx in best_indices]\n\n    # Body rows\n    body = ''\n    for _, row in wide.iterrows():\n        first_cell = MODEL_TO_COLUMN[row['Model']]\n        cells = [first_cell]\n\n        for metric_name in metric_names:\n            val_col = f'{metric_name}_Val'\n            test_col = f'{metric_name}_Test'\n\n            # Add validation value\n            if val_col in all_cols:\n                val_value = str(row[val_col])\n                # Format with subscript for std deviation\n                val_value = format_value_with_subscript(val_value)\n                if (\n                    row['Model'] in best_by_col.get(val_col, [])\n                    and val_value != ''\n                    and val_value != '-'\n                ):\n                    val_value = f'\\\\textbf{{{val_value}}}'\n                cells.append(val_value)\n            else:\n                cells.append('')\n\n            # Add test value\n            if test_col in all_cols:\n                test_value = str(row[test_col])\n                # Format with subscript for std deviation\n                test_value = format_value_with_subscript(test_value)\n                if (\n                    row['Model'] in best_by_col.get(test_col, [])\n                    and test_value != ''\n                    and test_value != '-'\n                ):\n                    test_value = f'\\\\textbf{{{test_value}}}'\n                cells.append(test_value)\n            else:\n                cells.append('')\n\n        body += ' &amp; '.join(cells)\n        body += ' \\\\\\\\\\n'\n        if row['Model'] in ['Roberta', 'RandomForestMLArgs']:\n            body += '\\\\midrule\\n'\n\n    # Table tail\n    tail = dedent(rf\"\"\"\n    \\bottomrule\n    \\end{{tabular}}%\n    }}\n    \\label{{tab:task-{task.lower()}}}\n    \\end{{table}}\n    \"\"\")\n\n    return hdr + body + tail\n</code></pre>"},{"location":"reference/run/multi_run/csv_to_latex/#run.multi_run.csv_to_latex.generate_latex_table_per_task_by_regime","title":"<code>generate_latex_table_per_task_by_regime(wide, task, eval_type)</code>","text":"<p>Produce a complete LaTeX table string for a specific task, broken down by regime, with all metrics shown for each regime.</p> <p>Parameters:</p> Name Type Description Default <code>wide</code> <code>DataFrame</code> <p>Wide DataFrame with model results (rows=models, cols=(regime, metric) combinations)</p> required <code>task</code> <code>str</code> <p>The data task key (e.g., 'CopCo_RCS')</p> required <code>eval_type</code> <code>str</code> <p>'val' or 'test'</p> required Source code in <code>src/run/multi_run/csv_to_latex.py</code> <pre><code>def generate_latex_table_per_task_by_regime(\n    wide: pd.DataFrame,\n    task: str,\n    eval_type: str,\n) -&gt; str:\n    \"\"\"\n    Produce a complete LaTeX table string for a specific task, broken down by regime,\n    with all metrics shown for each regime.\n\n    Args:\n        wide: Wide DataFrame with model results (rows=models, cols=(regime, metric) combinations)\n        task: The data task key (e.g., 'CopCo_RCS')\n        eval_type: 'val' or 'test'\n    \"\"\"\n\n    if wide.empty:\n        return ''\n\n    # Get all columns except 'Model'\n    all_cols = [col for col in wide.columns if col != 'Model']\n\n    if not all_cols:\n        return ''\n\n    # Organize columns by regime and metric\n    # Column format is: {Regime}_{Metric}\n    regimes = ['Unseen Reader', 'Unseen Text', 'Unseen Text \\\\&amp; Reader', 'Average']\n\n    # Extract unique metrics from columns\n    metrics = []\n    for col in all_cols:\n        for regime in regimes:\n            if col.startswith(f'{regime}_'):\n                metric = col[len(regime) + 1 :]\n                if metric not in metrics:\n                    metrics.append(metric)\n\n    # Build column format: one column for model name, then columns for each regime group\n    # Each regime has one column per metric\n    col_fmt_parts = ['l|']\n    for regime in regimes:\n        regime_metrics = [m for m in metrics if f'{regime}_{m}' in all_cols]\n        if regime_metrics:\n            col_fmt_parts.append('c' * len(regime_metrics))\n            col_fmt_parts.append('|')\n    col_fmt = ''.join(col_fmt_parts)\n\n    # Get task display name\n    task_label = DATASET_TO_COLUMN.get(task, task)\n\n    # Determine eval type label\n    eval_label = 'validation' if eval_type == 'val' else 'test'\n\n    # Header: caption, resizebox, begin tabular\n    hdr = dedent(f\"\"\"\n    \\\\begin{{table}}[ht]\n    \\\\centering\n    \\\\caption{{Model performance on the {task_label} for the \\\\textbf{{{eval_label}}} set.}}\n    \\\\resizebox{{\\\\textwidth}}{{!}}{{%\n    \\\\begin{{tabular}}{{@{{}}{col_fmt}@{{}}}}\n    \\\\toprule\n    \\\\multirow{{2}}{{*}}{{\\\\textbf{{Method}}}}\"\"\")\n\n    # Add regime multicolumn headers\n    for regime in regimes:\n        regime_metrics = [m for m in metrics if f'{regime}_{m}' in all_cols]\n        if regime_metrics:\n            hdr += f' &amp; \\\\multicolumn{{{len(regime_metrics)}}}{{c|}}{{\\\\textbf{{{regime}}}}}'\n    hdr += ' \\\\\\\\\\n'\n\n    # Add cmidrules for each regime group\n    cmid = []\n    offset = 2\n    for regime in regimes:\n        regime_metrics = [m for m in metrics if f'{regime}_{m}' in all_cols]\n        if regime_metrics:\n            end = offset + len(regime_metrics) - 1\n            cmid.append(f'\\\\cmidrule(lr){{{offset}-{end}}}')\n            offset = end + 1\n    hdr += ' '.join(cmid) + '\\n'\n\n    # Second header row: metric labels\n    hdr += ' '\n    for regime in regimes:\n        regime_metrics = [m for m in metrics if f'{regime}_{m}' in all_cols]\n        for metric in regime_metrics:\n            hdr += f' &amp; \\\\textbf{{{metric}}}'\n    hdr += ' \\\\\\\\\\n\\\\midrule\\n'\n\n    # Find best model for each (regime, metric) column\n    best_by_col = {}\n    for col in all_cols:\n        # Extract numeric values for comparison\n        numeric_values = wide[col].apply(extract_numeric_value)\n\n        # Extract metric name from column (format: {Regime}_{Metric})\n        for regime in regimes:\n            if col.startswith(f'{regime}_'):\n                metric_name = col[len(regime) + 1 :]\n                break\n        else:\n            metric_name = col\n\n        # Determine if higher is better for this metric\n        higher_is_better = is_metric_higher_better(metric_name)\n\n        # Find all best indices (handles ties)\n        best_indices = find_best_indices(numeric_values, higher_is_better)\n        if best_indices:\n            best_by_col[col] = [wide.loc[idx, 'Model'] for idx in best_indices]\n\n    # Body rows\n    body = ''\n    for _, row in wide.iterrows():\n        first_cell = MODEL_TO_COLUMN[row['Model']]\n        cells = [first_cell]\n\n        for regime in regimes:\n            regime_metrics = [m for m in metrics if f'{regime}_{m}' in all_cols]\n            for metric in regime_metrics:\n                col = f'{regime}_{metric}'\n                value = str(row[col])\n\n                # Format with subscript for std deviation\n                value = format_value_with_subscript(value)\n\n                # Bold if this is one of the best models for this regime+metric\n                if (\n                    row['Model'] in best_by_col.get(col, [])\n                    and value != ''\n                    and value != '-'\n                ):\n                    value = f'\\\\textbf{{{value}}}'\n\n                cells.append(value)\n\n        body += ' &amp; '.join(cells)\n        body += ' \\\\\\\\\\n'\n        if row['Model'] in ['Roberta', 'RandomForestMLArgs']:\n            body += '\\\\midrule\\n'\n\n    # Table tail\n    tail = dedent(rf\"\"\"\n    \\bottomrule\n    \\end{{tabular}}%\n    }}\n    \\label{{tab:task-{task.lower()}-{eval_type}-regime}}\n    \\end{{table}}\n    \"\"\")\n\n    return hdr + body + tail\n</code></pre>"},{"location":"reference/run/multi_run/csv_to_latex/#run.multi_run.csv_to_latex.is_metric_higher_better","title":"<code>is_metric_higher_better(metric_name)</code>","text":"<p>Determine if higher values are better for a given metric.</p> <p>Parameters:</p> Name Type Description Default <code>metric_name</code> <code>str</code> <p>The metric name (e.g., 'auroc', 'rmse', 'r2', 'R\u00b2')</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if higher is better, False if lower is better</p> Source code in <code>src/run/multi_run/csv_to_latex.py</code> <pre><code>def is_metric_higher_better(metric_name: str) -&gt; bool:\n    \"\"\"\n    Determine if higher values are better for a given metric.\n\n    Args:\n        metric_name: The metric name (e.g., 'auroc', 'rmse', 'r2', 'R\u00b2')\n\n    Returns:\n        True if higher is better, False if lower is better\n    \"\"\"\n    # Normalize metric name to lowercase for comparison\n    metric_lower = metric_name.lower().strip()\n\n    # Metrics where lower is better\n    lower_is_better = ['rmse', 'mae']\n\n    # Metrics where higher is better\n    higher_is_better = ['auroc', 'accuracy', 'balanced_accuracy', 'f1', 'r2', 'r\u00b2']\n\n    if metric_lower in lower_is_better:\n        return False\n    elif metric_lower in higher_is_better:\n        return True\n\n    # Default: if it's a regression metric not in the lists, assume lower is better\n    # Otherwise assume higher is better\n    if metric_lower in [m.lower() for m in RegrSupportedMetrics]:\n        return False\n    return True\n</code></pre>"},{"location":"reference/run/multi_run/csv_to_latex/#run.multi_run.csv_to_latex.keep_only_all_eval","title":"<code>keep_only_all_eval(df)</code>","text":"<p>Load the AUROC CSV and drop unused columns.</p> Source code in <code>src/run/multi_run/csv_to_latex.py</code> <pre><code>def keep_only_all_eval(df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    Load the AUROC CSV and drop unused columns.\n    \"\"\"\n    df = df.drop(\n        columns=[\n            'Seen subject unseen item',\n            'Unseen subject seen item',\n            'Unseen subject unseen item',\n        ],\n        errors='ignore',\n    )\n    return df\n</code></pre>"},{"location":"reference/run/multi_run/csv_to_latex/#run.multi_run.csv_to_latex.prepare_dataframe_for_csv","title":"<code>prepare_dataframe_for_csv(df, model_col='Model')</code>","text":"<p>Return a copy of <code>df</code> with human-friendly model names for CSV export.</p> Source code in <code>src/run/multi_run/csv_to_latex.py</code> <pre><code>def prepare_dataframe_for_csv(\n    df: pd.DataFrame, model_col: str = 'Model'\n) -&gt; pd.DataFrame:\n    \"\"\"Return a copy of ``df`` with human-friendly model names for CSV export.\"\"\"\n\n    csv_df = df.copy()\n    if model_col in csv_df.columns:\n        mapped = csv_df[model_col].map(MODEL_TO_COLUMN)\n        csv_df[model_col] = mapped.fillna(csv_df[model_col])\n    return csv_df\n</code></pre>"},{"location":"reference/run/multi_run/csv_to_latex/#run.multi_run.csv_to_latex.save_to_both_locations","title":"<code>save_to_both_locations(content, relative_path, is_csv=False)</code>","text":"<p>Save content to both the local results directory and the external output directory.</p> <p>Parameters:</p> Name Type Description Default <code>content</code> <code>str | DataFrame</code> <p>The content to write (str for text files, DataFrame for CSV files)</p> required <code>relative_path</code> <code>str</code> <p>The relative path within the results directory</p> required <code>is_csv</code> <code>bool</code> <p>Whether this is a CSV file (handled differently)</p> <code>False</code> Source code in <code>src/run/multi_run/csv_to_latex.py</code> <pre><code>def save_to_both_locations(\n    content: str | pd.DataFrame, relative_path: str, is_csv: bool = False\n):\n    \"\"\"\n    Save content to both the local results directory and the external output directory.\n\n    Args:\n        content: The content to write (str for text files, DataFrame for CSV files)\n        relative_path: The relative path within the results directory\n        is_csv: Whether this is a CSV file (handled differently)\n    \"\"\"\n    for base_dir in [LOCAL_OUTPUT_DIR, OVERLEAF_OUTPUT_DIR]:\n        # If Overleaf dir does not exist, log and skip saving there\n        if base_dir == OVERLEAF_OUTPUT_DIR and not base_dir.exists():\n            logger.warning(\n                f'Overleaf output dir does not exist ({OVERLEAF_OUTPUT_DIR}), skipping save to Overleaf'\n            )\n            continue\n\n        path = base_dir / relative_path\n        path.parent.mkdir(parents=True, exist_ok=True)\n        try:\n            if is_csv:\n                # content expected to be a DataFrame\n                content.to_csv(path, index=False)\n            else:\n                # content expected to be a string\n                path.write_text(content)\n            logger.info(f'Saved to: {path}')\n        except Exception as e:\n            logger.exception(f'Failed to save {relative_path} to {base_dir}: {e}')\n</code></pre>"},{"location":"reference/run/multi_run/raw_to_processed_results/","title":"raw_to_processed_results","text":""},{"location":"reference/run/multi_run/raw_to_processed_results/#run.multi_run.raw_to_processed_results.aggregate_df","title":"<code>aggregate_df(res_df, metric_name, metric_type, columns=ALL_REGIMES, error_type='std')</code>","text":"<p>Aggregates metrics data by computing mean and error statistics.</p> <p>Parameters:</p> Name Type Description Default <code>res_df</code> <code>DataFrame</code> <p>DataFrame containing lists of metric values for each regime</p> required <code>metric_name</code> <code>str</code> <p>Name of the metric (e.g., 'accuracy', 'balanced_accuracy')</p> required <code>columns</code> <code>list[str]</code> <p>List of evaluation regimes to process</p> <code>ALL_REGIMES</code> <code>error_type</code> <code>str</code> <p>Type of error to calculate ('std' or 'sem')</p> <code>'std'</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with aggregated metrics and formatted results</p> Source code in <code>src/run/multi_run/raw_to_processed_results.py</code> <pre><code>def aggregate_df(\n    res_df: pd.DataFrame,\n    metric_name: str,\n    metric_type: str,\n    columns: list[str] = ALL_REGIMES,\n    error_type: str = 'std',\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Aggregates metrics data by computing mean and error statistics.\n\n    Args:\n        res_df: DataFrame containing lists of metric values for each regime\n        metric_name: Name of the metric (e.g., 'accuracy', 'balanced_accuracy')\n        columns: List of evaluation regimes to process\n        error_type: Type of error to calculate ('std' or 'sem')\n\n    Returns:\n        DataFrame with aggregated metrics and formatted results\n    \"\"\"\n    res_df = res_df.copy()\n\n    for col in columns:\n        # Calculate mean values and convert to percentage rounded to 1 decimal place\n        avg_col = f'Avg {metric_name} {col}'\n        res_df[avg_col] = res_df[col].apply(np.nanmean)\n\n        # if metric_type is not 'Regression':\n        if metric_type == 'Regression':\n            res_df[avg_col] = res_df[avg_col].round(2)\n        else:\n            res_df[avg_col] = (100 * res_df[avg_col]).round(1)\n\n        def handle_missing(x):\n            if isinstance(x, list):\n                if len(x) &gt; 0:\n                    return np.std(x) / np.sqrt(len(x))\n            return 0\n\n        # Calculate error statistics based on specified type\n        if error_type == 'std':\n            err_col = f'Std {metric_name} {col}'\n            res_df[err_col] = res_df[col].apply(np.nanstd)\n        elif error_type == 'sem':\n            err_col = f'SEM {metric_name} {col}'\n            res_df[err_col] = res_df[col].apply(lambda x: handle_missing(x))\n        else:\n            raise ValueError(f'Invalid error type: {error_type}')\n\n        # Convert error to percentage and round\n        if metric_type == 'Regression':\n            res_df[err_col] = res_df[err_col].round(1)\n        else:\n            res_df[err_col] = (100 * res_df[err_col]).round(1)\n\n        # Format the final result string with mean \u00b1 error\n        formatted_col = f'{col}'.replace('_', ' ').capitalize()\n        res_df[formatted_col] = res_df.apply(\n            lambda x: f'{x[avg_col]} \u00b1 {x[err_col]}',\n            axis=1,\n        )\n\n        # Remove intermediate columns\n        res_df = res_df.drop([col, avg_col, err_col], axis=1)\n\n    return res_df\n</code></pre>"},{"location":"reference/run/multi_run/raw_to_processed_results/#run.multi_run.raw_to_processed_results.get_metric_from_raw_res","title":"<code>get_metric_from_raw_res(res, metric_type, metric_name, data_task, model, eval_type)</code>","text":"<p>Converts raw results dataframe into accuracy metrics by evaluation regime.</p> <p>Parameters:</p> Name Type Description Default <code>res</code> <code>DataFrame</code> <p>DataFrame containing predictions and labels</p> required <code>metric_name</code> <code>str</code> <p>Metric to compute ('accuracy' or 'balanced_accuracy')</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with metrics for each evaluation regime and overall</p> Source code in <code>src/run/multi_run/raw_to_processed_results.py</code> <pre><code>def get_metric_from_raw_res(\n    res: pd.DataFrame,\n    metric_type: object,\n    metric_name: str,\n    data_task: str,\n    model: str,\n    eval_type: str,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Converts raw results dataframe into accuracy metrics by evaluation regime.\n\n    Args:\n        res: DataFrame containing predictions and labels\n        metric_name: Metric to compute ('accuracy' or 'balanced_accuracy')\n\n    Returns:\n        DataFrame with metrics for each evaluation regime and overall\n    \"\"\"\n    res_df = defaultdict(list)\n    stats = []\n\n    # Calculate metrics for each evaluation regime and fold\n    for (eval_regime, fold), group in res.groupby(['eval_regime', 'fold_index']):\n        y_true = group['label']\n        prediction_prob = group['prediction_prob']\n        n_folds = group['fold_index'].nunique()\n        fold_list = group['fold_index'].unique().tolist()\n\n        new_stats = validate_results(\n            eval_type,\n            data_task,\n            model,\n            eval_regime,\n            metric_type,\n            metric_name,\n            y_true,\n            prediction_prob,\n            fold,\n            n_folds,\n            fold_list,\n        )\n\n        score = get_scores(\n            y_true=y_true,\n            prediction_prob=prediction_prob,\n            metric_name=metric_name,\n        )\n        res_df[eval_regime].append(score)\n        new_stats['score'] = score\n        stats.append(new_stats)\n\n    # Calculate overall metrics for each fold\n    for fold, group in res.groupby('fold_index'):\n        y_true = group['label']\n        prediction_prob = group['prediction_prob']\n        eval_regime = 'all'\n        n_folds = group['fold_index'].nunique()\n        fold_list = group['fold_index'].unique().tolist()\n\n        new_stats = validate_results(\n            eval_type,\n            data_task,\n            model,\n            eval_regime,\n            metric_type,\n            metric_name,\n            y_true,\n            prediction_prob,\n            fold,\n            n_folds,\n            fold_list,\n        )\n\n        score = get_scores(\n            y_true=y_true,\n            prediction_prob=prediction_prob,\n            metric_name=metric_name,\n        )\n        res_df[eval_regime].append(score)\n        new_stats['score'] = score\n        stats.append(new_stats)\n\n    # Calculate overall metrics for each evaluation regime\n    for eval_regime, group in res.groupby('eval_regime'):\n        y_true = group['label']\n        prediction_prob = group['prediction_prob']\n        fold = 'all'\n        n_folds = group['fold_index'].nunique()\n        fold_list = group['fold_index'].unique().tolist()\n\n        new_stats = validate_results(\n            eval_type,\n            data_task,\n            model,\n            eval_regime,\n            metric_type,\n            metric_name,\n            y_true,\n            prediction_prob,\n            fold,\n            n_folds,\n            fold_list,\n        )\n\n        score = get_scores(\n            y_true=y_true,\n            prediction_prob=prediction_prob,\n            metric_name=metric_name,\n        )\n        new_stats['score'] = score\n        stats.append(new_stats)\n\n    # Create DataFrame and ensure all regimes are included\n    df = pd.DataFrame(\n        res_df.items(),\n        columns=['Eval Regime', metric_name],\n    ).set_index('Eval Regime')\n    # Concatenate all stats DataFrames\n    stats_df = pd.DataFrame(stats)\n    # Reindex to ensure all regimes are present, even if empty\n    return df.reindex(ALL_REGIMES), stats_df\n</code></pre>"},{"location":"reference/run/multi_run/raw_to_processed_results/#run.multi_run.raw_to_processed_results.validate_results","title":"<code>validate_results(eval_type, data_task, model, eval_regime, metric_type, metric_name, y_true, prediction_prob, fold, n_folds, fold_list)</code>","text":"<p>Validate the results before computing statistics.</p> Source code in <code>src/run/multi_run/raw_to_processed_results.py</code> <pre><code>def validate_results(\n    eval_type: str,\n    data_task: str,\n    model: str,\n    eval_regime,\n    metric_type: str,\n    metric_name,\n    y_true: pd.Series,\n    prediction_prob: pd.Series,\n    fold,\n    n_folds: int,\n    fold_list,\n) -&gt; dict:\n    \"\"\"Validate the results before computing statistics.\"\"\"\n\n    # Process prediction probabilities to get predicted classes/values\n    y_pred = _process_prediction_prob(metric_name, prediction_prob)\n\n    unique_y_true = np.unique(y_true)\n    unique_y_pred = np.unique(y_pred)\n\n    # If classification, ensure predicted classes are a subset of true classes\n    if metric_type != 'Regression':\n        extra_classes = set(unique_y_pred) - set(unique_y_true)\n        # convert to float and round for readability\n        extra_classes = [round(float(x), 2) for x in extra_classes]\n        if len(extra_classes) &gt; 5:\n            extra_classes = (\n                list(extra_classes)[:5]\n                + ['...']\n                + [f'N extra classes: {len(extra_classes)}']\n            )\n\n    # get distribution if n_unique &lt; 10 else ['Too many classes: {n_unique}']\n    y_true_distribution = (\n        y_true.value_counts().to_dict()\n        if len(unique_y_true) &lt; 10\n        else {f'N unique: {len(unique_y_true)}'}\n    )\n    y_pred_distribution = (\n        y_pred.value_counts().to_dict()\n        if len(unique_y_pred) &lt; 10\n        else {f'N unique: {len(unique_y_pred)}'}\n    )\n\n    stats = {\n        'metric_type': metric_type,\n        'metric_name': metric_name.value,\n        'eval_type': eval_type,\n        'data_task': data_task,\n        'model': model,\n        'eval_regime': eval_regime,\n        'fold': fold,\n        'n_folds': n_folds,\n        'fold_list': fold_list,\n        'n_samples': len(y_true),\n        'n_unique_y_true': len(np.unique(y_true)),\n        'n_unique_y_pred': len(np.unique(y_pred)),\n        'y_true_single': len(np.unique(y_true)) == 1,\n        'y_pred_single': len(np.unique(y_pred)) == 1,\n        'y_true_distribution': y_true_distribution,\n        'y_pred_distribution': y_pred_distribution,\n        'extra_classes_in_y_pred': list(extra_classes)\n        if metric_type != 'Regression' and extra_classes\n        else None,\n    }\n\n    return stats\n</code></pre>"},{"location":"reference/run/multi_run/search_spaces/","title":"search_spaces","text":""},{"location":"reference/run/multi_run/sweep_creator/","title":"sweep_creator","text":"<p>This script creates and launches wandb sweeps for different models, data tasks, and trainers. It generates bash &amp; slurm scripts for running the sweeps on multiple GPUs or in a slurm environment. It uses the wandb library to create and manage the sweeps.</p>"},{"location":"reference/run/multi_run/sweep_creator/#run.multi_run.sweep_creator.HyperArgs","title":"<code>HyperArgs</code>","text":"<p>               Bases: <code>Tap</code></p> <pre><code>Usage:\n1. check that 'search_space_by_model' has the correct hyperparameter search space\n</code></pre> <p>for the model you wish to sweep.     2. run 'python src/run/multi_run/sweep_creator.py --models  --data_task_names  --trainer_names '        * To run multiple models/data_tasks/trainers, separate them with spaces     3. the script will create executable bash scripts for each sweep (fold_idx), which will launch the wandb sweeps.     4. run the bash script ./.sh     5. If you want to run on multiple GPUs, use the --gpu_count flag. Not tested for &gt;1 GPUs. Source code in <code>src/run/multi_run/sweep_creator.py</code> <pre><code>class HyperArgs(Tap):\n    \"\"\"\n        Usage:\n        1. check that 'search_space_by_model' has the correct hyperparameter search space\n    for the model you wish to sweep.\n        2. run 'python src/run/multi_run/sweep_creator.py --models &lt;models&gt;\n    --data_task_names &lt;data_task_names&gt; --trainer_names &lt;trainer_names&gt;'\n           * To run multiple models/data_tasks/trainers, separate them with spaces\n        3. the script will create executable bash scripts for each sweep (fold_idx),\n    which will launch the wandb sweeps.\n        4. run the bash script ./&lt;bash_script&gt;.sh\n        5. If you want to run on multiple GPUs, use the --gpu_count flag. Not tested for &gt;1 GPUs.\n    \"\"\"\n\n    run_cap: int = (\n        250  # Maximum number of runs to execute. Relevant for non-grid search.\n    )\n    wandb_project: str = 'debug'  # Name of the wandb project to log to.\n    wandb_entity: str = 'EyeRead'  # Name of the wandb entity to log to.\n    folds: list[int] = [0]  # List of fold indices to run.\n    gpu_count: int = 1  # Number of GPUs to use. &gt;1  not tested.\n    search_algorithm: Literal['bayes', 'grid', 'random'] = (\n        'grid'  # Search algorithm to use.\n    )\n\n    # Slurm settings.\n    slurm_cpus: int = 10  # Number of CPUs to use. Ideally number of workers + 2.\n    slurm_mem: str = '75G'  # Amount of memory to use.\n    slurm_mailto: str = 'shubi@campus.technion.ac.il'  # Email to send notifications to.\n    num_duplicates_per_gpu: int = 1  # Number of duplicates to run on each GPU.\n\n    # Model, data, and trainer settings as lists to support multiple values\n    models: list[str] = []  # List of models to sweep\n    base_models: list[str] = []  # List of base models to sweep\n    data_tasks: list[str] = []  # List of data tasks to sweep\n    trainers: list[str] = [\n        'TrainerDL'\n    ]  # List of trainers to sweep (default is 'default')\n\n    # Filled in by the script\n    trainer: str | None = None\n    data_task: str | None = None\n    model: str | None = None\n    base_model: str | None = None\n</code></pre>"},{"location":"reference/run/multi_run/sweep_creator/#run.multi_run.sweep_creator.create_bash_scripts","title":"<code>create_bash_scripts(hyper_args, sweep_ids, mode)</code>","text":"<p>Create bash scripts for the given sweep ids. Args:     hyper_args (HyperArgs): Hyperparameters for the sweep.     sweep_ids (list[str]): List of sweep ids.</p> Source code in <code>src/run/multi_run/sweep_creator.py</code> <pre><code>def create_bash_scripts(\n    hyper_args: HyperArgs, sweep_ids: list[str], mode: Literal['lacc', 'david']\n) -&gt; None:\n    \"\"\"\n    Create bash scripts for the given sweep ids.\n    Args:\n        hyper_args (HyperArgs): Hyperparameters for the sweep.\n        sweep_ids (list[str]): List of sweep ids.\n    \"\"\"\n    assert hyper_args.model is not None\n    base_path = (\n        Path('sweeps') / hyper_args.wandb_project / 'bash' / mode / hyper_args.model\n    )\n    base_path.mkdir(parents=True, exist_ok=True)\n    filename = base_path / (\n        f'{hyper_args.model}_{hyper_args.data_task}_folds_'\n        + '_'.join(map(str, hyper_args.folds))\n        + '.sh'\n    )\n    main_command = '; '.join(\n        ['conda activate eyebench']\n        + [\n            f'CUDA_VISIBLE_DEVICES=${{GPU_NUM}} wandb agent '\n            f'{hyper_args.wandb_entity}/{hyper_args.wandb_project}/{sweep_id}'\n            for sweep_id in sweep_ids\n        ]\n    )\n    write_bash_script(\n        filename=filename, main_command=main_command, sweep_ids=sweep_ids, mode=mode\n    )\n</code></pre>"},{"location":"reference/run/multi_run/sweep_creator/#run.multi_run.sweep_creator.create_slurm_scripts","title":"<code>create_slurm_scripts(hyper_args, sweep_ids, slurm_qos)</code>","text":"<p>Create slurm scripts for the given sweep ids. Args:     hyper_args (HyperArgs): Hyperparameters for the sweep.     sweep_ids (list[str]): List of sweep ids.     slurm_qos (str): 'normal' or 'basic' for DGX.</p> Source code in <code>src/run/multi_run/sweep_creator.py</code> <pre><code>def create_slurm_scripts(\n    hyper_args: HyperArgs, sweep_ids: list[str], slurm_qos: Literal['normal', 'basic']\n) -&gt; None:\n    \"\"\"\n    Create slurm scripts for the given sweep ids.\n    Args:\n        hyper_args (HyperArgs): Hyperparameters for the sweep.\n        sweep_ids (list[str]): List of sweep ids.\n        slurm_qos (str): 'normal' or 'basic' for DGX.\n    \"\"\"\n    assert hyper_args.model is not None\n    base_path = Path('sweeps') / hyper_args.wandb_project / 'slurm' / hyper_args.model\n    base_path.mkdir(parents=True, exist_ok=True)\n    filename = base_path / (\n        f'{hyper_args.model}_{hyper_args.data_task}_folds_'\n        + '_'.join(map(str, hyper_args.folds))\n        + f'{slurm_qos}.job'\n    )\n    write_slurm_script(\n        filename=filename,\n        hyper_args=hyper_args,\n        sweep_ids=sweep_ids,\n        slurm_qos=slurm_qos,\n    )\n</code></pre>"},{"location":"reference/run/multi_run/sweep_creator/#run.multi_run.sweep_creator.create_sweep_configs","title":"<code>create_sweep_configs(args)</code>","text":"<p>Create sweep configurations for the given hyperparameters.</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <code>HyperArgs</code> <p>Hyperparameters for the sweep.</p> required <p>Returns:</p> Type Description <code>list[dict]</code> <p>list[dict]: List of sweep configurations.</p> Source code in <code>src/run/multi_run/sweep_creator.py</code> <pre><code>def create_sweep_configs(args: HyperArgs) -&gt; list[dict]:\n    \"\"\"\n    Create sweep configurations for the given hyperparameters.\n\n    Args:\n        args (HyperArgs): Hyperparameters for the sweep.\n\n    Returns:\n        list[dict]: List of sweep configurations.\n    \"\"\"\n    search_space = search_space_by_model[args.base_model]\n    logger.info(f'Creating sweep configs for {args.base_model}')\n    _, total_count = count_hyperparameter_configs(search_space)\n    logger.info(args)\n    if total_count &gt; args.run_cap:\n        logger.warning(\n            f'Warning: The number of hyperparameter configurations ({total_count}) is less than the run cap ({args.run_cap}).'\n        )\n\n    sweep_configs = [\n        {\n            'program': 'src/run/single_run/train.py',\n            'method': args.search_algorithm,\n            'metric': {\n                'goal': 'minimize',\n                'name': 'loss/val_all',\n            },\n            'entity': args.wandb_entity,\n            'project': args.wandb_project,\n            'name': f'{args.model}_{args.data_task}_fold_{fold_idx}',\n            'parameters': search_space,\n            'run_cap': args.run_cap,\n            'command': [\n                '${env}',\n                '${interpreter}',\n                '${program}',\n                '${args_no_hypens}',\n                f'+model={args.model}',\n                f'+data={args.data_task}',\n                f'+trainer={args.trainer}',\n                f'data.fold_index={fold_idx}',\n                f'trainer.devices={args.gpu_count}',\n                f'trainer.wandb_job_type={args.model}_{args.data_task}',\n            ],\n        }\n        for fold_idx in args.folds\n    ]\n\n    return sweep_configs\n</code></pre>"},{"location":"reference/run/multi_run/sweep_creator/#run.multi_run.sweep_creator.launch_sweeps","title":"<code>launch_sweeps(entity, project, sweep_configs)</code>","text":"<p>Launch wandb sweeps for the given configurations.</p> <p>Parameters:</p> Name Type Description Default <code>entity</code> <code>str</code> <p>Name of the wandb entity.</p> required <code>project</code> <code>str</code> <p>Name of the wandb project.</p> required <code>sweep_configs</code> <code>List[Dict]</code> <p>List of sweep configurations.</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>List[str]: List of sweep ids.</p> Source code in <code>src/run/multi_run/sweep_creator.py</code> <pre><code>def launch_sweeps(entity: str, project: str, sweep_configs: list[dict]) -&gt; list[str]:\n    \"\"\"\n    Launch wandb sweeps for the given configurations.\n\n    Args:\n        entity (str): Name of the wandb entity.\n        project (str): Name of the wandb project.\n        sweep_configs (List[Dict]): List of sweep configurations.\n\n    Returns:\n        List[str]: List of sweep ids.\n    \"\"\"\n    sweep_ids = [\n        wandb.sweep(cfg, entity=entity, project=project) for cfg in sweep_configs\n    ]\n    return sweep_ids\n</code></pre>"},{"location":"reference/run/multi_run/sweep_creator/#run.multi_run.sweep_creator.write_bash_script","title":"<code>write_bash_script(filename, main_command, sweep_ids, mode)</code>","text":"<p>Write a bash script to launch wandb agents in tmux sessions.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>Name of the bash script file.</p> required <code>main_command</code> <code>str</code> <p>Main command to run in the tmux session.</p> required <code>sweep_ids</code> <code>list[str]</code> <p>List of sweep ids.</p> required Source code in <code>src/run/multi_run/sweep_creator.py</code> <pre><code>def write_bash_script(\n    filename: Path,\n    main_command: str,\n    sweep_ids: list[str],\n    mode: Literal['lacc', 'david'],\n) -&gt; None:\n    \"\"\"\n    Write a bash script to launch wandb agents in tmux sessions.\n\n    Args:\n        filename (str): Name of the bash script file.\n        main_command (str): Main command to run in the tmux session.\n        sweep_ids (list[str]): List of sweep ids.\n    \"\"\"\n    if mode == 'lacc':\n        conda_path = 'source $HOME/miniforge3/etc/profile.d/conda.sh'\n        cd_path = 'cd $HOME/eyebench_private'\n    elif mode == 'david':\n        conda_path = 'source ~/.conda/envs/eyebench/etc/profile.d/mamba.sh'\n        cd_path = 'cd /mnt/mlshare/reich3/eyebench_private'\n    else:\n        raise ValueError(f'Invalid mode: {mode}')\n\n    full_command = f\"\"\"#!/bin/bash\n\nif ! command -v tmux &amp;&gt;/dev/null; then\n    echo \"tmux could not be found, please install tmux first.\"\n    exit 1  \nfi\n\n{conda_path}\n{cd_path}\n\nGPU_NUM=$1\nRUNS_ON_GPU=${{2:-1}}\nfor ((i=1; i&lt;=RUNS_ON_GPU; i++)); do\n    session_name=\"wandb-gpu${{GPU_NUM}}-dup${{i}}-unified-{sweep_ids[0]}-{len(sweep_ids)}\"\n    tmux new-session -d -s \"${{session_name}}\" \"{main_command}\"; tmux set-option -t \"${{session_name}}\" remain-on-exit off\n    echo \"Launched W&amp;B agent for GPU ${{GPU_NUM}}, Dup ${{i}} in tmux session ${{session_name}}\"\ndone\n\"\"\"\n\n    with open(filename, 'w', encoding='utf-8') as f:\n        f.write(full_command)\n    os.chmod(filename, os.stat(filename).st_mode | stat.S_IEXEC)\n    logger.info(f'Created bash script: {filename}')\n</code></pre>"},{"location":"reference/run/multi_run/sweep_creator/#run.multi_run.sweep_creator.write_slurm_script","title":"<code>write_slurm_script(filename, hyper_args, sweep_ids, slurm_qos)</code>","text":"<p>Write a slurm script to launch wandb agents in slurm jobs.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>Name of the slurm script file.</p> required <code>hyper_args</code> <code>HyperArgs</code> <p>Hyperparameters for the sweep.</p> required <code>sweep_ids</code> <code>list[str]</code> <p>List of sweep ids.</p> required <code>slurm_qos</code> <code>str</code> <p>Slurm quality of service (normal or basic).</p> required Source code in <code>src/run/multi_run/sweep_creator.py</code> <pre><code>def write_slurm_script(\n    filename: Path,\n    hyper_args: HyperArgs,\n    sweep_ids: list[str],\n    slurm_qos: Literal['normal', 'basic'],\n) -&gt; None:\n    \"\"\"\n    Write a slurm script to launch wandb agents in slurm jobs.\n\n    Args:\n        filename (str): Name of the slurm script file.\n        hyper_args (HyperArgs): Hyperparameters for the sweep.\n        sweep_ids (list[str]): List of sweep ids.\n        slurm_qos (str): Slurm quality of service (normal or basic).\n    \"\"\"\n    base_srun_command = f\"\"\"\nsrun --overlap --ntasks=1 --nodes=1 --cpus-per-task=$SLURM_CPUS_PER_TASK -p work,mig \\\\\n    --container-image=/rg/berzak_prj/shubi/prj/rev05_pytorchlightning+pytorch_lightning.sqsh \\\\\n    --container-mounts=\"/rg/berzak_prj/shubi:/home/shubi\" \\\\\n    --container-workdir=/home/shubi/eyebench_private \\\\\n    bash -c \"\necho 'Starting job on $(date)'\nsource /home/shubi/prj/nvidia_pytorch_25_03_py3_mamba_wrapper.sh\nconda activate eyebench\nwandb agent {hyper_args.wandb_entity}/{hyper_args.wandb_project}/$SWEEP_ID\"\n    \"\"\"\n    srun_command = f\"\"\"{base_srun_command}\"\"\"\n    if hyper_args.num_duplicates_per_gpu &gt; 1:\n        srun_command = f\"\"\"{srun_command}\\nsleep 600\\nwait\"\"\"\n        for _ in range(hyper_args.num_duplicates_per_gpu - 1):\n            srun_command += f\"\"\"{base_srun_command}\\nsleep 10\\n\"\"\"\n        srun_command += 'wait'\n\n    with open(filename, 'w', encoding='utf-8') as f:\n        f.write(\n            f\"\"\"#!/bin/bash\n\n#SBATCH --job-name={hyper_args.model}_{hyper_args.data_task}-array\n#SBATCH --output=logs/{hyper_args.model}_{hyper_args.data_task}-%A_%a.out\n#SBATCH --error=logs/{hyper_args.model}_{hyper_args.data_task}-%A_%a.err\n#SBATCH --partition=work,mig\n#SBATCH --ntasks={hyper_args.num_duplicates_per_gpu}\n#SBATCH --nodes=1\n#SBATCH --gpus={hyper_args.gpu_count}\n#SBATCH --qos={slurm_qos}\n#SBATCH --cpus-per-task={hyper_args.slurm_cpus}\n#SBATCH --mem={hyper_args.slurm_mem}\n#SBATCH --array=0-{len(sweep_ids) - 1}\n#SBATCH --mail-type=ALL\n#SBATCH --mail-user={hyper_args.slurm_mailto}\n\nsweep_ids=({' '.join(sweep_ids)})\nSWEEP_ID=${{sweep_ids[$SLURM_ARRAY_TASK_ID]}}\n\n{srun_command}\n\"\"\"\n        )\n    os.chmod(filename, os.stat(filename).st_mode | stat.S_IEXEC)\n    logger.info(f'Created Slurm array job script: {filename}')\n</code></pre>"},{"location":"reference/run/multi_run/utils/","title":"utils","text":""},{"location":"reference/run/multi_run/utils/#run.multi_run.utils.count_hyperparameter_configs","title":"<code>count_hyperparameter_configs(config, log_specific_values=True, n_hours=24)</code>","text":"<p>Traverse an arbitrarily nested dict, find all dicts with a 'values' key whose value is a list, and:     1. Print the full key path, the number of values, and the values themselves     2. Compute the product of all those lengths     3. Print the time each run should get if runs are evenly distributed over 24 hours</p> Source code in <code>src/run/multi_run/utils.py</code> <pre><code>def count_hyperparameter_configs(\n    config: dict, log_specific_values: bool = True, n_hours: int = 24\n) -&gt; tuple:\n    \"\"\"\n    Traverse an arbitrarily nested dict, find all dicts with a 'values' key\n    whose value is a list, and:\n        1. Print the full key path, the number of values, and the values themselves\n        2. Compute the product of all those lengths\n        3. Print the time each run should get if runs are evenly distributed over 24 hours\n    \"\"\"\n\n    counts = {}\n    values_map = {}\n\n    def recurse(d: dict, path: str = ''):\n        for key, val in d.items():\n            current_path = f'{path}.{key}' if path else key\n            if isinstance(val, dict):\n                if 'values' in val and isinstance(val['values'], list):\n                    counts[current_path] = len(val['values'])\n                    values_map[current_path] = val['values']\n                else:\n                    recurse(val, current_path)\n\n    recurse(config)\n\n    if log_specific_values:\n        # Print per-parameter counts and their values\n        for param, n in counts.items():\n            if n &gt; 1:\n                logger.info(\n                    f'{param.replace(\".parameters\", \"\")}: {n} possible values \u2192 {values_map[param]}'\n                )\n\n    # Compute total combinations\n    total = 1\n    for n in counts.values():\n        total *= n\n\n    # Compute time per run over a 24-hour period\n    seconds = n_hours * 3600\n    time_per_run = seconds / total if total else 0\n    # Format as DD:HH:MM:SS\n    td = datetime.timedelta(seconds=round(time_per_run))\n    days = td.days\n    hours, remainder = divmod(td.seconds, 3600)\n    minutes, seconds = divmod(remainder, 60)\n    formatted = f'{days:02}:{hours:02}:{minutes:02}:{seconds:02}'\n    logger.info(f'Total number of runs: {total}. Time per run: {formatted}')\n    return formatted, int(total)\n</code></pre>"},{"location":"reference/run/single_run/test_dl/","title":"test_dl","text":"<p>Main file for testing cognitive state decoding models</p>"},{"location":"reference/run/single_run/test_dl/#run.single_run.test_dl.get_fold_paths","title":"<code>get_fold_paths(base_path)</code>","text":"<p>Return sorted fold directories under the provided evaluation path.</p> Source code in <code>src/run/single_run/test_dl.py</code> <pre><code>def get_fold_paths(base_path: Path) -&gt; list[Path]:\n    \"\"\"Return sorted fold directories under the provided evaluation path.\"\"\"\n    fold_paths: list[Path] = []\n    for path in base_path.glob('fold_index=*'):\n        if not path.is_dir():\n            continue\n        try:\n            int(path.name.split('=')[1])\n        except (IndexError, ValueError):\n            logger.warning(f'Skipping unexpected directory {path.name}')\n            continue\n        fold_paths.append(path)\n    fold_paths.sort(key=lambda p: int(p.name.split('=')[1]))\n    return fold_paths\n</code></pre>"},{"location":"reference/run/single_run/test_ml/","title":"test_ml","text":"summary <p>Given a list of experiments, where each experiment is a dictionary that maps fold_idx to a completed w&amp;b sweep, this script ought to 1. take the best hyperparameters from each fold (sweep) or from a    designated run_id if requested, 2. fit the model on that fold, 3. save test predictions to file.</p>"},{"location":"reference/run/single_run/test_ml/#run.single_run.test_ml.Experiment","title":"<code>Experiment</code>  <code>dataclass</code>","text":"<p>Class representing an experiment.</p> Source code in <code>src/run/single_run/test_ml.py</code> <pre><code>@dataclass\nclass Experiment:\n    \"\"\"\n    Class representing an experiment.\n    \"\"\"\n\n    dataset_name: str = field(init=False)\n    model_name: str = field(init=False)\n    model_args: dataclass\n    data_args: dataclass\n    save_folder_name: Path = field(default_factory=Path)\n    sweeps: list[Sweep] = field(default_factory=list)\n    wandb_project: str = 'ml_debug'\n\n    def load_sweep_ids_from_yaml(self, yaml_path: str) -&gt; list[str]:\n        \"\"\"\n        Load sweep IDs from a YAML file.\n        \"\"\"\n        with open(yaml_path, 'r', encoding='utf-8') as f:\n            sweep_cfg = yaml.safe_load(f)\n        return sweep_cfg.get('sweep_ids', [])\n\n    def __post_init__(self):\n        self.dataset_name = self.data_args.__name__\n        self.model_name = self.model_args.__name__\n        # Load sweep IDs from the YAML file\n        sweep_ids = self.load_sweep_ids_from_yaml(\n            f'sweeps/{self.wandb_project}/configs/{self.model_name}_{self.dataset_name}.yaml'\n        )\n        # Create Sweep objects for each sweep ID\n        self.sweeps = [Sweep(sweep_id=sweep_id) for sweep_id in sweep_ids]\n\n        self.save_folder_name = Path(\n            f'results/raw/+data={self.dataset_name},'\n            f'+model={self.model_name},+trainer=TrainerML,trainer.wandb_job_type='\n            f'{self.model_name}_{self.dataset_name}',\n        )\n\n        self.save_folder_name.mkdir(parents=True, exist_ok=True)\n</code></pre>"},{"location":"reference/run/single_run/test_ml/#run.single_run.test_ml.Experiment.load_sweep_ids_from_yaml","title":"<code>load_sweep_ids_from_yaml(yaml_path)</code>","text":"<p>Load sweep IDs from a YAML file.</p> Source code in <code>src/run/single_run/test_ml.py</code> <pre><code>def load_sweep_ids_from_yaml(self, yaml_path: str) -&gt; list[str]:\n    \"\"\"\n    Load sweep IDs from a YAML file.\n    \"\"\"\n    with open(yaml_path, 'r', encoding='utf-8') as f:\n        sweep_cfg = yaml.safe_load(f)\n    return sweep_cfg.get('sweep_ids', [])\n</code></pre>"},{"location":"reference/run/single_run/test_ml/#run.single_run.test_ml.HyperArgs","title":"<code>HyperArgs</code>","text":"<p>               Bases: <code>Tap</code></p> <p>Command line arguments for the script.</p> Source code in <code>src/run/single_run/test_ml.py</code> <pre><code>class HyperArgs(Tap):\n    \"\"\"\n    Command line arguments for the script.\n    \"\"\"\n\n    wandb_entity: str = 'EyeRead'  # Name of the wandb entity to log to.\n    wandb_run_id: str | None = None  # Provide if you want a single run.\n    data_task: str = 'CopCo_TYP'  # Name of the data task (e.g., CopCo_TYP).\n    wandb_project: str = 'CopCo_TYP_20250714'  # Name of the wandb project.\n</code></pre>"},{"location":"reference/run/single_run/test_ml/#run.single_run.test_ml.Sweep","title":"<code>Sweep</code>  <code>dataclass</code>","text":"<p>Class representing a sweep in wandb.</p> <p>Attributes:</p> Name Type Description <code>sweep_id</code> <code>str</code> <p>The ID of the sweep in wandb.</p> <code>cfg_of_best</code> <code>dict</code> <p>The configuration of the best run in the sweep.</p> <code>fold_index</code> <code>int | None</code> <p>The index of the fold, if applicable.</p> Source code in <code>src/run/single_run/test_ml.py</code> <pre><code>@dataclass\nclass Sweep:\n    \"\"\"\n    Class representing a sweep in wandb.\n\n    Attributes:\n        sweep_id (str): The ID of the sweep in wandb.\n        cfg_of_best (dict): The configuration of the best run in the sweep.\n        fold_index (int | None): The index of the fold, if applicable.\n    \"\"\"\n\n    sweep_id: str\n    cfg_of_best: dict = field(default_factory=dict)\n    fold_index: int | None = None\n</code></pre>"},{"location":"reference/run/single_run/test_ml/#run.single_run.test_ml.checks","title":"<code>checks(experiments_list)</code>","text":"<p>Basic consistency check: ensure that each sweep_id is unique across experiments.</p> Source code in <code>src/run/single_run/test_ml.py</code> <pre><code>def checks(experiments_list: list[Experiment]) -&gt; None:\n    \"\"\"\n    Basic consistency check: ensure that each sweep_id is unique across experiments.\n    \"\"\"\n    sweep_ids = [sweep.sweep_id for exp in experiments_list for sweep in exp.sweeps]\n    assert len(sweep_ids) == len(set(sweep_ids)), 'Duplicate sweep IDs found!'\n</code></pre>"},{"location":"reference/run/single_run/test_ml/#run.single_run.test_ml.get_config_from_run","title":"<code>get_config_from_run(api, entity, project, run_id)</code>","text":"<p>Fetches the config of a single run by run_id.</p> Source code in <code>src/run/single_run/test_ml.py</code> <pre><code>def get_config_from_run(\n    api: wandb.Api, entity: str, project: str, run_id: str\n) -&gt; dict[str, Any]:\n    \"\"\"\n    Fetches the config of a single run by run_id.\n    \"\"\"\n    return api.run(path=f'{entity}/{project}/{run_id}').config\n</code></pre>"},{"location":"reference/run/single_run/test_ml/#run.single_run.test_ml.get_config_from_sweep","title":"<code>get_config_from_sweep(api, entity, project, sweep_id)</code>","text":"<p>Fetches the config of the best run (by the sweep's objective) from a given sweep_id.</p> Source code in <code>src/run/single_run/test_ml.py</code> <pre><code>def get_config_from_sweep(\n    api: wandb.Api,\n    entity: str,\n    project: str,\n    sweep_id: str,\n) -&gt; dict[str, Any]:\n    \"\"\"\n    Fetches the config of the *best run* (by the sweep's objective) from a given sweep_id.\n    \"\"\"\n    sweep_obj = api.sweep(path=f'{entity}/{project}/{sweep_id}')\n    best_run = sweep_obj.best_run()\n    return best_run.config\n</code></pre>"},{"location":"reference/run/single_run/test_ml/#run.single_run.test_ml.predict_on_val_and_test","title":"<code>predict_on_val_and_test(model, val_datasets, test_datasets)</code>","text":"<p>Predict on all val and test datasets, returning one list of results.</p> Source code in <code>src/run/single_run/test_ml.py</code> <pre><code>def predict_on_val_and_test(\n    model: torch.nn.Module,\n    val_datasets: list[np.ndarray],\n    test_datasets: list[np.ndarray],\n) -&gt; list[np.ndarray]:\n    \"\"\"Predict on all val and test datasets, returning one list of results.\"\"\"\n    results = []\n    # Predict on validation datasets\n    for val_dataset in val_datasets:\n        results.append(model.predict(val_dataset))\n\n    # Predict on test datasets\n    for test_dataset in test_datasets:\n        results.append(model.predict(test_dataset))\n\n    return results\n</code></pre>"},{"location":"reference/run/single_run/test_ml/#run.single_run.test_ml.process_results","title":"<code>process_results(results, dm, cfg, fold_index)</code>","text":"<p>Given all results from val and test datasets, build a unified DataFrame with all relevant columns.</p>"},{"location":"reference/run/single_run/test_ml/#run.single_run.test_ml.process_results--todo-almost-duplicate-code-with-test_dlpy","title":"TODO almost duplicate code with test_dl.py","text":"Source code in <code>src/run/single_run/test_ml.py</code> <pre><code>def process_results(\n    results: list[tuple[torch.Tensor, ...]],\n    dm: base_datamodule.ETDataModuleFast,\n    cfg: Args,\n    fold_index: int,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Given all results from val and test datasets, build\n    a unified DataFrame with all relevant columns.\n    # TODO almost duplicate code with test_dl.py\n    \"\"\"\n    group_level_metrics = []\n\n    for index, eval_type_results in enumerate(results):\n        # based on predict_dataloader (first 3 are val, last three test)\n        eval_type = 'val' if index in [0, 1, 2] else 'test'\n        if eval_type == 'val':\n            dataset = dm.val_datasets[index]\n        else:\n            dataset = dm.test_datasets[index % 3]\n\n        # Decide whether we have grouped trial keys\n\n        trial_info = extract_trial_info(\n            dataset, cols_to_keep=cfg.data.groupby_columns\n        ).reset_index(drop=True)\n\n        # Unpack model outputs\n        preds, probs, y_true = eval_type_results\n        if probs is None:\n            probs = preds\n        df = pd.DataFrame(\n            {\n                'label': y_true.numpy(),\n                'prediction_prob': probs.numpy().tolist(),\n                'eval_regime': REGIMES[index % 3],\n                'eval_type': eval_type,\n                'fold_index': fold_index,\n            },\n        )\n\n        group_level_metrics.append(pd.concat([df, trial_info], axis=1))\n\n    res = pd.concat(group_level_metrics)\n\n    return res\n</code></pre>"},{"location":"reference/run/single_run/train/","title":"train","text":"<p>Main file for cognitive state decoding training</p>"},{"location":"reference/run/single_run/utils/","title":"utils","text":""},{"location":"reference/run/single_run/utils/#run.single_run.utils.convert_string_to_list","title":"<code>convert_string_to_list(s)</code>","text":"<p>Converts a Pandas Series containing stringified lists into actual lists.</p> <p>Parameters:</p> Name Type Description Default <code>s</code> <code>Series</code> <p>Series with stringified lists.</p> required <p>Returns:</p> Type Description <code>list[list[float]]</code> <p>list[list[float]]: List of lists with float values.</p> Source code in <code>src/run/single_run/utils.py</code> <pre><code>def convert_string_to_list(s: pd.Series) -&gt; list[list[float]]:\n    \"\"\"\n    Converts a Pandas Series containing stringified lists into actual lists.\n\n    Args:\n        s (pd.Series): Series with stringified lists.\n\n    Returns:\n        list[list[float]]: List of lists with float values.\n    \"\"\"\n    return s.apply(ast.literal_eval).tolist()\n</code></pre>"},{"location":"reference/run/single_run/utils/#run.single_run.utils.get_config","title":"<code>get_config(config_path)</code>","text":"<p>Load the config for testing.</p> Source code in <code>src/run/single_run/utils.py</code> <pre><code>def get_config(config_path: Path) -&gt; Args:\n    \"\"\"\n    Load the config for testing.\n    \"\"\"\n    output_dir = to_absolute_path(str(config_path))\n    overrides = OmegaConf.load(join(output_dir, '.hydra/overrides.yaml'))\n    hydra_config = OmegaConf.load(join(output_dir, '.hydra/hydra.yaml'))\n\n    # getting the config name from the previous job.\n    config_name = hydra_config.hydra.job.config_name\n\n    # compose a new config from scratch\n    cfg = compose(config_name, overrides=overrides)\n    updated_cfg = instantiate(cfg, _convert_='object')\n\n    return updated_cfg\n</code></pre>"},{"location":"reference/run/single_run/utils/#run.single_run.utils.instantiate_config","title":"<code>instantiate_config(cfg)</code>","text":"<p>Instantiate the config object with the appropriate datamodule and model.</p> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <code>dict</code> <p>The configuration object.</p> required <p>Returns:</p> Name Type Description <code>Args</code> <code>Args</code> <p>The instantiated configuration object.</p> Source code in <code>src/run/single_run/utils.py</code> <pre><code>def instantiate_config(cfg: DictConfig) -&gt; Args:\n    \"\"\"\n    Instantiate the config object with the appropriate datamodule and model.\n\n    Args:\n        cfg (dict): The configuration object.\n\n    Returns:\n        Args: The instantiated configuration object.\n    \"\"\"\n    args: Args = instantiate(config=cfg, _convert_='object')\n    args.data.full_dataset_name = args.data.dataset_name\n    args.model.full_model_name = args.model.model_name\n    args.model.max_time_limit = args.model.max_time\n    args.model.is_ml = args.model.base_model_name in MLModelNames\n    args.model.use_class_weighted_loss = (\n        args.model.use_class_weighted_loss\n        if len(list(args.data.class_names)) &gt; 1\n        else False\n    )\n    return _configure_model_backbone(args)\n</code></pre>"},{"location":"reference/run/single_run/utils/#run.single_run.utils.update_cfg_with_wandb","title":"<code>update_cfg_with_wandb(cfg)</code>","text":"<p>Update the configuration object with the wandb config. This function will overwrite the config with the wandb config if the wandb config is not empty. Args:</p> <pre><code>cfg (Args): The configuration object.\n</code></pre> <p>Returns:     Args: The updated configuration object.</p> Source code in <code>src/run/single_run/utils.py</code> <pre><code>def update_cfg_with_wandb(cfg: Args) -&gt; Args:\n    \"\"\"\n    Update the configuration object with the wandb config.\n    This function will overwrite the config with the wandb config if the\n    wandb config is not empty.\n    Args:\n\n        cfg (Args): The configuration object.\n    Returns:\n        Args: The updated configuration object.\n    \"\"\"\n\n    logger.info('Overwriting args with wandb config')\n\n    def validate_and_setattr(\n        obj: object, attr_name: str, value: str | int | float\n    ) -&gt; None:\n        if not hasattr(obj, attr_name):\n            raise AttributeError(\n                f\"Attribute '{attr_name}' does not exist in {obj.__class__.__name__}\",\n            )\n        setattr(obj, attr_name, value)\n\n    for key, value in wandb.config.items():\n        if isinstance(value, dict):\n            if not hasattr(cfg, key):\n                raise AttributeError(\n                    f\"Attribute '{key}' does not exist in {cfg.__class__.__name__}\",\n                )\n            sub_cfg = getattr(cfg, key)\n\n            for sub_key, sub_value in value.items():\n                if isinstance(sub_value, dict):\n                    if not hasattr(sub_cfg, sub_key):\n                        raise AttributeError(\n                            f\"Attribute '{sub_key}' does not exist in {key}\",\n                        )\n                    sub_sub_cfg = getattr(sub_cfg, sub_key)\n\n                    for sub_sub_key, sub_sub_value in sub_value.items():\n                        logger.info(\n                            f'Setting cfg.{key}.{sub_key}.{sub_sub_key} to {sub_sub_value}',\n                        )\n                        validate_and_setattr(sub_sub_cfg, sub_sub_key, sub_sub_value)\n                else:\n                    logger.info(f'Setting cfg.{key}.{sub_key} to {sub_value}')\n                    validate_and_setattr(sub_cfg, sub_key, sub_value)\n        else:\n            logger.info(f'Setting cfg.{key} to {value}')\n            validate_and_setattr(cfg, key, value)\n\n    return _configure_model_backbone(cfg)\n</code></pre>"},{"location":"results/","title":"EyeBench V1.0 Benchmark Results","text":"<p>Interactive tables sourced from the latest formatted benchmark exports. Values show the mean and standard deviation across folds.</p> <p></p>"},{"location":"results/#overall-leaderboard-test","title":"Overall Leaderboard (Test)","text":"<p>Macro-level comparison across every benchmark task on the held-out test folds.</p> ModelLayoutSaccade/FixationWord-LevelTrial-LevelLinguisticEmbeddingsAvg Normalized ScoreMean RankMajority Class / Chance------0.36710.1Reading Speed---\u2713--0.42110.29Text-Only Roberta-----\u27130.6726.1Logistic Regression [meziere2023using]---\u2713--0.5717.67SVM [hollenstein2023zuco]---\u2713--0.5217.38Random Forest [makowski2024detection]-\u2713-\u2713\u2713-0.7884.48AhnRNN [ahn2020towards]\u2713\u2713----0.369.48AhnCNN [ahn2020towards]\u2713\u2713----0.5316.95BEyeLSTM [reich_inferring_2022]\u2713\u2713\u2713\u2713\u2713-0.4149.1PLM-AS [Yang2023PLMASPL]-\u2713---\u27130.4948.0PLM-AS-RM [haller2022eye]-\u2713\u2713--\u27130.4758.95RoBERTEye-W [Shubi2024Finegrained]\u2713-\u2713\u2713\u2713\u27130.7574.43RoBERTEye-F [Shubi2024Finegrained]\u2713\u2713\u2713\u2713\u2713\u27130.6546.81MAG-Eye [Shubi2024Finegrained]\u2713-\u2713\u2713\u2713\u27130.6865.62PostFusion-Eye [Shubi2024Finegrained]\u2713\u2713\u2713\u2713\u2713\u27130.5469.81"},{"location":"results/#results-tasks-combined","title":"Results Tasks Combined","text":"<p>Side-by-side view of the primary metric for each EyeBench task, averaged over folds.</p> ModelOneStop RCSBSAT RCPoTeC RCPoTeC DEIITBHGC CVCopCo TYPSBSAT STDCopCo RCSMECOL2 LEXMajority Class / Chance50.0 \u00b1 0.050.0 \u00b1 0.050.0 \u00b1 0.051.4 \u00b1 1.350.4 \u00b1 0.349.9 \u00b1 0.00.73 \u00b1 0.02.66 \u00b1 0.112.45 \u00b1 0.1Reading Speed49.6 \u00b1 0.850.8 \u00b1 1.651.9 \u00b1 2.260.4 \u00b1 1.757.3 \u00b1 0.756.6 \u00b1 2.10.77 \u00b1 0.02.68 \u00b1 0.128.95 \u00b1 14.3Text-Only Roberta61.1 \u00b1 1.055.9 \u00b1 2.356.3 \u00b1 1.662.0 \u00b1 4.058.8 \u00b1 1.550.1 \u00b1 0.40.72 \u00b1 0.02.64 \u00b1 0.012.45 \u00b1 0.1Logistic Regression [meziere2023using]53.0 \u00b1 0.852.3 \u00b1 1.054.1 \u00b1 0.754.0 \u00b1 1.754.6 \u00b1 1.180.6 \u00b1 2.30.82 \u00b1 0.02.74 \u00b1 0.110.34 \u00b1 0.0SVM [hollenstein2023zuco]50.7 \u00b1 0.750.2 \u00b1 0.950.6 \u00b1 0.754.5 \u00b1 1.554.5 \u00b1 0.672.5 \u00b1 1.60.73 \u00b1 0.02.76 \u00b1 0.110.94 \u00b1 0.0Random Forest [makowski2024detection]58.0 \u00b1 0.652.3 \u00b1 0.754.3 \u00b1 1.262.3 \u00b1 3.456.4 \u00b1 0.382.9 \u00b1 1.50.77 \u00b1 0.02.51 \u00b1 0.110.09 \u00b1 0.0AhnRNN [ahn2020towards]50.0 \u00b1 0.050.0 \u00b1 0.050.0 \u00b1 0.050.0 \u00b1 0.150.9 \u00b1 0.750.0 \u00b1 0.10.72 \u00b1 0.02.63 \u00b1 0.012.46 \u00b1 0.1AhnCNN [ahn2020towards]49.7 \u00b1 0.750.8 \u00b1 1.851.6 \u00b1 2.060.6 \u00b1 3.452.9 \u00b1 1.083.4 \u00b1 1.10.72 \u00b1 0.02.63 \u00b1 0.012.27 \u00b1 0.0BEyeLSTM [reich_inferring_2022]52.5 \u00b1 0.850.1 \u00b1 0.554.7 \u00b1 1.451.8 \u00b1 3.551.3 \u00b1 1.280.2 \u00b1 1.51.43 \u00b1 0.72.63 \u00b1 0.012.68 \u00b1 0.3PLM-AS [Yang2023PLMASPL]56.1 \u00b1 0.949.5 \u00b1 1.156.5 \u00b1 0.351.3 \u00b1 2.451.4 \u00b1 0.657.9 \u00b1 4.60.71 \u00b1 0.02.64 \u00b1 0.012.46 \u00b1 0.1PLM-AS-RM [haller2022eye]58.4 \u00b1 0.553.9 \u00b1 1.159.0 \u00b1 1.264.2 \u00b1 4.053.4 \u00b1 1.569.2 \u00b1 1.01.21 \u00b1 0.02.67 \u00b1 0.031.79 \u00b1 0.2RoBERTEye-W [Shubi2024Finegrained]61.4 \u00b1 0.957.4 \u00b1 3.756.8 \u00b1 1.262.5 \u00b1 7.358.0 \u00b1 2.275.6 \u00b1 2.30.71 \u00b1 0.02.65 \u00b1 0.011.16 \u00b1 0.1RoBERTEye-F [Shubi2024Finegrained]61.9 \u00b1 0.856.0 \u00b1 2.754.7 \u00b1 2.264.5 \u00b1 3.158.4 \u00b1 1.170.8 \u00b1 1.90.73 \u00b1 0.02.66 \u00b1 0.111.25 \u00b1 0.2MAG-Eye [Shubi2024Finegrained]62.9 \u00b1 0.556.0 \u00b1 2.158.3 \u00b1 1.357.6 \u00b1 7.158.0 \u00b1 2.053.3 \u00b1 3.70.72 \u00b1 0.02.64 \u00b1 0.012.46 \u00b1 0.1PostFusion-Eye [Shubi2024Finegrained]61.1 \u00b1 0.655.4 \u00b1 2.653.0 \u00b1 1.853.6 \u00b1 0.957.5 \u00b1 1.473.2 \u00b1 1.50.8 \u00b1 0.12.79 \u00b1 0.114.0 \u00b1 0.4"},{"location":"results/copco_rcs/","title":"CopCo RCS","text":"<p>Reading Comprehension Skill (CopCo)</p> <p></p>"},{"location":"results/copco_rcs/#test","title":"Test","text":"ModelUnseen Reader RMSEUnseen Text RMSEUnseen Text and Reader RMSEAverage RMSEUnseen Reader MAEUnseen Text MAEUnseen Text and Reader MAEAverage MAEUnseen Reader R\u00b2Unseen Text R\u00b2Unseen Text and Reader R\u00b2Average R\u00b2Majority Class / Chance2.66 \u00b1 0.12.67 \u00b1 0.02.56 \u00b1 0.02.66 \u00b1 0.12.19 \u00b1 0.12.2 \u00b1 0.02.13 \u00b1 0.12.18 \u00b1 0.0-0.05 \u00b1 0.0-0.05 \u00b1 0.0-0.09 \u00b1 0.0-0.03 \u00b1 0.0Reading Speed2.69 \u00b1 0.12.68 \u00b1 0.12.6 \u00b1 0.02.68 \u00b1 0.12.21 \u00b1 0.12.24 \u00b1 0.02.18 \u00b1 0.12.22 \u00b1 0.0-0.07 \u00b1 0.0-0.06 \u00b1 0.0-0.12 \u00b1 0.0-0.05 \u00b1 0.0Text-Only Roberta2.65 \u00b1 0.12.63 \u00b1 0.02.61 \u00b1 0.12.64 \u00b1 0.02.22 \u00b1 0.12.19 \u00b1 0.12.23 \u00b1 0.12.21 \u00b1 0.1-0.04 \u00b1 0.0-0.01 \u00b1 0.0-0.13 \u00b1 0.1-0.02 \u00b1 0.0Logistic Regression [meziere2023using]2.74 \u00b1 0.12.75 \u00b1 0.02.7 \u00b1 0.02.74 \u00b1 0.12.26 \u00b1 0.12.3 \u00b1 0.02.3 \u00b1 0.12.28 \u00b1 0.0-0.11 \u00b1 0.0-0.12 \u00b1 0.0-0.21 \u00b1 0.0-0.1 \u00b1 0.0SVM [hollenstein2023zuco]2.89 \u00b1 0.12.63 \u00b1 0.12.74 \u00b1 0.12.76 \u00b1 0.12.28 \u00b1 0.02.1 \u00b1 0.12.17 \u00b1 0.12.18 \u00b1 0.0-0.28 \u00b1 0.2-0.03 \u00b1 0.1-0.27 \u00b1 0.1-0.12 \u00b1 0.1Random Forest [makowski2024detection]2.76 \u00b1 0.12.22 \u00b1 0.12.54 \u00b1 0.12.51 \u00b1 0.12.2 \u00b1 0.11.75 \u00b1 0.02.08 \u00b1 0.11.99 \u00b1 0.1-0.14 \u00b1 0.10.27 \u00b1 0.1-0.07 \u00b1 0.10.08 \u00b1 0.0AhnRNN [ahn2020towards]2.64 \u00b1 0.12.62 \u00b1 0.02.58 \u00b1 0.12.63 \u00b1 0.02.21 \u00b1 0.12.19 \u00b1 0.02.19 \u00b1 0.12.2 \u00b1 0.0-0.03 \u00b1 0.0-0.0 \u00b1 0.0-0.1 \u00b1 0.1-0.01 \u00b1 0.0AhnCNN [ahn2020towards]2.64 \u00b1 0.12.61 \u00b1 0.02.58 \u00b1 0.12.63 \u00b1 0.02.22 \u00b1 0.12.19 \u00b1 0.02.19 \u00b1 0.12.2 \u00b1 0.0-0.03 \u00b1 0.0-0.0 \u00b1 0.0-0.11 \u00b1 0.1-0.01 \u00b1 0.0BEyeLSTM [reich_inferring_2022]2.64 \u00b1 0.12.62 \u00b1 0.02.59 \u00b1 0.12.63 \u00b1 0.02.22 \u00b1 0.12.19 \u00b1 0.02.19 \u00b1 0.12.21 \u00b1 0.0-0.04 \u00b1 0.0-0.01 \u00b1 0.0-0.11 \u00b1 0.1-0.01 \u00b1 0.0PLM-AS [Yang2023PLMASPL]2.66 \u00b1 0.12.62 \u00b1 0.02.6 \u00b1 0.12.64 \u00b1 0.02.21 \u00b1 0.12.19 \u00b1 0.12.19 \u00b1 0.12.2 \u00b1 0.0-0.05 \u00b1 0.0-0.01 \u00b1 0.0-0.12 \u00b1 0.1-0.02 \u00b1 0.0PLM-AS-RM [haller2022eye]2.69 \u00b1 0.12.65 \u00b1 0.02.6 \u00b1 0.12.67 \u00b1 0.02.23 \u00b1 0.12.23 \u00b1 0.02.19 \u00b1 0.12.23 \u00b1 0.1-0.07 \u00b1 0.0-0.03 \u00b1 0.0-0.12 \u00b1 0.0-0.04 \u00b1 0.0RoBERTEye-W [Shubi2024Finegrained]2.67 \u00b1 0.12.63 \u00b1 0.02.6 \u00b1 0.12.65 \u00b1 0.02.24 \u00b1 0.12.2 \u00b1 0.02.2 \u00b1 0.12.22 \u00b1 0.0-0.06 \u00b1 0.0-0.02 \u00b1 0.0-0.12 \u00b1 0.0-0.03 \u00b1 0.0RoBERTEye-F [Shubi2024Finegrained]2.67 \u00b1 0.12.64 \u00b1 0.12.65 \u00b1 0.12.66 \u00b1 0.12.24 \u00b1 0.12.2 \u00b1 0.12.25 \u00b1 0.12.23 \u00b1 0.1-0.06 \u00b1 0.0-0.02 \u00b1 0.0-0.16 \u00b1 0.1-0.04 \u00b1 0.0MAG-Eye [Shubi2024Finegrained]2.65 \u00b1 0.12.63 \u00b1 0.02.59 \u00b1 0.12.64 \u00b1 0.02.22 \u00b1 0.12.2 \u00b1 0.12.21 \u00b1 0.12.21 \u00b1 0.0-0.04 \u00b1 0.0-0.01 \u00b1 0.0-0.11 \u00b1 0.1-0.02 \u00b1 0.0PostFusion-Eye [Shubi2024Finegrained]2.9 \u00b1 0.12.67 \u00b1 0.12.77 \u00b1 0.12.79 \u00b1 0.12.4 \u00b1 0.12.23 \u00b1 0.12.37 \u00b1 0.12.33 \u00b1 0.1-0.25 \u00b1 0.1-0.05 \u00b1 0.0-0.27 \u00b1 0.0-0.14 \u00b1 0.0"},{"location":"results/copco_rcs/#validation","title":"Validation","text":"ModelUnseen Reader RMSEUnseen Text RMSEUnseen Text and Reader RMSEAverage RMSEUnseen Reader MAEUnseen Text MAEUnseen Text and Reader MAEAverage MAEUnseen Reader R\u00b2Unseen Text R\u00b2Unseen Text and Reader R\u00b2Average R\u00b2Majority Class / Chance2.82 \u00b1 0.12.48 \u00b1 0.02.58 \u00b1 0.12.65 \u00b1 0.02.36 \u00b1 0.11.98 \u00b1 0.02.11 \u00b1 0.12.17 \u00b1 0.1-0.06 \u00b1 0.0-0.08 \u00b1 0.1-0.1 \u00b1 0.0-0.03 \u00b1 0.0Reading Speed2.82 \u00b1 0.12.44 \u00b1 0.02.62 \u00b1 0.12.66 \u00b1 0.02.38 \u00b1 0.11.95 \u00b1 0.12.17 \u00b1 0.12.19 \u00b1 0.1-0.07 \u00b1 0.0-0.04 \u00b1 0.0-0.14 \u00b1 0.1-0.03 \u00b1 0.0Text-Only Roberta2.77 \u00b1 0.12.46 \u00b1 0.12.55 \u00b1 0.12.62 \u00b1 0.12.38 \u00b1 0.11.99 \u00b1 0.02.17 \u00b1 0.12.2 \u00b1 0.0-0.02 \u00b1 0.0-0.05 \u00b1 0.0-0.07 \u00b1 0.0-0.0 \u00b1 0.0Logistic Regression [meziere2023using]3.02 \u00b1 0.12.49 \u00b1 0.02.71 \u00b1 0.12.78 \u00b1 0.12.58 \u00b1 0.11.98 \u00b1 0.02.3 \u00b1 0.12.3 \u00b1 0.0-0.23 \u00b1 0.1-0.08 \u00b1 0.1-0.22 \u00b1 0.1-0.13 \u00b1 0.1SVM [hollenstein2023zuco]2.97 \u00b1 0.12.31 \u00b1 0.12.63 \u00b1 0.12.68 \u00b1 0.12.47 \u00b1 0.11.71 \u00b1 0.02.1 \u00b1 0.12.12 \u00b1 0.1-0.19 \u00b1 0.10.05 \u00b1 0.1-0.15 \u00b1 0.1-0.05 \u00b1 0.0Random Forest [makowski2024detection]2.86 \u00b1 0.11.82 \u00b1 0.12.78 \u00b1 0.22.52 \u00b1 0.12.34 \u00b1 0.11.38 \u00b1 0.12.25 \u00b1 0.21.97 \u00b1 0.1-0.1 \u00b1 0.00.41 \u00b1 0.1-0.29 \u00b1 0.10.07 \u00b1 0.0AhnRNN [ahn2020towards]2.76 \u00b1 0.12.46 \u00b1 0.12.55 \u00b1 0.12.62 \u00b1 0.12.37 \u00b1 0.12.01 \u00b1 0.12.16 \u00b1 0.12.2 \u00b1 0.0-0.01 \u00b1 0.0-0.05 \u00b1 0.0-0.08 \u00b1 0.0-0.0 \u00b1 0.0AhnCNN [ahn2020towards]2.76 \u00b1 0.12.46 \u00b1 0.12.55 \u00b1 0.12.62 \u00b1 0.12.38 \u00b1 0.12.0 \u00b1 0.12.16 \u00b1 0.12.2 \u00b1 0.0-0.01 \u00b1 0.0-0.05 \u00b1 0.0-0.07 \u00b1 0.00.0 \u00b1 0.0BEyeLSTM [reich_inferring_2022]2.76 \u00b1 0.12.45 \u00b1 0.12.56 \u00b1 0.12.62 \u00b1 0.12.38 \u00b1 0.12.0 \u00b1 0.12.17 \u00b1 0.12.2 \u00b1 0.1-0.01 \u00b1 0.0-0.05 \u00b1 0.0-0.08 \u00b1 0.1-0.0 \u00b1 0.0PLM-AS [Yang2023PLMASPL]2.76 \u00b1 0.12.46 \u00b1 0.12.54 \u00b1 0.12.62 \u00b1 0.12.37 \u00b1 0.12.0 \u00b1 0.12.13 \u00b1 0.12.19 \u00b1 0.0-0.02 \u00b1 0.0-0.05 \u00b1 0.0-0.07 \u00b1 0.0-0.0 \u00b1 0.0PLM-AS-RM [haller2022eye]2.79 \u00b1 0.12.47 \u00b1 0.12.63 \u00b1 0.12.65 \u00b1 0.02.39 \u00b1 0.12.0 \u00b1 0.12.22 \u00b1 0.12.22 \u00b1 0.1-0.04 \u00b1 0.0-0.06 \u00b1 0.0-0.15 \u00b1 0.1-0.03 \u00b1 0.0RoBERTEye-W [Shubi2024Finegrained]2.77 \u00b1 0.12.44 \u00b1 0.02.55 \u00b1 0.12.61 \u00b1 0.12.37 \u00b1 0.11.98 \u00b1 0.02.15 \u00b1 0.12.18 \u00b1 0.0-0.02 \u00b1 0.0-0.04 \u00b1 0.0-0.07 \u00b1 0.00.0 \u00b1 0.0RoBERTEye-F [Shubi2024Finegrained]2.77 \u00b1 0.12.45 \u00b1 0.02.54 \u00b1 0.12.62 \u00b1 0.12.4 \u00b1 0.11.98 \u00b1 0.02.15 \u00b1 0.12.2 \u00b1 0.0-0.03 \u00b1 0.0-0.05 \u00b1 0.0-0.07 \u00b1 0.1-0.0 \u00b1 0.0MAG-Eye [Shubi2024Finegrained]2.76 \u00b1 0.12.46 \u00b1 0.12.55 \u00b1 0.12.62 \u00b1 0.12.37 \u00b1 0.11.99 \u00b1 0.12.18 \u00b1 0.12.2 \u00b1 0.0-0.01 \u00b1 0.0-0.05 \u00b1 0.0-0.08 \u00b1 0.00.0 \u00b1 0.0PostFusion-Eye [Shubi2024Finegrained]2.86 \u00b1 0.22.52 \u00b1 0.12.66 \u00b1 0.12.71 \u00b1 0.12.49 \u00b1 0.12.06 \u00b1 0.12.34 \u00b1 0.12.3 \u00b1 0.1-0.08 \u00b1 0.0-0.11 \u00b1 0.1-0.17 \u00b1 0.1-0.07 \u00b1 0.0"},{"location":"results/copco_typ/","title":"CopCo TYP","text":"<p>Dyslexia Detection (CopCo)</p> <p></p>"},{"location":"results/copco_typ/#test","title":"Test","text":"ModelUnseen Reader Balanced AccuracyUnseen Text Balanced AccuracyUnseen Text and Reader Balanced AccuracyAverage Balanced AccuracyUnseen Reader AUROCUnseen Text AUROCUnseen Text and Reader AUROCAverage AUROCMajority Class / Chance50.3 \u00b1 0.349.6 \u00b1 0.450.1 \u00b1 0.149.9 \u00b1 0.050.3 \u00b1 0.349.6 \u00b1 0.450.1 \u00b1 0.149.9 \u00b1 0.0Reading Speed57.7 \u00b1 2.254.9 \u00b1 1.850.6 \u00b1 2.755.1 \u00b1 2.160.7 \u00b1 2.056.2 \u00b1 2.050.9 \u00b1 4.856.6 \u00b1 2.1Text-Only Roberta50.0 \u00b1 0.050.0 \u00b1 0.050.0 \u00b1 0.050.0 \u00b1 0.047.0 \u00b1 4.450.0 \u00b1 0.250.4 \u00b1 1.150.1 \u00b1 0.4Logistic Regression [meziere2023using]75.5 \u00b1 3.176.6 \u00b1 1.663.5 \u00b1 5.173.8 \u00b1 2.183.1 \u00b1 3.183.3 \u00b1 1.668.9 \u00b1 6.680.6 \u00b1 2.3SVM [hollenstein2023zuco]70.7 \u00b1 2.477.4 \u00b1 1.764.7 \u00b1 3.172.5 \u00b1 1.670.7 \u00b1 2.477.4 \u00b1 1.764.7 \u00b1 3.172.5 \u00b1 1.6Random Forest [makowski2024detection]69.8 \u00b1 4.281.5 \u00b1 2.259.7 \u00b1 4.972.7 \u00b1 1.980.1 \u00b1 3.691.5 \u00b1 1.565.9 \u00b1 6.682.9 \u00b1 1.5AhnRNN [ahn2020towards]50.0 \u00b1 0.050.0 \u00b1 0.050.0 \u00b1 0.050.0 \u00b1 0.050.1 \u00b1 0.150.0 \u00b1 0.050.0 \u00b1 0.150.0 \u00b1 0.1AhnCNN [ahn2020towards]77.7 \u00b1 1.877.5 \u00b1 2.765.6 \u00b1 2.475.0 \u00b1 0.885.3 \u00b1 1.685.7 \u00b1 2.374.9 \u00b1 2.883.4 \u00b1 1.1BEyeLSTM [reich_inferring_2022]71.9 \u00b1 2.176.8 \u00b1 1.764.7 \u00b1 5.072.9 \u00b1 1.379.4 \u00b1 2.885.0 \u00b1 1.369.2 \u00b1 6.280.2 \u00b1 1.5PLM-AS [Yang2023PLMASPL]55.2 \u00b1 4.357.3 \u00b1 3.455.9 \u00b1 2.256.0 \u00b1 3.257.6 \u00b1 5.958.5 \u00b1 4.959.4 \u00b1 0.957.9 \u00b1 4.6PLM-AS-RM [haller2022eye]60.9 \u00b1 2.871.6 \u00b1 2.454.6 \u00b1 1.163.7 \u00b1 0.963.9 \u00b1 4.380.1 \u00b1 1.955.0 \u00b1 1.969.2 \u00b1 1.0RoBERTEye-W [Shubi2024Finegrained]70.0 \u00b1 4.068.5 \u00b1 3.061.9 \u00b1 4.667.4 \u00b1 2.978.3 \u00b1 3.276.7 \u00b1 2.968.2 \u00b1 5.375.6 \u00b1 2.3RoBERTEye-F [Shubi2024Finegrained]60.6 \u00b1 2.160.3 \u00b1 2.454.0 \u00b1 2.158.9 \u00b1 1.471.9 \u00b1 4.274.7 \u00b1 1.363.3 \u00b1 2.370.8 \u00b1 1.9MAG-Eye [Shubi2024Finegrained]47.2 \u00b1 2.449.7 \u00b1 0.251.4 \u00b1 1.250.3 \u00b1 0.345.9 \u00b1 7.554.7 \u00b1 4.156.1 \u00b1 1.053.3 \u00b1 3.7PostFusion-Eye [Shubi2024Finegrained]64.7 \u00b1 4.368.9 \u00b1 2.357.0 \u00b1 3.764.4 \u00b1 2.473.1 \u00b1 4.078.1 \u00b1 1.665.5 \u00b1 3.473.2 \u00b1 1.5"},{"location":"results/copco_typ/#validation","title":"Validation","text":"ModelUnseen Reader Balanced AccuracyUnseen Text Balanced AccuracyUnseen Text and Reader Balanced AccuracyAverage Balanced AccuracyUnseen Reader AUROCUnseen Text AUROCUnseen Text and Reader AUROCAverage AUROCMajority Class / Chance50.9 \u00b1 0.850.9 \u00b1 0.850.0 \u00b1 0.050.5 \u00b1 0.450.9 \u00b1 0.850.9 \u00b1 0.850.0 \u00b1 0.050.5 \u00b1 0.4Reading Speed57.9 \u00b1 4.155.1 \u00b1 2.252.1 \u00b1 2.155.1 \u00b1 2.560.4 \u00b1 4.557.7 \u00b1 4.650.9 \u00b1 4.856.3 \u00b1 2.6Text-Only Roberta50.0 \u00b1 0.050.0 \u00b1 0.050.0 \u00b1 0.050.0 \u00b1 0.053.5 \u00b1 2.551.6 \u00b1 1.149.4 \u00b1 0.552.4 \u00b1 1.7Logistic Regression [meziere2023using]72.1 \u00b1 2.979.2 \u00b1 3.364.1 \u00b1 3.372.4 \u00b1 1.578.3 \u00b1 2.485.3 \u00b1 3.569.7 \u00b1 5.678.7 \u00b1 2.0SVM [hollenstein2023zuco]72.8 \u00b1 2.283.2 \u00b1 2.667.0 \u00b1 3.174.8 \u00b1 1.072.8 \u00b1 2.283.2 \u00b1 2.667.0 \u00b1 3.174.8 \u00b1 1.0Random Forest [makowski2024detection]72.7 \u00b1 4.086.9 \u00b1 2.266.0 \u00b1 2.775.2 \u00b1 1.881.5 \u00b1 3.895.2 \u00b1 1.273.1 \u00b1 3.683.9 \u00b1 2.3AhnRNN [ahn2020towards]50.0 \u00b1 0.050.0 \u00b1 0.050.0 \u00b1 0.050.0 \u00b1 0.049.9 \u00b1 0.149.9 \u00b1 0.149.9 \u00b1 0.049.9 \u00b1 0.0AhnCNN [ahn2020towards]73.9 \u00b1 3.077.5 \u00b1 2.668.3 \u00b1 1.173.5 \u00b1 1.682.0 \u00b1 1.886.5 \u00b1 3.174.9 \u00b1 1.781.3 \u00b1 1.5BEyeLSTM [reich_inferring_2022]72.6 \u00b1 2.276.7 \u00b1 3.268.3 \u00b1 3.773.1 \u00b1 2.180.3 \u00b1 2.585.0 \u00b1 3.774.0 \u00b1 4.080.4 \u00b1 2.6PLM-AS [Yang2023PLMASPL]52.5 \u00b1 2.061.6 \u00b1 3.353.3 \u00b1 1.255.8 \u00b1 2.254.2 \u00b1 3.666.4 \u00b1 4.555.4 \u00b1 2.158.8 \u00b1 2.3PLM-AS-RM [haller2022eye]63.9 \u00b1 4.668.9 \u00b1 5.057.1 \u00b1 3.862.8 \u00b1 2.165.9 \u00b1 5.774.2 \u00b1 6.462.1 \u00b1 4.267.2 \u00b1 3.0RoBERTEye-W [Shubi2024Finegrained]62.4 \u00b1 3.369.8 \u00b1 3.859.6 \u00b1 2.463.5 \u00b1 1.771.2 \u00b1 3.277.6 \u00b1 4.267.5 \u00b1 3.271.6 \u00b1 1.8RoBERTEye-F [Shubi2024Finegrained]57.0 \u00b1 1.561.2 \u00b1 3.255.2 \u00b1 1.857.6 \u00b1 1.770.5 \u00b1 3.175.5 \u00b1 3.465.7 \u00b1 2.269.9 \u00b1 1.8MAG-Eye [Shubi2024Finegrained]51.2 \u00b1 1.052.8 \u00b1 2.549.8 \u00b1 0.251.7 \u00b1 1.560.5 \u00b1 1.962.0 \u00b1 5.959.4 \u00b1 4.060.6 \u00b1 4.0PostFusion-Eye [Shubi2024Finegrained]65.6 \u00b1 2.372.0 \u00b1 4.260.6 \u00b1 2.865.9 \u00b1 2.275.7 \u00b1 2.079.3 \u00b1 4.070.7 \u00b1 2.774.1 \u00b1 1.8"},{"location":"results/iitbhgc_cv/","title":"IITBHGC CV","text":"<p>Claim Verification (IITBHGC)</p> <p></p>"},{"location":"results/iitbhgc_cv/#test","title":"Test","text":"ModelUnseen Reader Balanced AccuracyUnseen Text Balanced AccuracyUnseen Text and Reader Balanced AccuracyAverage Balanced AccuracyUnseen Reader AUROCUnseen Text AUROCUnseen Text and Reader AUROCAverage AUROCMajority Class / Chance49.5 \u00b1 0.451.0 \u00b1 0.950.6 \u00b1 0.650.4 \u00b1 0.349.5 \u00b1 0.451.0 \u00b1 0.950.6 \u00b1 0.650.4 \u00b1 0.3Reading Speed55.6 \u00b1 0.455.5 \u00b1 1.055.8 \u00b1 1.455.6 \u00b1 0.656.5 \u00b1 1.156.9 \u00b1 1.458.0 \u00b1 1.357.3 \u00b1 0.7Text-Only Roberta55.9 \u00b1 2.649.8 \u00b1 0.349.7 \u00b1 0.752.5 \u00b1 1.462.5 \u00b1 2.755.1 \u00b1 1.256.1 \u00b1 5.458.8 \u00b1 1.5Logistic Regression [meziere2023using]53.3 \u00b1 1.154.2 \u00b1 1.555.7 \u00b1 0.853.9 \u00b1 1.053.2 \u00b1 0.955.5 \u00b1 1.655.6 \u00b1 0.754.6 \u00b1 1.1SVM [hollenstein2023zuco]52.9 \u00b1 0.855.0 \u00b1 1.457.8 \u00b1 1.454.5 \u00b1 0.652.9 \u00b1 0.855.0 \u00b1 1.457.8 \u00b1 1.454.5 \u00b1 0.6Random Forest [makowski2024detection]56.0 \u00b1 1.652.2 \u00b1 0.551.6 \u00b1 2.153.4 \u00b1 0.659.6 \u00b1 0.854.0 \u00b1 0.455.7 \u00b1 1.456.4 \u00b1 0.3AhnRNN [ahn2020towards]50.0 \u00b1 0.050.0 \u00b1 0.050.0 \u00b1 0.050.0 \u00b1 0.051.2 \u00b1 1.050.7 \u00b1 0.550.6 \u00b1 0.550.9 \u00b1 0.7AhnCNN [ahn2020towards]50.9 \u00b1 1.051.4 \u00b1 1.155.4 \u00b1 2.351.8 \u00b1 0.951.4 \u00b1 1.252.9 \u00b1 1.855.5 \u00b1 2.252.9 \u00b1 1.0BEyeLSTM [reich_inferring_2022]51.6 \u00b1 1.849.0 \u00b1 1.451.7 \u00b1 0.950.2 \u00b1 1.153.3 \u00b1 2.348.9 \u00b1 1.453.1 \u00b1 1.251.3 \u00b1 1.2PLM-AS [Yang2023PLMASPL]53.3 \u00b1 2.348.8 \u00b1 0.947.5 \u00b1 2.050.6 \u00b1 0.953.0 \u00b1 2.450.4 \u00b1 0.750.3 \u00b1 4.251.4 \u00b1 0.6PLM-AS-RM [haller2022eye]52.6 \u00b1 1.250.6 \u00b1 0.850.6 \u00b1 1.851.3 \u00b1 0.855.6 \u00b1 2.651.7 \u00b1 1.151.3 \u00b1 1.753.4 \u00b1 1.5RoBERTEye-W [Shubi2024Finegrained]55.9 \u00b1 3.050.5 \u00b1 1.153.1 \u00b1 1.753.4 \u00b1 2.164.3 \u00b1 1.553.0 \u00b1 2.054.5 \u00b1 3.258.0 \u00b1 2.2RoBERTEye-F [Shubi2024Finegrained]53.4 \u00b1 2.949.4 \u00b1 0.649.8 \u00b1 0.250.9 \u00b1 0.762.2 \u00b1 2.354.9 \u00b1 0.658.8 \u00b1 2.558.4 \u00b1 1.1MAG-Eye [Shubi2024Finegrained]57.1 \u00b1 2.949.7 \u00b1 1.250.8 \u00b1 2.352.8 \u00b1 1.565.3 \u00b1 1.951.7 \u00b1 2.455.2 \u00b1 4.458.0 \u00b1 2.0PostFusion-Eye [Shubi2024Finegrained]51.9 \u00b1 0.450.9 \u00b1 0.152.6 \u00b1 1.451.5 \u00b1 0.360.2 \u00b1 1.956.3 \u00b1 2.659.0 \u00b1 0.957.5 \u00b1 1.4"},{"location":"results/iitbhgc_cv/#validation","title":"Validation","text":"ModelUnseen Reader Balanced AccuracyUnseen Text Balanced AccuracyUnseen Text and Reader Balanced AccuracyAverage Balanced AccuracyUnseen Reader AUROCUnseen Text AUROCUnseen Text and Reader AUROCAverage AUROCMajority Class / Chance50.6 \u00b1 0.551.4 \u00b1 1.251.3 \u00b1 1.151.0 \u00b1 0.850.6 \u00b1 0.551.4 \u00b1 1.251.3 \u00b1 1.151.0 \u00b1 0.8Reading Speed56.1 \u00b1 0.856.1 \u00b1 0.956.5 \u00b1 1.556.2 \u00b1 0.456.9 \u00b1 1.556.0 \u00b1 1.058.0 \u00b1 1.357.0 \u00b1 0.7Text-Only Roberta60.0 \u00b1 4.051.1 \u00b1 1.248.9 \u00b1 1.253.9 \u00b1 1.866.1 \u00b1 3.554.7 \u00b1 1.355.2 \u00b1 2.059.2 \u00b1 1.9Logistic Regression [meziere2023using]54.1 \u00b1 1.554.4 \u00b1 1.255.9 \u00b1 2.254.6 \u00b1 1.254.8 \u00b1 1.856.2 \u00b1 0.555.2 \u00b1 2.255.2 \u00b1 1.4SVM [hollenstein2023zuco]57.6 \u00b1 1.355.6 \u00b1 1.254.6 \u00b1 1.356.1 \u00b1 0.657.6 \u00b1 1.355.6 \u00b1 1.254.6 \u00b1 1.356.1 \u00b1 0.6Random Forest [makowski2024detection]60.0 \u00b1 1.256.5 \u00b1 0.855.3 \u00b1 1.857.5 \u00b1 0.863.7 \u00b1 1.660.6 \u00b1 2.057.1 \u00b1 2.161.2 \u00b1 1.4AhnRNN [ahn2020towards]50.0 \u00b1 0.050.0 \u00b1 0.050.0 \u00b1 0.050.0 \u00b1 0.050.9 \u00b1 0.750.9 \u00b1 0.650.1 \u00b1 0.150.8 \u00b1 0.6AhnCNN [ahn2020towards]53.7 \u00b1 1.552.2 \u00b1 0.353.7 \u00b1 1.053.2 \u00b1 0.657.5 \u00b1 0.455.4 \u00b1 0.755.6 \u00b1 1.456.1 \u00b1 0.3BEyeLSTM [reich_inferring_2022]56.6 \u00b1 0.952.9 \u00b1 1.552.2 \u00b1 0.454.0 \u00b1 0.961.8 \u00b1 0.755.3 \u00b1 1.154.9 \u00b1 0.957.5 \u00b1 0.7PLM-AS [Yang2023PLMASPL]52.6 \u00b1 1.153.6 \u00b1 1.851.7 \u00b1 2.453.1 \u00b1 1.054.0 \u00b1 1.856.7 \u00b1 3.451.8 \u00b1 2.254.6 \u00b1 2.2PLM-AS-RM [haller2022eye]57.1 \u00b1 2.950.7 \u00b1 0.850.0 \u00b1 1.053.2 \u00b1 1.564.1 \u00b1 3.350.6 \u00b1 0.651.0 \u00b1 2.856.1 \u00b1 1.2RoBERTEye-W [Shubi2024Finegrained]58.7 \u00b1 3.654.0 \u00b1 2.251.3 \u00b1 1.355.5 \u00b1 2.671.4 \u00b1 2.359.4 \u00b1 1.856.2 \u00b1 0.963.9 \u00b1 1.9RoBERTEye-F [Shubi2024Finegrained]55.2 \u00b1 4.249.7 \u00b1 0.350.6 \u00b1 0.651.6 \u00b1 1.365.8 \u00b1 3.155.8 \u00b1 2.456.0 \u00b1 2.959.5 \u00b1 0.4MAG-Eye [Shubi2024Finegrained]62.3 \u00b1 4.252.0 \u00b1 1.250.7 \u00b1 1.655.7 \u00b1 1.972.2 \u00b1 1.456.0 \u00b1 0.955.3 \u00b1 2.062.9 \u00b1 1.1PostFusion-Eye [Shubi2024Finegrained]51.9 \u00b1 0.750.9 \u00b1 0.853.4 \u00b1 0.751.6 \u00b1 0.559.3 \u00b1 1.156.7 \u00b1 2.060.0 \u00b1 1.257.7 \u00b1 0.6"},{"location":"results/mecol2_lex/","title":"MECOL2 LEX","text":"<p>Vocabulary Knowledge (MECOL2)</p> <p></p>"},{"location":"results/mecol2_lex/#test","title":"Test","text":"ModelUnseen Reader RMSEUnseen Text RMSEUnseen Text and Reader RMSEAverage RMSEUnseen Reader MAEUnseen Text MAEUnseen Text and Reader MAEAverage MAEUnseen Reader R\u00b2Unseen Text R\u00b2Unseen Text and Reader R\u00b2Average R\u00b2Majority Class / Chance12.47 \u00b1 0.112.46 \u00b1 0.012.39 \u00b1 0.112.45 \u00b1 0.110.41 \u00b1 0.110.4 \u00b1 0.010.32 \u00b1 0.110.4 \u00b1 0.1-0.0 \u00b1 0.0-0.0 \u00b1 0.0-0.0 \u00b1 0.0-0.0 \u00b1 0.0Reading Speed28.98 \u00b1 14.328.92 \u00b1 14.328.97 \u00b1 14.328.95 \u00b1 14.327.18 \u00b1 14.527.14 \u00b1 14.527.14 \u00b1 14.627.15 \u00b1 14.5-9.86 \u00b1 8.5-9.58 \u00b1 8.3-10.06 \u00b1 8.7-9.76 \u00b1 8.5Text-Only Roberta12.47 \u00b1 0.112.46 \u00b1 0.012.39 \u00b1 0.112.45 \u00b1 0.110.41 \u00b1 0.110.41 \u00b1 0.010.32 \u00b1 0.110.4 \u00b1 0.1-0.0 \u00b1 0.0-0.0 \u00b1 0.0-0.0 \u00b1 0.0-0.0 \u00b1 0.0Logistic Regression [meziere2023using]10.37 \u00b1 0.110.31 \u00b1 0.110.32 \u00b1 0.210.34 \u00b1 0.08.35 \u00b1 0.18.31 \u00b1 0.18.31 \u00b1 0.28.32 \u00b1 0.00.31 \u00b1 0.00.31 \u00b1 0.00.3 \u00b1 0.00.31 \u00b1 0.0SVM [hollenstein2023zuco]10.98 \u00b1 0.110.94 \u00b1 0.110.84 \u00b1 0.110.94 \u00b1 0.08.78 \u00b1 0.08.76 \u00b1 0.08.68 \u00b1 0.18.76 \u00b1 0.00.22 \u00b1 0.00.23 \u00b1 0.00.23 \u00b1 0.00.23 \u00b1 0.0Random Forest [makowski2024detection]10.2 \u00b1 0.19.93 \u00b1 0.010.24 \u00b1 0.210.09 \u00b1 0.08.2 \u00b1 0.17.95 \u00b1 0.08.26 \u00b1 0.28.1 \u00b1 0.00.33 \u00b1 0.00.36 \u00b1 0.00.31 \u00b1 0.00.34 \u00b1 0.0AhnRNN [ahn2020towards]12.48 \u00b1 0.112.46 \u00b1 0.012.4 \u00b1 0.212.46 \u00b1 0.110.42 \u00b1 0.110.4 \u00b1 0.010.32 \u00b1 0.110.4 \u00b1 0.1-0.0 \u00b1 0.00.0 \u00b1 0.0-0.01 \u00b1 0.0-0.0 \u00b1 0.0AhnCNN [ahn2020towards]12.28 \u00b1 0.112.3 \u00b1 0.012.2 \u00b1 0.112.27 \u00b1 0.010.25 \u00b1 0.110.27 \u00b1 0.010.16 \u00b1 0.110.24 \u00b1 0.00.03 \u00b1 0.00.03 \u00b1 0.00.03 \u00b1 0.00.03 \u00b1 0.0BEyeLSTM [reich_inferring_2022]12.65 \u00b1 0.312.42 \u00b1 0.113.42 \u00b1 0.912.68 \u00b1 0.310.38 \u00b1 0.210.33 \u00b1 0.110.41 \u00b1 0.310.36 \u00b1 0.1-0.03 \u00b1 0.00.01 \u00b1 0.0-0.19 \u00b1 0.1-0.04 \u00b1 0.0PLM-AS [Yang2023PLMASPL]12.48 \u00b1 0.112.46 \u00b1 0.012.4 \u00b1 0.212.46 \u00b1 0.110.42 \u00b1 0.110.41 \u00b1 0.010.32 \u00b1 0.110.4 \u00b1 0.1-0.0 \u00b1 0.0-0.0 \u00b1 0.0-0.01 \u00b1 0.0-0.0 \u00b1 0.0PLM-AS-RM [haller2022eye]31.8 \u00b1 0.331.81 \u00b1 0.131.69 \u00b1 0.431.79 \u00b1 0.229.28 \u00b1 0.329.28 \u00b1 0.229.2 \u00b1 0.429.27 \u00b1 0.2-5.53 \u00b1 0.1-5.52 \u00b1 0.1-5.57 \u00b1 0.2-5.52 \u00b1 0.1RoBERTEye-W [Shubi2024Finegrained]11.11 \u00b1 0.211.2 \u00b1 0.111.16 \u00b1 0.211.16 \u00b1 0.19.15 \u00b1 0.19.21 \u00b1 0.19.2 \u00b1 0.29.18 \u00b1 0.00.2 \u00b1 0.00.19 \u00b1 0.00.19 \u00b1 0.00.2 \u00b1 0.0RoBERTEye-F [Shubi2024Finegrained]11.28 \u00b1 0.311.18 \u00b1 0.211.32 \u00b1 0.311.25 \u00b1 0.29.28 \u00b1 0.29.19 \u00b1 0.19.32 \u00b1 0.39.25 \u00b1 0.10.18 \u00b1 0.00.19 \u00b1 0.00.16 \u00b1 0.00.18 \u00b1 0.0MAG-Eye [Shubi2024Finegrained]12.47 \u00b1 0.112.46 \u00b1 0.012.39 \u00b1 0.112.46 \u00b1 0.110.42 \u00b1 0.110.41 \u00b1 0.010.32 \u00b1 0.110.4 \u00b1 0.1-0.0 \u00b1 0.0-0.0 \u00b1 0.0-0.0 \u00b1 0.0-0.0 \u00b1 0.0PostFusion-Eye [Shubi2024Finegrained]13.44 \u00b1 0.413.99 \u00b1 0.815.26 \u00b1 1.214.0 \u00b1 0.410.63 \u00b1 0.311.17 \u00b1 0.711.92 \u00b1 1.011.04 \u00b1 0.4-0.17 \u00b1 0.0-0.28 \u00b1 0.1-0.54 \u00b1 0.2-0.27 \u00b1 0.1"},{"location":"results/mecol2_lex/#validation","title":"Validation","text":"ModelUnseen Reader RMSEUnseen Text RMSEUnseen Text and Reader RMSEAverage RMSEUnseen Reader MAEUnseen Text MAEUnseen Text and Reader MAEAverage MAEUnseen Reader R\u00b2Unseen Text R\u00b2Unseen Text and Reader R\u00b2Average R\u00b2Majority Class / Chance12.49 \u00b1 0.112.45 \u00b1 0.112.39 \u00b1 0.112.45 \u00b1 0.110.42 \u00b1 0.110.4 \u00b1 0.110.31 \u00b1 0.110.39 \u00b1 0.1-0.0 \u00b1 0.0-0.0 \u00b1 0.0-0.0 \u00b1 0.0-0.0 \u00b1 0.0Reading Speed28.76 \u00b1 14.229.0 \u00b1 14.428.61 \u00b1 14.228.82 \u00b1 14.226.97 \u00b1 14.427.22 \u00b1 14.626.83 \u00b1 14.427.04 \u00b1 14.5-9.09 \u00b1 7.9-9.63 \u00b1 8.3-9.07 \u00b1 7.9-9.28 \u00b1 8.0Text-Only Roberta12.49 \u00b1 0.112.45 \u00b1 0.112.38 \u00b1 0.112.45 \u00b1 0.110.42 \u00b1 0.110.4 \u00b1 0.110.31 \u00b1 0.110.39 \u00b1 0.1-0.0 \u00b1 0.0-0.0 \u00b1 0.0-0.0 \u00b1 0.0-0.0 \u00b1 0.0Logistic Regression [meziere2023using]10.37 \u00b1 0.210.27 \u00b1 0.110.32 \u00b1 0.110.32 \u00b1 0.18.35 \u00b1 0.18.29 \u00b1 0.18.31 \u00b1 0.18.32 \u00b1 0.10.31 \u00b1 0.00.32 \u00b1 0.00.3 \u00b1 0.00.31 \u00b1 0.0SVM [hollenstein2023zuco]10.95 \u00b1 0.110.9 \u00b1 0.010.79 \u00b1 0.110.9 \u00b1 0.18.77 \u00b1 0.18.74 \u00b1 0.08.63 \u00b1 0.18.73 \u00b1 0.00.23 \u00b1 0.00.23 \u00b1 0.00.24 \u00b1 0.00.23 \u00b1 0.0Random Forest [makowski2024detection]10.19 \u00b1 0.19.71 \u00b1 0.010.26 \u00b1 0.110.01 \u00b1 0.18.16 \u00b1 0.17.79 \u00b1 0.08.32 \u00b1 0.28.04 \u00b1 0.00.33 \u00b1 0.00.39 \u00b1 0.00.31 \u00b1 0.00.35 \u00b1 0.0AhnRNN [ahn2020towards]12.48 \u00b1 0.112.45 \u00b1 0.112.37 \u00b1 0.112.45 \u00b1 0.110.41 \u00b1 0.110.41 \u00b1 0.110.3 \u00b1 0.110.39 \u00b1 0.10.0 \u00b1 0.0-0.0 \u00b1 0.00.0 \u00b1 0.00.0 \u00b1 0.0AhnCNN [ahn2020towards]12.33 \u00b1 0.112.27 \u00b1 0.112.21 \u00b1 0.112.28 \u00b1 0.110.28 \u00b1 0.110.25 \u00b1 0.110.17 \u00b1 0.210.25 \u00b1 0.10.02 \u00b1 0.00.03 \u00b1 0.00.02 \u00b1 0.00.03 \u00b1 0.0BEyeLSTM [reich_inferring_2022]12.64 \u00b1 0.412.33 \u00b1 0.112.55 \u00b1 0.412.51 \u00b1 0.310.4 \u00b1 0.210.3 \u00b1 0.110.29 \u00b1 0.210.34 \u00b1 0.1-0.03 \u00b1 0.00.02 \u00b1 0.0-0.03 \u00b1 0.1-0.01 \u00b1 0.0PLM-AS [Yang2023PLMASPL]12.48 \u00b1 0.112.45 \u00b1 0.112.37 \u00b1 0.112.45 \u00b1 0.110.41 \u00b1 0.110.41 \u00b1 0.110.3 \u00b1 0.110.39 \u00b1 0.1-0.0 \u00b1 0.0-0.0 \u00b1 0.0-0.0 \u00b1 0.0-0.0 \u00b1 0.0PLM-AS-RM [haller2022eye]31.81 \u00b1 0.331.78 \u00b1 0.031.7 \u00b1 0.431.78 \u00b1 0.229.27 \u00b1 0.329.25 \u00b1 0.129.2 \u00b1 0.429.25 \u00b1 0.2-5.5 \u00b1 0.2-5.52 \u00b1 0.1-5.58 \u00b1 0.2-5.52 \u00b1 0.1RoBERTEye-W [Shubi2024Finegrained]11.05 \u00b1 0.111.11 \u00b1 0.111.16 \u00b1 0.111.1 \u00b1 0.09.1 \u00b1 0.09.11 \u00b1 0.19.22 \u00b1 0.29.13 \u00b1 0.00.22 \u00b1 0.00.2 \u00b1 0.00.18 \u00b1 0.00.21 \u00b1 0.0RoBERTEye-F [Shubi2024Finegrained]11.17 \u00b1 0.110.98 \u00b1 0.011.03 \u00b1 0.111.07 \u00b1 0.09.2 \u00b1 0.19.04 \u00b1 0.09.11 \u00b1 0.19.12 \u00b1 0.00.2 \u00b1 0.00.22 \u00b1 0.00.2 \u00b1 0.00.21 \u00b1 0.0MAG-Eye [Shubi2024Finegrained]12.49 \u00b1 0.112.45 \u00b1 0.112.38 \u00b1 0.112.45 \u00b1 0.110.42 \u00b1 0.110.4 \u00b1 0.110.31 \u00b1 0.110.39 \u00b1 0.1-0.0 \u00b1 0.0-0.0 \u00b1 0.0-0.0 \u00b1 0.0-0.0 \u00b1 0.0PostFusion-Eye [Shubi2024Finegrained]13.01 \u00b1 0.413.72 \u00b1 0.913.15 \u00b1 0.313.35 \u00b1 0.510.37 \u00b1 0.311.0 \u00b1 0.810.2 \u00b1 0.110.59 \u00b1 0.4-0.09 \u00b1 0.1-0.23 \u00b1 0.2-0.13 \u00b1 0.1-0.16 \u00b1 0.1"},{"location":"results/onestop_rc/","title":"OneStop RC","text":"<p>Reading Comprehension (OneStop)</p> <p></p>"},{"location":"results/onestop_rc/#test","title":"Test","text":"ModelUnseen Reader Balanced AccuracyUnseen Text Balanced AccuracyUnseen Text and Reader Balanced AccuracyAverage Balanced AccuracyUnseen Reader AUROCUnseen Text AUROCUnseen Text and Reader AUROCAverage AUROCMajority Class / Chance50.0 \u00b1 0.050.0 \u00b1 0.050.0 \u00b1 0.050.0 \u00b1 0.050.0 \u00b1 0.050.0 \u00b1 0.050.0 \u00b1 0.050.0 \u00b1 0.0Reading Speed50.6 \u00b1 1.249.4 \u00b1 0.848.0 \u00b1 1.549.9 \u00b1 0.649.8 \u00b1 1.749.3 \u00b1 0.947.7 \u00b1 2.249.6 \u00b1 0.8Text-Only Roberta58.6 \u00b1 1.751.4 \u00b1 0.553.7 \u00b1 1.655.0 \u00b1 1.066.3 \u00b1 1.155.2 \u00b1 1.555.0 \u00b1 2.861.1 \u00b1 1.0Logistic Regression [meziere2023using]51.1 \u00b1 1.252.2 \u00b1 0.452.4 \u00b1 2.551.7 \u00b1 0.752.2 \u00b1 1.553.4 \u00b1 0.954.3 \u00b1 3.253.0 \u00b1 0.8SVM [hollenstein2023zuco]50.0 \u00b1 0.951.6 \u00b1 0.650.8 \u00b1 1.550.7 \u00b1 0.750.0 \u00b1 0.951.6 \u00b1 0.650.8 \u00b1 1.550.7 \u00b1 0.7Random Forest [makowski2024detection]56.2 \u00b1 0.853.5 \u00b1 1.054.7 \u00b1 1.555.1 \u00b1 0.559.4 \u00b1 0.956.0 \u00b1 1.357.2 \u00b1 1.958.0 \u00b1 0.6AhnRNN [ahn2020towards]50.0 \u00b1 0.050.0 \u00b1 0.050.0 \u00b1 0.050.0 \u00b1 0.050.0 \u00b1 0.050.0 \u00b1 0.050.0 \u00b1 0.050.0 \u00b1 0.0AhnCNN [ahn2020towards]50.1 \u00b1 0.150.1 \u00b1 0.149.7 \u00b1 0.950.0 \u00b1 0.048.3 \u00b1 1.551.7 \u00b1 0.548.1 \u00b1 3.149.7 \u00b1 0.7BEyeLSTM [reich_inferring_2022]53.0 \u00b1 0.650.0 \u00b1 0.751.6 \u00b1 1.351.5 \u00b1 0.554.8 \u00b1 1.150.3 \u00b1 1.251.0 \u00b1 2.552.5 \u00b1 0.8PLM-AS [Yang2023PLMASPL]56.0 \u00b1 0.950.9 \u00b1 0.853.8 \u00b1 2.253.5 \u00b1 0.859.6 \u00b1 0.952.1 \u00b1 1.055.9 \u00b1 1.956.1 \u00b1 0.9PLM-AS-RM [haller2022eye]58.0 \u00b1 0.752.5 \u00b1 0.856.2 \u00b1 2.355.2 \u00b1 0.462.0 \u00b1 0.854.1 \u00b1 1.358.6 \u00b1 2.158.4 \u00b1 0.5RoBERTEye-W [Shubi2024Finegrained]58.4 \u00b1 1.851.1 \u00b1 0.854.1 \u00b1 2.254.7 \u00b1 1.066.5 \u00b1 1.554.7 \u00b1 1.557.2 \u00b1 2.761.4 \u00b1 0.9RoBERTEye-F [Shubi2024Finegrained]56.6 \u00b1 1.350.7 \u00b1 0.452.0 \u00b1 1.553.6 \u00b1 0.967.3 \u00b1 1.255.7 \u00b1 1.154.8 \u00b1 3.461.9 \u00b1 0.8MAG-Eye [Shubi2024Finegrained]58.3 \u00b1 1.750.9 \u00b1 0.450.1 \u00b1 0.554.3 \u00b1 0.967.7 \u00b1 1.057.7 \u00b1 0.557.8 \u00b1 2.262.9 \u00b1 0.5PostFusion-Eye [Shubi2024Finegrained]57.0 \u00b1 1.552.2 \u00b1 0.752.2 \u00b1 1.154.7 \u00b1 0.964.5 \u00b1 0.957.1 \u00b1 1.154.7 \u00b1 3.261.1 \u00b1 0.6"},{"location":"results/onestop_rc/#validation","title":"Validation","text":"ModelUnseen Reader Balanced AccuracyUnseen Text Balanced AccuracyUnseen Text and Reader Balanced AccuracyAverage Balanced AccuracyUnseen Reader AUROCUnseen Text AUROCUnseen Text and Reader AUROCAverage AUROCMajority Class / Chance50.0 \u00b1 0.050.0 \u00b1 0.050.0 \u00b1 0.050.0 \u00b1 0.050.0 \u00b1 0.050.0 \u00b1 0.050.0 \u00b1 0.050.0 \u00b1 0.0Reading Speed48.6 \u00b1 1.350.0 \u00b1 0.949.4 \u00b1 1.549.4 \u00b1 0.748.4 \u00b1 1.849.8 \u00b1 1.050.6 \u00b1 2.349.2 \u00b1 0.8Text-Only Roberta59.2 \u00b1 1.852.2 \u00b1 0.853.0 \u00b1 1.955.6 \u00b1 1.367.4 \u00b1 1.756.8 \u00b1 1.258.9 \u00b1 2.462.5 \u00b1 1.2Logistic Regression [meziere2023using]52.1 \u00b1 1.252.9 \u00b1 0.554.0 \u00b1 2.152.7 \u00b1 0.653.0 \u00b1 1.553.8 \u00b1 0.853.1 \u00b1 3.053.6 \u00b1 0.7SVM [hollenstein2023zuco]50.9 \u00b1 0.552.8 \u00b1 0.852.8 \u00b1 1.551.9 \u00b1 0.550.9 \u00b1 0.552.8 \u00b1 0.852.8 \u00b1 1.551.9 \u00b1 0.5Random Forest [makowski2024detection]59.2 \u00b1 0.854.6 \u00b1 1.153.8 \u00b1 1.656.9 \u00b1 0.461.4 \u00b1 0.856.8 \u00b1 1.356.6 \u00b1 1.759.3 \u00b1 0.5AhnRNN [ahn2020towards]50.0 \u00b1 0.050.0 \u00b1 0.050.0 \u00b1 0.050.0 \u00b1 0.050.0 \u00b1 0.050.0 \u00b1 0.050.0 \u00b1 0.050.0 \u00b1 0.0AhnCNN [ahn2020towards]50.5 \u00b1 0.550.2 \u00b1 0.150.2 \u00b1 0.250.3 \u00b1 0.352.3 \u00b1 0.751.3 \u00b1 0.950.5 \u00b1 2.051.8 \u00b1 0.5BEyeLSTM [reich_inferring_2022]53.7 \u00b1 0.652.0 \u00b1 0.952.3 \u00b1 1.752.8 \u00b1 0.556.8 \u00b1 1.054.9 \u00b1 1.554.9 \u00b1 2.355.5 \u00b1 1.0PLM-AS [Yang2023PLMASPL]57.5 \u00b1 1.252.0 \u00b1 0.953.3 \u00b1 1.254.8 \u00b1 1.062.0 \u00b1 1.254.0 \u00b1 1.354.6 \u00b1 1.958.1 \u00b1 1.1PLM-AS-RM [haller2022eye]59.7 \u00b1 0.652.3 \u00b1 0.660.5 \u00b1 2.056.4 \u00b1 0.663.7 \u00b1 0.855.1 \u00b1 0.861.3 \u00b1 1.859.9 \u00b1 0.8RoBERTEye-W [Shubi2024Finegrained]59.3 \u00b1 1.950.2 \u00b1 0.752.5 \u00b1 1.554.7 \u00b1 1.168.1 \u00b1 1.556.9 \u00b1 1.057.8 \u00b1 1.463.1 \u00b1 1.0RoBERTEye-F [Shubi2024Finegrained]57.5 \u00b1 1.251.0 \u00b1 0.750.8 \u00b1 0.754.0 \u00b1 0.867.9 \u00b1 1.457.4 \u00b1 0.958.9 \u00b1 2.863.2 \u00b1 0.8MAG-Eye [Shubi2024Finegrained]58.4 \u00b1 1.550.9 \u00b1 0.551.4 \u00b1 1.454.4 \u00b1 0.868.5 \u00b1 0.958.3 \u00b1 0.960.4 \u00b1 2.063.7 \u00b1 0.8PostFusion-Eye [Shubi2024Finegrained]59.1 \u00b1 2.053.2 \u00b1 0.855.3 \u00b1 2.156.1 \u00b1 1.366.2 \u00b1 0.958.5 \u00b1 1.259.8 \u00b1 2.862.5 \u00b1 0.7"},{"location":"results/potec_de/","title":"PoTeC DE","text":"<p>Domain Expertise (PoTeC)</p> <p></p>"},{"location":"results/potec_de/#test","title":"Test","text":"ModelUnseen Reader Balanced AccuracyUnseen Text Balanced AccuracyUnseen Text and Reader Balanced AccuracyAverage Balanced AccuracyUnseen Reader AUROCUnseen Text AUROCUnseen Text and Reader AUROCAverage AUROCMajority Class / Chance52.5 \u00b1 2.349.9 \u00b1 0.649.9 \u00b1 1.351.4 \u00b1 1.352.5 \u00b1 2.349.9 \u00b1 0.649.9 \u00b1 1.351.4 \u00b1 1.3Reading Speed59.2 \u00b1 2.259.1 \u00b1 4.057.7 \u00b1 4.759.0 \u00b1 1.060.2 \u00b1 1.461.0 \u00b1 5.556.8 \u00b1 6.860.4 \u00b1 1.7Text-Only Roberta50.0 \u00b1 0.050.0 \u00b1 0.050.0 \u00b1 0.050.0 \u00b1 0.065.7 \u00b1 4.457.6 \u00b1 5.655.2 \u00b1 2.762.0 \u00b1 4.0Logistic Regression [meziere2023using]55.3 \u00b1 1.650.6 \u00b1 2.842.5 \u00b1 5.051.6 \u00b1 1.558.8 \u00b1 2.053.1 \u00b1 2.841.6 \u00b1 7.854.0 \u00b1 1.7SVM [hollenstein2023zuco]53.5 \u00b1 2.257.0 \u00b1 1.649.3 \u00b1 4.454.5 \u00b1 1.553.5 \u00b1 2.257.0 \u00b1 1.649.3 \u00b1 4.454.5 \u00b1 1.5Random Forest [makowski2024detection]56.9 \u00b1 3.650.7 \u00b1 0.651.7 \u00b1 1.753.6 \u00b1 1.869.2 \u00b1 3.652.7 \u00b1 7.460.2 \u00b1 4.162.3 \u00b1 3.4AhnRNN [ahn2020towards]50.0 \u00b1 0.050.0 \u00b1 0.050.0 \u00b1 0.050.0 \u00b1 0.050.0 \u00b1 0.149.9 \u00b1 0.150.0 \u00b1 0.050.0 \u00b1 0.1AhnCNN [ahn2020towards]50.6 \u00b1 0.549.9 \u00b1 0.149.8 \u00b1 0.250.2 \u00b1 0.260.7 \u00b1 2.459.8 \u00b1 7.360.8 \u00b1 6.660.6 \u00b1 3.4BEyeLSTM [reich_inferring_2022]64.1 \u00b1 4.147.1 \u00b1 3.350.5 \u00b1 5.253.0 \u00b1 2.765.7 \u00b1 3.847.2 \u00b1 6.846.8 \u00b1 12.151.8 \u00b1 3.5PLM-AS [Yang2023PLMASPL]53.0 \u00b1 2.047.7 \u00b1 1.150.2 \u00b1 1.150.0 \u00b1 0.652.6 \u00b1 2.852.6 \u00b1 2.549.0 \u00b1 9.551.3 \u00b1 2.4PLM-AS-RM [haller2022eye]55.5 \u00b1 3.949.9 \u00b1 0.150.0 \u00b1 0.052.4 \u00b1 1.764.7 \u00b1 5.865.4 \u00b1 3.660.7 \u00b1 4.164.2 \u00b1 4.0RoBERTEye-W [Shubi2024Finegrained]50.0 \u00b1 0.050.0 \u00b1 0.050.0 \u00b1 0.050.0 \u00b1 0.065.3 \u00b1 5.362.6 \u00b1 9.861.3 \u00b1 9.462.5 \u00b1 7.3RoBERTEye-F [Shubi2024Finegrained]50.0 \u00b1 0.050.0 \u00b1 0.050.0 \u00b1 0.050.0 \u00b1 0.071.3 \u00b1 2.152.0 \u00b1 6.566.7 \u00b1 1.864.5 \u00b1 3.1MAG-Eye [Shubi2024Finegrained]50.8 \u00b1 0.750.0 \u00b1 0.050.0 \u00b1 0.050.4 \u00b1 0.465.2 \u00b1 7.647.4 \u00b1 9.348.9 \u00b1 13.457.6 \u00b1 7.1PostFusion-Eye [Shubi2024Finegrained]49.9 \u00b1 0.150.0 \u00b1 0.050.0 \u00b1 0.050.0 \u00b1 0.055.0 \u00b1 4.050.7 \u00b1 4.252.3 \u00b1 4.753.6 \u00b1 0.9"},{"location":"results/potec_de/#validation","title":"Validation","text":"ModelUnseen Reader Balanced AccuracyUnseen Text Balanced AccuracyUnseen Text and Reader Balanced AccuracyAverage Balanced AccuracyUnseen Reader AUROCUnseen Text AUROCUnseen Text and Reader AUROCAverage AUROCMajority Class / Chance52.9 \u00b1 1.649.8 \u00b1 0.649.8 \u00b1 1.051.0 \u00b1 0.552.9 \u00b1 1.649.8 \u00b1 0.649.8 \u00b1 1.051.0 \u00b1 0.5Reading Speed60.2 \u00b1 4.756.8 \u00b1 6.557.1 \u00b1 3.958.9 \u00b1 3.762.3 \u00b1 3.356.6 \u00b1 7.656.8 \u00b1 6.860.1 \u00b1 3.0Text-Only Roberta50.0 \u00b1 0.050.0 \u00b1 0.050.0 \u00b1 0.050.0 \u00b1 0.069.8 \u00b1 3.063.3 \u00b1 9.362.8 \u00b1 5.366.7 \u00b1 3.8Logistic Regression [meziere2023using]55.9 \u00b1 3.250.2 \u00b1 2.747.0 \u00b1 7.053.2 \u00b1 1.454.6 \u00b1 4.051.1 \u00b1 2.946.0 \u00b1 7.053.3 \u00b1 1.5SVM [hollenstein2023zuco]56.4 \u00b1 1.559.1 \u00b1 2.559.0 \u00b1 2.357.8 \u00b1 1.856.4 \u00b1 1.559.1 \u00b1 2.559.0 \u00b1 2.357.8 \u00b1 1.8Random Forest [makowski2024detection]62.8 \u00b1 6.053.3 \u00b1 4.049.2 \u00b1 2.058.4 \u00b1 3.263.6 \u00b1 5.159.4 \u00b1 7.349.2 \u00b1 5.560.9 \u00b1 3.6AhnRNN [ahn2020towards]50.0 \u00b1 0.050.0 \u00b1 0.050.0 \u00b1 0.050.0 \u00b1 0.050.0 \u00b1 0.250.1 \u00b1 0.150.0 \u00b1 0.050.0 \u00b1 0.1AhnCNN [ahn2020towards]49.9 \u00b1 0.150.0 \u00b1 0.050.0 \u00b1 0.050.0 \u00b1 0.059.9 \u00b1 5.560.0 \u00b1 9.459.0 \u00b1 5.361.1 \u00b1 2.6BEyeLSTM [reich_inferring_2022]67.5 \u00b1 1.960.0 \u00b1 5.059.2 \u00b1 4.963.6 \u00b1 1.871.1 \u00b1 1.971.5 \u00b1 5.667.3 \u00b1 3.371.7 \u00b1 2.3PLM-AS [Yang2023PLMASPL]56.2 \u00b1 3.452.5 \u00b1 1.847.1 \u00b1 0.754.4 \u00b1 2.561.4 \u00b1 6.555.5 \u00b1 2.534.4 \u00b1 6.356.2 \u00b1 4.7PLM-AS-RM [haller2022eye]61.4 \u00b1 5.950.0 \u00b1 0.050.0 \u00b1 0.057.0 \u00b1 3.870.8 \u00b1 3.848.5 \u00b1 11.242.0 \u00b1 10.857.9 \u00b1 7.7RoBERTEye-W [Shubi2024Finegrained]50.0 \u00b1 0.050.0 \u00b1 0.050.0 \u00b1 0.050.0 \u00b1 0.070.6 \u00b1 6.367.0 \u00b1 9.473.7 \u00b1 7.369.9 \u00b1 4.3RoBERTEye-F [Shubi2024Finegrained]50.0 \u00b1 0.050.0 \u00b1 0.050.0 \u00b1 0.050.0 \u00b1 0.073.3 \u00b1 4.870.9 \u00b1 5.957.8 \u00b1 7.571.4 \u00b1 1.6MAG-Eye [Shubi2024Finegrained]54.7 \u00b1 4.150.0 \u00b1 0.050.0 \u00b1 0.052.5 \u00b1 2.267.8 \u00b1 3.574.4 \u00b1 5.570.9 \u00b1 8.271.6 \u00b1 3.5PostFusion-Eye [Shubi2024Finegrained]50.6 \u00b1 0.650.0 \u00b1 0.050.0 \u00b1 0.050.4 \u00b1 0.459.5 \u00b1 2.164.5 \u00b1 6.754.8 \u00b1 7.261.1 \u00b1 4.2"},{"location":"results/potec_rc/","title":"PoTeC RC","text":"<p>Reading Comprehension (PoTeC)</p> <p></p>"},{"location":"results/potec_rc/#test","title":"Test","text":"ModelUnseen Reader Balanced AccuracyUnseen Text Balanced AccuracyUnseen Text and Reader Balanced AccuracyAverage Balanced AccuracyUnseen Reader AUROCUnseen Text AUROCUnseen Text and Reader AUROCAverage AUROCMajority Class / Chance50.0 \u00b1 0.050.0 \u00b1 0.050.0 \u00b1 0.050.0 \u00b1 0.050.0 \u00b1 0.050.0 \u00b1 0.050.0 \u00b1 0.050.0 \u00b1 0.0Reading Speed52.5 \u00b1 1.250.7 \u00b1 1.649.3 \u00b1 3.651.3 \u00b1 1.752.2 \u00b1 1.551.1 \u00b1 2.053.5 \u00b1 6.151.9 \u00b1 2.2Text-Only Roberta57.6 \u00b1 0.948.8 \u00b1 1.750.3 \u00b1 1.751.7 \u00b1 1.362.5 \u00b1 1.548.2 \u00b1 1.848.2 \u00b1 2.456.3 \u00b1 1.6Logistic Regression [meziere2023using]53.6 \u00b1 0.953.9 \u00b1 1.851.3 \u00b1 2.152.9 \u00b1 0.453.9 \u00b1 1.855.9 \u00b1 2.253.2 \u00b1 0.654.1 \u00b1 0.7SVM [hollenstein2023zuco]51.3 \u00b1 0.950.1 \u00b1 0.750.6 \u00b1 0.950.6 \u00b1 0.751.3 \u00b1 0.950.1 \u00b1 0.750.6 \u00b1 0.950.6 \u00b1 0.7Random Forest [makowski2024detection]55.8 \u00b1 1.548.3 \u00b1 1.448.2 \u00b1 2.351.8 \u00b1 1.259.2 \u00b1 2.249.9 \u00b1 1.746.1 \u00b1 2.654.3 \u00b1 1.2AhnRNN [ahn2020towards]50.0 \u00b1 0.050.0 \u00b1 0.050.0 \u00b1 0.050.0 \u00b1 0.050.0 \u00b1 0.050.0 \u00b1 0.050.0 \u00b1 0.050.0 \u00b1 0.0AhnCNN [ahn2020towards]50.0 \u00b1 1.149.7 \u00b1 1.249.5 \u00b1 2.049.4 \u00b1 0.951.4 \u00b1 1.853.9 \u00b1 2.647.4 \u00b1 2.351.6 \u00b1 2.0BEyeLSTM [reich_inferring_2022]58.5 \u00b1 1.151.6 \u00b1 0.951.0 \u00b1 1.653.2 \u00b1 0.861.1 \u00b1 1.951.5 \u00b1 2.751.7 \u00b1 4.254.7 \u00b1 1.4PLM-AS [Yang2023PLMASPL]54.6 \u00b1 1.550.1 \u00b1 0.650.0 \u00b1 1.252.1 \u00b1 0.858.3 \u00b1 0.653.8 \u00b1 0.653.7 \u00b1 1.056.5 \u00b1 0.3PLM-AS-RM [haller2022eye]58.1 \u00b1 1.049.0 \u00b1 1.046.1 \u00b1 1.753.9 \u00b1 1.461.8 \u00b1 1.153.2 \u00b1 2.351.5 \u00b1 4.759.0 \u00b1 1.2RoBERTEye-W [Shubi2024Finegrained]58.1 \u00b1 1.049.7 \u00b1 0.148.8 \u00b1 0.752.6 \u00b1 0.961.1 \u00b1 0.551.7 \u00b1 2.549.7 \u00b1 3.256.8 \u00b1 1.2RoBERTEye-F [Shubi2024Finegrained]50.2 \u00b1 0.250.0 \u00b1 0.050.0 \u00b1 0.050.0 \u00b1 0.157.3 \u00b1 2.752.7 \u00b1 2.149.1 \u00b1 3.754.7 \u00b1 2.2MAG-Eye [Shubi2024Finegrained]59.3 \u00b1 1.149.8 \u00b1 0.249.1 \u00b1 0.854.2 \u00b1 1.163.7 \u00b1 1.548.7 \u00b1 2.448.6 \u00b1 3.958.3 \u00b1 1.3PostFusion-Eye [Shubi2024Finegrained]52.6 \u00b1 1.650.1 \u00b1 1.150.0 \u00b1 1.651.3 \u00b1 0.756.6 \u00b1 2.051.2 \u00b1 2.448.6 \u00b1 2.653.0 \u00b1 1.8"},{"location":"results/potec_rc/#validation","title":"Validation","text":"ModelUnseen Reader Balanced AccuracyUnseen Text Balanced AccuracyUnseen Text and Reader Balanced AccuracyAverage Balanced AccuracyUnseen Reader AUROCUnseen Text AUROCUnseen Text and Reader AUROCAverage AUROCMajority Class / Chance50.0 \u00b1 0.050.0 \u00b1 0.050.0 \u00b1 0.050.0 \u00b1 0.050.0 \u00b1 0.050.0 \u00b1 0.050.0 \u00b1 0.050.0 \u00b1 0.0Reading Speed50.0 \u00b1 1.551.1 \u00b1 1.546.7 \u00b1 3.949.7 \u00b1 1.949.5 \u00b1 2.152.7 \u00b1 1.745.2 \u00b1 5.950.0 \u00b1 2.6Text-Only Roberta60.1 \u00b1 1.950.0 \u00b1 0.050.0 \u00b1 0.055.3 \u00b1 1.263.5 \u00b1 2.456.8 \u00b1 4.855.2 \u00b1 5.359.8 \u00b1 1.3Logistic Regression [meziere2023using]52.1 \u00b1 2.453.8 \u00b1 2.151.8 \u00b1 1.252.9 \u00b1 0.452.7 \u00b1 3.055.2 \u00b1 2.850.5 \u00b1 2.053.7 \u00b1 0.8SVM [hollenstein2023zuco]51.4 \u00b1 1.254.8 \u00b1 1.749.2 \u00b1 2.052.1 \u00b1 0.851.4 \u00b1 1.254.8 \u00b1 1.749.2 \u00b1 2.052.1 \u00b1 0.8Random Forest [makowski2024detection]58.5 \u00b1 1.551.8 \u00b1 0.952.1 \u00b1 1.055.2 \u00b1 1.057.9 \u00b1 1.454.9 \u00b1 2.551.5 \u00b1 2.456.1 \u00b1 1.5AhnRNN [ahn2020towards]50.0 \u00b1 0.050.0 \u00b1 0.050.0 \u00b1 0.050.0 \u00b1 0.050.0 \u00b1 0.050.0 \u00b1 0.050.0 \u00b1 0.050.0 \u00b1 0.0AhnCNN [ahn2020towards]52.1 \u00b1 0.452.0 \u00b1 1.949.9 \u00b1 0.951.7 \u00b1 0.954.3 \u00b1 1.254.7 \u00b1 2.152.1 \u00b1 1.654.4 \u00b1 1.4BEyeLSTM [reich_inferring_2022]58.6 \u00b1 1.153.0 \u00b1 1.752.8 \u00b1 1.756.8 \u00b1 0.562.1 \u00b1 1.856.0 \u00b1 2.460.1 \u00b1 2.160.7 \u00b1 1.2PLM-AS [Yang2023PLMASPL]54.9 \u00b1 0.551.0 \u00b1 1.651.8 \u00b1 1.053.3 \u00b1 0.559.8 \u00b1 1.051.4 \u00b1 1.455.2 \u00b1 2.456.2 \u00b1 0.4PLM-AS-RM [haller2022eye]59.4 \u00b1 0.753.3 \u00b1 1.551.6 \u00b1 1.356.8 \u00b1 0.763.0 \u00b1 1.453.8 \u00b1 2.250.4 \u00b1 3.457.8 \u00b1 0.6RoBERTEye-W [Shubi2024Finegrained]60.5 \u00b1 0.252.4 \u00b1 1.552.7 \u00b1 2.556.7 \u00b1 0.863.1 \u00b1 1.854.0 \u00b1 1.054.8 \u00b1 2.959.1 \u00b1 0.3RoBERTEye-F [Shubi2024Finegrained]51.0 \u00b1 0.949.6 \u00b1 0.450.1 \u00b1 0.150.5 \u00b1 0.556.5 \u00b1 3.754.8 \u00b1 2.554.6 \u00b1 2.056.8 \u00b1 1.0MAG-Eye [Shubi2024Finegrained]59.9 \u00b1 1.953.3 \u00b1 2.251.5 \u00b1 1.556.7 \u00b1 1.065.4 \u00b1 2.157.7 \u00b1 1.858.7 \u00b1 1.661.7 \u00b1 0.8PostFusion-Eye [Shubi2024Finegrained]54.8 \u00b1 1.651.4 \u00b1 1.251.8 \u00b1 0.853.3 \u00b1 1.056.9 \u00b1 1.253.1 \u00b1 2.257.5 \u00b1 2.755.9 \u00b1 1.6"},{"location":"results/sbsat_rc/","title":"SBSAT RC","text":"<p>Reading Comprehension (SBSAT)</p> <p></p>"},{"location":"results/sbsat_rc/#test","title":"Test","text":"ModelUnseen Reader Balanced AccuracyUnseen Text Balanced AccuracyUnseen Text and Reader Balanced AccuracyAverage Balanced AccuracyUnseen Reader AUROCUnseen Text AUROCUnseen Text and Reader AUROCAverage AUROCMajority Class / Chance50.0 \u00b1 0.050.0 \u00b1 0.050.0 \u00b1 0.050.0 \u00b1 0.050.0 \u00b1 0.050.0 \u00b1 0.050.0 \u00b1 0.050.0 \u00b1 0.0Reading Speed51.1 \u00b1 1.850.9 \u00b1 0.449.9 \u00b1 0.150.2 \u00b1 0.952.0 \u00b1 2.150.8 \u00b1 0.352.4 \u00b1 1.050.8 \u00b1 1.6Text-Only Roberta61.5 \u00b1 1.851.2 \u00b1 0.750.4 \u00b1 0.556.1 \u00b1 1.167.1 \u00b1 1.443.7 \u00b1 4.546.0 \u00b1 2.355.9 \u00b1 2.3Logistic Regression [meziere2023using]53.6 \u00b1 1.551.3 \u00b1 0.749.9 \u00b1 1.051.8 \u00b1 0.953.7 \u00b1 1.352.3 \u00b1 0.947.8 \u00b1 3.152.3 \u00b1 1.0SVM [hollenstein2023zuco]50.7 \u00b1 0.650.3 \u00b1 1.648.3 \u00b1 0.750.2 \u00b1 0.950.7 \u00b1 0.650.3 \u00b1 1.648.3 \u00b1 0.750.2 \u00b1 0.9Random Forest [makowski2024detection]54.2 \u00b1 0.951.3 \u00b1 1.248.9 \u00b1 1.552.1 \u00b1 0.654.5 \u00b1 0.951.6 \u00b1 1.148.2 \u00b1 0.952.3 \u00b1 0.7AhnRNN [ahn2020towards]50.0 \u00b1 0.050.0 \u00b1 0.050.0 \u00b1 0.050.0 \u00b1 0.050.0 \u00b1 0.050.0 \u00b1 0.050.0 \u00b1 0.050.0 \u00b1 0.0AhnCNN [ahn2020towards]50.9 \u00b1 0.451.3 \u00b1 0.750.9 \u00b1 0.851.1 \u00b1 0.549.8 \u00b1 2.052.7 \u00b1 1.448.4 \u00b1 3.250.8 \u00b1 1.8BEyeLSTM [reich_inferring_2022]50.9 \u00b1 0.949.9 \u00b1 0.948.5 \u00b1 2.049.9 \u00b1 0.451.6 \u00b1 1.451.1 \u00b1 1.348.9 \u00b1 2.950.1 \u00b1 0.5PLM-AS [Yang2023PLMASPL]49.1 \u00b1 0.750.7 \u00b1 1.348.2 \u00b1 1.449.5 \u00b1 0.249.7 \u00b1 1.649.9 \u00b1 0.649.5 \u00b1 3.649.5 \u00b1 1.1PLM-AS-RM [haller2022eye]51.2 \u00b1 0.451.5 \u00b1 1.251.2 \u00b1 1.051.2 \u00b1 0.753.0 \u00b1 1.754.7 \u00b1 2.254.2 \u00b1 1.453.9 \u00b1 1.1RoBERTEye-W [Shubi2024Finegrained]55.6 \u00b1 2.752.7 \u00b1 2.653.9 \u00b1 3.154.3 \u00b1 2.559.9 \u00b1 3.652.5 \u00b1 4.956.1 \u00b1 4.257.4 \u00b1 3.7RoBERTEye-F [Shubi2024Finegrained]55.7 \u00b1 4.051.0 \u00b1 1.452.5 \u00b1 1.353.8 \u00b1 2.458.7 \u00b1 5.453.4 \u00b1 2.651.8 \u00b1 1.056.0 \u00b1 2.7MAG-Eye [Shubi2024Finegrained]60.6 \u00b1 2.548.3 \u00b1 2.446.6 \u00b1 4.654.1 \u00b1 1.965.6 \u00b1 2.344.8 \u00b1 4.342.0 \u00b1 6.356.0 \u00b1 2.1PostFusion-Eye [Shubi2024Finegrained]53.9 \u00b1 2.249.1 \u00b1 0.951.0 \u00b1 1.051.7 \u00b1 1.157.9 \u00b1 3.555.9 \u00b1 4.052.7 \u00b1 5.955.4 \u00b1 2.6"},{"location":"results/sbsat_rc/#validation","title":"Validation","text":"ModelUnseen Reader Balanced AccuracyUnseen Text Balanced AccuracyUnseen Text and Reader Balanced AccuracyAverage Balanced AccuracyUnseen Reader AUROCUnseen Text AUROCUnseen Text and Reader AUROCAverage AUROCMajority Class / Chance50.0 \u00b1 0.050.0 \u00b1 0.050.0 \u00b1 0.050.0 \u00b1 0.050.0 \u00b1 0.050.0 \u00b1 0.050.0 \u00b1 0.050.0 \u00b1 0.0Reading Speed53.6 \u00b1 1.750.5 \u00b1 0.250.5 \u00b1 0.452.4 \u00b1 1.553.5 \u00b1 1.949.4 \u00b1 0.750.9 \u00b1 1.552.2 \u00b1 1.3Text-Only Roberta63.5 \u00b1 2.756.7 \u00b1 3.555.6 \u00b1 2.260.0 \u00b1 2.367.6 \u00b1 2.362.8 \u00b1 5.261.5 \u00b1 6.064.8 \u00b1 0.4Logistic Regression [meziere2023using]52.7 \u00b1 1.452.8 \u00b1 1.552.4 \u00b1 2.452.5 \u00b1 1.452.4 \u00b1 1.851.4 \u00b1 1.451.8 \u00b1 3.151.9 \u00b1 1.5SVM [hollenstein2023zuco]52.1 \u00b1 1.152.6 \u00b1 1.152.9 \u00b1 2.652.7 \u00b1 1.252.1 \u00b1 1.152.6 \u00b1 1.152.9 \u00b1 2.652.7 \u00b1 1.2Random Forest [makowski2024detection]53.1 \u00b1 1.049.9 \u00b1 2.354.1 \u00b1 2.651.9 \u00b1 1.355.1 \u00b1 1.451.2 \u00b1 2.254.9 \u00b1 3.053.5 \u00b1 1.4AhnRNN [ahn2020towards]50.0 \u00b1 0.050.0 \u00b1 0.050.0 \u00b1 0.050.0 \u00b1 0.050.0 \u00b1 0.050.0 \u00b1 0.050.0 \u00b1 0.050.0 \u00b1 0.0AhnCNN [ahn2020towards]50.8 \u00b1 0.450.5 \u00b1 0.748.5 \u00b1 0.850.2 \u00b1 0.354.5 \u00b1 1.649.9 \u00b1 1.550.9 \u00b1 0.551.7 \u00b1 1.2BEyeLSTM [reich_inferring_2022]52.2 \u00b1 1.450.1 \u00b1 1.250.0 \u00b1 1.251.0 \u00b1 1.255.1 \u00b1 2.351.9 \u00b1 1.552.6 \u00b1 1.852.7 \u00b1 1.7PLM-AS [Yang2023PLMASPL]50.1 \u00b1 0.851.1 \u00b1 1.548.2 \u00b1 2.250.1 \u00b1 1.149.2 \u00b1 1.751.2 \u00b1 3.350.1 \u00b1 1.050.1 \u00b1 1.7PLM-AS-RM [haller2022eye]52.0 \u00b1 0.251.2 \u00b1 1.151.8 \u00b1 0.851.6 \u00b1 0.554.0 \u00b1 1.852.0 \u00b1 2.953.8 \u00b1 1.053.2 \u00b1 1.3RoBERTEye-W [Shubi2024Finegrained]59.4 \u00b1 4.650.5 \u00b1 0.552.1 \u00b1 1.454.2 \u00b1 2.164.2 \u00b1 4.950.8 \u00b1 3.749.8 \u00b1 2.057.3 \u00b1 2.6RoBERTEye-F [Shubi2024Finegrained]58.3 \u00b1 3.455.8 \u00b1 2.953.7 \u00b1 2.857.2 \u00b1 2.960.1 \u00b1 4.358.4 \u00b1 4.155.2 \u00b1 3.159.3 \u00b1 3.4MAG-Eye [Shubi2024Finegrained]63.2 \u00b1 2.953.2 \u00b1 3.152.8 \u00b1 2.856.8 \u00b1 2.769.2 \u00b1 2.949.1 \u00b1 6.750.3 \u00b1 7.160.2 \u00b1 3.5PostFusion-Eye [Shubi2024Finegrained]55.8 \u00b1 3.149.5 \u00b1 0.650.4 \u00b1 0.252.9 \u00b1 1.757.4 \u00b1 5.151.4 \u00b1 2.250.6 \u00b1 1.255.3 \u00b1 3.1"},{"location":"results/sbsat_std/","title":"SBSAT STD","text":"<p>Subjective Text Difficulty (SBSAT)</p> <p></p>"},{"location":"results/sbsat_std/#test","title":"Test","text":"ModelUnseen Reader RMSEUnseen Text RMSEUnseen Text and Reader RMSEAverage RMSEUnseen Reader MAEUnseen Text MAEUnseen Text and Reader MAEAverage MAEUnseen Reader R\u00b2Unseen Text R\u00b2Unseen Text and Reader R\u00b2Average R\u00b2Majority Class / Chance0.71 \u00b1 0.00.75 \u00b1 0.10.71 \u00b1 0.10.73 \u00b1 0.00.52 \u00b1 0.10.57 \u00b1 0.00.55 \u00b1 0.10.55 \u00b1 0.0-0.08 \u00b1 0.0-0.39 \u00b1 0.1-0.78 \u00b1 0.4-0.12 \u00b1 0.1Reading Speed0.71 \u00b1 0.00.82 \u00b1 0.00.8 \u00b1 0.10.77 \u00b1 0.00.58 \u00b1 0.10.7 \u00b1 0.00.68 \u00b1 0.00.65 \u00b1 0.0-0.09 \u00b1 0.1-0.73 \u00b1 0.3-1.29 \u00b1 0.5-0.25 \u00b1 0.1Text-Only Roberta0.67 \u00b1 0.00.75 \u00b1 0.00.74 \u00b1 0.10.72 \u00b1 0.00.56 \u00b1 0.00.64 \u00b1 0.00.64 \u00b1 0.10.61 \u00b1 0.00.03 \u00b1 0.0-0.46 \u00b1 0.3-1.03 \u00b1 0.6-0.07 \u00b1 0.1Logistic Regression [meziere2023using]0.74 \u00b1 0.00.89 \u00b1 0.00.83 \u00b1 0.10.82 \u00b1 0.00.6 \u00b1 0.00.76 \u00b1 0.10.71 \u00b1 0.10.68 \u00b1 0.0-0.19 \u00b1 0.1-1.1 \u00b1 0.5-1.54 \u00b1 0.7-0.42 \u00b1 0.1SVM [hollenstein2023zuco]0.72 \u00b1 0.00.73 \u00b1 0.00.7 \u00b1 0.10.73 \u00b1 0.00.52 \u00b1 0.10.54 \u00b1 0.00.53 \u00b1 0.10.53 \u00b1 0.0-0.12 \u00b1 0.1-0.31 \u00b1 0.1-0.72 \u00b1 0.3-0.1 \u00b1 0.0Random Forest [makowski2024detection]0.74 \u00b1 0.00.78 \u00b1 0.10.75 \u00b1 0.10.77 \u00b1 0.00.59 \u00b1 0.00.6 \u00b1 0.10.6 \u00b1 0.10.6 \u00b1 0.0-0.18 \u00b1 0.1-0.48 \u00b1 0.2-0.92 \u00b1 0.4-0.24 \u00b1 0.1AhnRNN [ahn2020towards]0.69 \u00b1 0.00.76 \u00b1 0.00.72 \u00b1 0.10.72 \u00b1 0.00.55 \u00b1 0.00.62 \u00b1 0.00.6 \u00b1 0.10.58 \u00b1 0.0-0.02 \u00b1 0.0-0.48 \u00b1 0.3-0.98 \u00b1 0.6-0.09 \u00b1 0.0AhnCNN [ahn2020towards]0.69 \u00b1 0.00.76 \u00b1 0.10.72 \u00b1 0.10.72 \u00b1 0.00.55 \u00b1 0.00.62 \u00b1 0.10.6 \u00b1 0.10.59 \u00b1 0.0-0.02 \u00b1 0.0-0.49 \u00b1 0.3-1.0 \u00b1 0.7-0.09 \u00b1 0.1BEyeLSTM [reich_inferring_2022]0.67 \u00b1 0.01.74 \u00b1 1.01.61 \u00b1 0.91.43 \u00b1 0.70.51 \u00b1 0.01.39 \u00b1 0.81.4 \u00b1 0.81.01 \u00b1 0.50.03 \u00b1 0.0-11.43 \u00b1 9.8-17.16 \u00b1 14.6-6.91 \u00b1 6.1PLM-AS [Yang2023PLMASPL]0.7 \u00b1 0.00.73 \u00b1 0.00.7 \u00b1 0.00.71 \u00b1 0.00.56 \u00b1 0.00.58 \u00b1 0.00.58 \u00b1 0.00.57 \u00b1 0.0-0.06 \u00b1 0.0-0.31 \u00b1 0.1-0.74 \u00b1 0.4-0.06 \u00b1 0.0PLM-AS-RM [haller2022eye]1.2 \u00b1 0.11.18 \u00b1 0.11.11 \u00b1 0.21.21 \u00b1 0.01.04 \u00b1 0.11.04 \u00b1 0.11.02 \u00b1 0.21.04 \u00b1 0.0-2.13 \u00b1 0.3-2.4 \u00b1 0.5-3.16 \u00b1 1.0-2.08 \u00b1 0.2RoBERTEye-W [Shubi2024Finegrained]0.67 \u00b1 0.00.74 \u00b1 0.00.72 \u00b1 0.10.71 \u00b1 0.00.54 \u00b1 0.00.6 \u00b1 0.00.6 \u00b1 0.10.58 \u00b1 0.00.04 \u00b1 0.0-0.4 \u00b1 0.2-0.9 \u00b1 0.5-0.05 \u00b1 0.0RoBERTEye-F [Shubi2024Finegrained]0.67 \u00b1 0.00.77 \u00b1 0.00.75 \u00b1 0.10.73 \u00b1 0.00.56 \u00b1 0.00.65 \u00b1 0.00.64 \u00b1 0.10.61 \u00b1 0.00.02 \u00b1 0.0-0.49 \u00b1 0.2-1.07 \u00b1 0.5-0.1 \u00b1 0.0MAG-Eye [Shubi2024Finegrained]0.67 \u00b1 0.00.76 \u00b1 0.10.74 \u00b1 0.10.72 \u00b1 0.00.54 \u00b1 0.00.62 \u00b1 0.00.62 \u00b1 0.10.59 \u00b1 0.00.03 \u00b1 0.0-0.44 \u00b1 0.2-1.01 \u00b1 0.5-0.09 \u00b1 0.1PostFusion-Eye [Shubi2024Finegrained]0.71 \u00b1 0.00.85 \u00b1 0.10.85 \u00b1 0.10.8 \u00b1 0.10.57 \u00b1 0.00.66 \u00b1 0.10.69 \u00b1 0.10.63 \u00b1 0.1-0.08 \u00b1 0.0-0.81 \u00b1 0.3-1.8 \u00b1 0.9-0.34 \u00b1 0.2"},{"location":"results/sbsat_std/#validation","title":"Validation","text":"ModelUnseen Reader RMSEUnseen Text RMSEUnseen Text and Reader RMSEAverage RMSEUnseen Reader MAEUnseen Text MAEUnseen Text and Reader MAEAverage MAEUnseen Reader R\u00b2Unseen Text R\u00b2Unseen Text and Reader R\u00b2Average R\u00b2Majority Class / Chance0.73 \u00b1 0.10.68 \u00b1 0.00.64 \u00b1 0.00.7 \u00b1 0.00.55 \u00b1 0.10.5 \u00b1 0.10.48 \u00b1 0.10.51 \u00b1 0.0-0.21 \u00b1 0.2-0.09 \u00b1 0.0-0.36 \u00b1 0.1-0.05 \u00b1 0.0Reading Speed0.68 \u00b1 0.00.77 \u00b1 0.10.79 \u00b1 0.10.74 \u00b1 0.00.52 \u00b1 0.00.66 \u00b1 0.10.67 \u00b1 0.10.61 \u00b1 0.0-0.06 \u00b1 0.0-0.48 \u00b1 0.3-1.37 \u00b1 0.7-0.17 \u00b1 0.1Text-Only Roberta0.67 \u00b1 0.00.67 \u00b1 0.00.62 \u00b1 0.00.66 \u00b1 0.00.57 \u00b1 0.00.55 \u00b1 0.00.52 \u00b1 0.00.55 \u00b1 0.0-0.03 \u00b1 0.1-0.06 \u00b1 0.0-0.32 \u00b1 0.20.06 \u00b1 0.0Logistic Regression [meziere2023using]0.71 \u00b1 0.00.77 \u00b1 0.10.78 \u00b1 0.10.75 \u00b1 0.00.55 \u00b1 0.00.66 \u00b1 0.10.63 \u00b1 0.10.61 \u00b1 0.0-0.16 \u00b1 0.0-0.55 \u00b1 0.4-1.28 \u00b1 0.7-0.22 \u00b1 0.1SVM [hollenstein2023zuco]0.7 \u00b1 0.00.72 \u00b1 0.00.65 \u00b1 0.10.7 \u00b1 0.00.5 \u00b1 0.10.52 \u00b1 0.00.49 \u00b1 0.10.51 \u00b1 0.0-0.11 \u00b1 0.1-0.2 \u00b1 0.1-0.47 \u00b1 0.2-0.06 \u00b1 0.0Random Forest [makowski2024detection]0.64 \u00b1 0.00.77 \u00b1 0.00.81 \u00b1 0.10.73 \u00b1 0.00.49 \u00b1 0.00.61 \u00b1 0.00.66 \u00b1 0.10.57 \u00b1 0.00.05 \u00b1 0.1-0.4 \u00b1 0.2-1.37 \u00b1 0.5-0.15 \u00b1 0.1AhnRNN [ahn2020towards]0.72 \u00b1 0.00.67 \u00b1 0.00.61 \u00b1 0.00.68 \u00b1 0.00.59 \u00b1 0.00.53 \u00b1 0.00.49 \u00b1 0.00.55 \u00b1 0.0-0.2 \u00b1 0.1-0.05 \u00b1 0.0-0.26 \u00b1 0.2-0.0 \u00b1 0.0AhnCNN [ahn2020towards]0.72 \u00b1 0.00.67 \u00b1 0.00.6 \u00b1 0.00.68 \u00b1 0.00.59 \u00b1 0.00.53 \u00b1 0.00.49 \u00b1 0.00.54 \u00b1 0.0-0.18 \u00b1 0.1-0.03 \u00b1 0.0-0.22 \u00b1 0.10.02 \u00b1 0.0BEyeLSTM [reich_inferring_2022]0.67 \u00b1 0.00.68 \u00b1 0.00.59 \u00b1 0.00.66 \u00b1 0.00.51 \u00b1 0.00.54 \u00b1 0.10.49 \u00b1 0.00.52 \u00b1 0.0-0.03 \u00b1 0.0-0.09 \u00b1 0.0-0.17 \u00b1 0.10.05 \u00b1 0.1PLM-AS [Yang2023PLMASPL]0.69 \u00b1 0.00.71 \u00b1 0.00.66 \u00b1 0.00.69 \u00b1 0.00.56 \u00b1 0.00.57 \u00b1 0.00.53 \u00b1 0.00.56 \u00b1 0.0-0.09 \u00b1 0.1-0.19 \u00b1 0.1-0.53 \u00b1 0.3-0.04 \u00b1 0.0PLM-AS-RM [haller2022eye]1.2 \u00b1 0.11.18 \u00b1 0.11.12 \u00b1 0.21.2 \u00b1 0.01.05 \u00b1 0.11.02 \u00b1 0.11.02 \u00b1 0.21.03 \u00b1 0.0-2.33 \u00b1 0.4-2.24 \u00b1 0.4-3.18 \u00b1 1.0-2.14 \u00b1 0.3RoBERTEye-W [Shubi2024Finegrained]0.64 \u00b1 0.00.67 \u00b1 0.00.61 \u00b1 0.00.65 \u00b1 0.00.53 \u00b1 0.00.54 \u00b1 0.00.5 \u00b1 0.00.53 \u00b1 0.00.05 \u00b1 0.1-0.07 \u00b1 0.0-0.3 \u00b1 0.20.09 \u00b1 0.0RoBERTEye-F [Shubi2024Finegrained]0.62 \u00b1 0.00.68 \u00b1 0.00.65 \u00b1 0.00.65 \u00b1 0.00.52 \u00b1 0.00.56 \u00b1 0.00.55 \u00b1 0.00.54 \u00b1 0.00.1 \u00b1 0.1-0.1 \u00b1 0.1-0.48 \u00b1 0.30.09 \u00b1 0.0MAG-Eye [Shubi2024Finegrained]0.64 \u00b1 0.00.67 \u00b1 0.00.61 \u00b1 0.00.65 \u00b1 0.00.53 \u00b1 0.00.52 \u00b1 0.00.49 \u00b1 0.00.52 \u00b1 0.00.05 \u00b1 0.1-0.04 \u00b1 0.0-0.26 \u00b1 0.10.09 \u00b1 0.0PostFusion-Eye [Shubi2024Finegrained]0.7 \u00b1 0.10.68 \u00b1 0.00.63 \u00b1 0.00.68 \u00b1 0.00.57 \u00b1 0.00.55 \u00b1 0.00.52 \u00b1 0.00.55 \u00b1 0.0-0.13 \u00b1 0.2-0.08 \u00b1 0.0-0.34 \u00b1 0.20.0 \u00b1 0.0"}]}